{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8067c989",
   "metadata": {},
   "source": [
    "# DD2417 Final Project - Dating Historical Texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea74981f",
   "metadata": {},
   "source": [
    "## Libraries + Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5995f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seed all experiments and setup\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f081e4ec",
   "metadata": {},
   "source": [
    "## Data - Setup and Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459303ab",
   "metadata": {},
   "source": [
    "### Path Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7593bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "raw_dataset_path = \"./Datasets/raw_data\"\n",
    "\n",
    "# raw split\n",
    "raw_train_split_path = \"./Datasets/raw_train_split\"\n",
    "raw_test_split_path = \"./Datasets/raw_test_split\"\n",
    "\n",
    "# clean split\n",
    "clean_train_split_path = \"./Datasets/clean_train_split\"\n",
    "clean_test_split_path = \"./Datasets/clean_test_split\"\n",
    "\n",
    "# model\n",
    "model_dataset_path = \"./Datasets/model_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59447748",
   "metadata": {},
   "source": [
    "#### Cleanup Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b068f3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(train_path, test_path):\n",
    "    print(f\"clean up train path - {train_path}\")\n",
    "    train_dir = os.listdir(train_path)\n",
    "    train_dir.sort()\n",
    "    for dir in train_dir:\n",
    "        decade_path = os.path.join(train_path, dir)\n",
    "        if os.path.isdir(decade_path):\n",
    "            text_files = os.listdir(decade_path)\n",
    "            text_files.sort()\n",
    "            for file in text_files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(decade_path, file)\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"succesfully remove {file}\")\n",
    "            os.rmdir(decade_path)\n",
    "            print(f\"succesfully removed directory {dir}\")\n",
    "            print()\n",
    "\n",
    "    print(f\"clean up test path - {test_path}\")\n",
    "    test_dir = os.listdir(test_path)\n",
    "    test_dir.sort()\n",
    "    for dir in test_dir:\n",
    "        decade_path = os.path.join(test_path, dir)\n",
    "        if os.path.isdir(decade_path):\n",
    "            text_files = os.listdir(decade_path)\n",
    "            text_files.sort()\n",
    "            for file in text_files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(decade_path, file)\n",
    "                    os.remove(file_path)\n",
    "                    print(f\"succesfully remove {file}\")\n",
    "            os.rmdir(decade_path)\n",
    "            print(f\"succesfully removed directory {dir}\")\n",
    "            print()\n",
    "\n",
    "    os.rmdir(train_path)\n",
    "    print(f\"succesfully removed {train_path}\")\n",
    "    os.rmdir(test_path)\n",
    "    print(f\"succesfully removed {test_path}\")\n",
    "    print(f\"succesfully cleaned up training and test files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_model_data(model_dataset_path):\n",
    "    print(f\"clean up model path - {model_dataset_path}\")\n",
    "    files = os.listdir(model_dataset_path)\n",
    "\n",
    "    # Remove each file\n",
    "    for file in files:\n",
    "        file_path = os.path.join(model_dataset_path, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            os.remove(file_path)\n",
    "            print(f\"successfully removed {file}\")\n",
    "    os.rmdir(model_dataset_path)\n",
    "    print(f\"successfully removed {model_dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278e6f0",
   "metadata": {},
   "source": [
    "#### Create data directories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b02b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw split\n",
    "if os.path.exists(raw_train_split_path) and os.path.exists(raw_test_split_path):\n",
    "    cleanup(raw_train_split_path, raw_test_split_path)\n",
    "\n",
    "os.makedirs(raw_train_split_path)\n",
    "print(f\"create raw train split directory\")\n",
    "\n",
    "os.makedirs(raw_test_split_path)\n",
    "print(f\"create raw test split directory\")\n",
    "\n",
    "os.makedirs(model_dataset_path)\n",
    "print(f\"create model data directory \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6409dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean split\n",
    "if os.path.exists(clean_train_split_path) and os.path.exists(clean_test_split_path):\n",
    "    cleanup(clean_train_split_path, clean_test_split_path)\n",
    "\n",
    "os.makedirs(clean_train_split_path)\n",
    "print(f\"create clean train split directory\")\n",
    "os.makedirs(clean_test_split_path)\n",
    "print(f\"create clean test split directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d4fea9",
   "metadata": {},
   "source": [
    "### Book Data Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c982361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count all the data files in the raw data file\n",
    "print(f\"count the number of books in each decade directory in the raw data\")\n",
    "total_books = 0\n",
    "for decade in range(1700, 1900, 10):\n",
    "    decade_path = f\"{raw_dataset_path}/{decade}\"\n",
    "    if os.path.exists(decade_path):\n",
    "        text_files = [f for f in os.listdir(decade_path) if f.endswith(\".txt\")]\n",
    "        print(f\"{decade}: {len(text_files)} books\")\n",
    "        total_books += len(text_files)\n",
    "print(f\"total number of books for project: {total_books}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732386ba",
   "metadata": {},
   "source": [
    "#### Get the Titles of Books in the Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a9caf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the titles of the books\n",
    "def get_book_titles(dataset_path):\n",
    "    book_titles = {}\n",
    "    for year in range(1770, 1900, 10):\n",
    "        decade_path = f\"{dataset_path}/{year}\"\n",
    "        book_titles[year] = []\n",
    "\n",
    "        # print(f\"decade: {year}\")\n",
    "        text_files = sorted([f for f in os.listdir(decade_path) if f.endswith(\".txt\")])\n",
    "        for filename in text_files:\n",
    "            file_path = os.path.join(decade_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            title_match = re.search(r\"^Title:\\s*(.+)$\", text, re.MULTILINE)\n",
    "            book_title = title_match.group(1).strip()\n",
    "            # print(f\"book_title: {book_title}\")\n",
    "            # filename_to_title[filename] = book_title\n",
    "            book_titles[year].append(book_title)\n",
    "        # print(f\"number of titles in decade: {year} -> {len(book_titles[year])}\")\n",
    "        print()\n",
    "    return book_titles\n",
    "\n",
    "\n",
    "book_titles = get_book_titles(raw_dataset_path)\n",
    "print(f\"{book_titles[1770]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0bcc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_dataset_info(dataset_path):\n",
    "    years = [i for i in range(1770, 1900, 10)]\n",
    "    book_titles = get_book_titles(dataset_path)\n",
    "\n",
    "    book_data = []\n",
    "    for decade in years:\n",
    "        decade_path = f\"{dataset_path}/{decade}\"\n",
    "        if os.path.exists(decade_path):\n",
    "            text_files = sorted(\n",
    "                [f for f in os.listdir(decade_path) if f.endswith(\".txt\")]\n",
    "            )\n",
    "            for index, filename in enumerate(text_files):\n",
    "                if decade in book_titles and index < len(book_titles[decade]):\n",
    "                    book_title = book_titles[decade][index]\n",
    "                else:\n",
    "                    book_title = f\"unknown_book_{index + 1}\"\n",
    "                book_info = {\n",
    "                    \"decade\": decade,\n",
    "                    \"filename\": filename,\n",
    "                    \"book_title\": book_title,\n",
    "                    \"filepath\": os.path.join(decade_path, filename),\n",
    "                    \"book_id\": f\"{decade}_{book_title[:20].replace(' ', '_')}\",\n",
    "                }\n",
    "                book_data.append(book_info)\n",
    "    print(f\"total number of books processed: {len(book_data)}\")\n",
    "    return book_data\n",
    "\n",
    "\n",
    "raw_data_info = raw_dataset_info(raw_dataset_path)\n",
    "print(f\"The length of result after calling raw dataset info: {len(raw_data_info)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69383a3",
   "metadata": {},
   "source": [
    "## Data Split - Stratified Split of Books - Training Books, Testing Books\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b8b337",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This functions performs a stratified split of the data. Since we have a limited number of books, we took 80% of the total books to be used \n",
    "for training and then held out 20% of the books for testing. Our original number of books was 160 - 122 were used for Training/Validation, \n",
    "38 were used for testing\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_stratified_split(book_data, train_split=0.8):\n",
    "    train_books, test_books = [], []\n",
    "    books_by_decade = {}\n",
    "\n",
    "    books_by_decade = {}\n",
    "    for book in book_data:\n",
    "        decade = book[\"decade\"]\n",
    "        if decade not in books_by_decade:\n",
    "            books_by_decade[decade] = []\n",
    "        books_by_decade[decade].append(book)\n",
    "\n",
    "    # debug check\n",
    "    # for decade, books in books_by_decade.items():\n",
    "    #     print(f\"decade: {decade}, number of books: {len(books)}\")\n",
    "\n",
    "    for decade, books in sorted(books_by_decade.items()):\n",
    "        shuffled_books = books.copy()\n",
    "        random.shuffle(shuffled_books)\n",
    "\n",
    "        total_books = len(books)\n",
    "        train_size = max(1, int(total_books * train_split))\n",
    "        test_size = total_books - train_size\n",
    "        decade_train = shuffled_books[:train_size]\n",
    "        decade_test = shuffled_books[train_size:]\n",
    "\n",
    "        train_books.extend(decade_train)\n",
    "        test_books.extend(decade_test)\n",
    "\n",
    "    print(f\"TRAIN BOOKS: {len(train_books)}\")\n",
    "    print(f\"TEST BOOKS: {len(test_books)}\")\n",
    "\n",
    "    return train_books, test_books\n",
    "\n",
    "\n",
    "raw_book_data = raw_dataset_info(raw_dataset_path)\n",
    "raw_train, raw_test = create_stratified_split(raw_book_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef616d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_stratified_split(dataset, file_path):\n",
    "    for i, book in enumerate(dataset):\n",
    "        print(f\"book: {i + 1}\")\n",
    "        # decade\n",
    "        book_decade = str(book[\"decade\"])\n",
    "        # title\n",
    "        book_title = book[\"book_title\"]\n",
    "        # filename\n",
    "        book_filename = book[\"filename\"]\n",
    "        # path\n",
    "        book_path = book[\"filepath\"]\n",
    "\n",
    "        print(f\"read book <- {book_path}\")\n",
    "        with open(book_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_book = f.read()\n",
    "\n",
    "        decade_path = os.path.join(file_path, book_decade)\n",
    "        if not os.path.isdir(decade_path):\n",
    "            os.makedirs(decade_path)\n",
    "        out_file = decade_path + \"/\" + book_filename\n",
    "        book[\"file_path\"] = out_file\n",
    "        print(f\"new book filepath: {book_path}\")\n",
    "        print(f\"write book -> {out_file}\")\n",
    "        with open(out_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(raw_book)\n",
    "        print(f\"wrote book successfully!!!\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d1fb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_stratified_split(raw_train, raw_train_split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_stratified_split(raw_test, raw_test_split_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66beed86",
   "metadata": {},
   "source": [
    "## Data-Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd203c",
   "metadata": {},
   "source": [
    "## Data-Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "This function uses regex expressions to clean up the data. It removes mentiones of the year as specified in the project instructions \n",
    "as well as the header, footer and whitespace in the project gutenburg books\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    # remove everything up to and including start\n",
    "    start_match = re.search(\n",
    "        r\"\\*\\*\\* START OF.*?\\*\\*\\*\", text, re.IGNORECASE | re.DOTALL\n",
    "    )\n",
    "    if start_match:\n",
    "        text = text[start_match.end() :]\n",
    "\n",
    "    # remove everything after end\n",
    "    end_match = re.search(r\"\\*\\*\\* END OF.*?\\*\\*\\*\", text, re.IGNORECASE | re.DOTALL)\n",
    "    if end_match:\n",
    "        text = text[: end_match.start()]\n",
    "\n",
    "    # remove years\n",
    "    text = re.sub(r\"\\b1[0-9]{3}\\b\", \"\", text)\n",
    "\n",
    "    # remove whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b776fa6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function cleans up the data in the stratified split using the clean function \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def clean_stratified_split(raw_split_path, clean_split_path):\n",
    "    decade_dirs = [\n",
    "        dir\n",
    "        for dir in os.listdir(raw_split_path)\n",
    "        if os.path.isdir(os.path.join(raw_split_path, dir))\n",
    "    ]\n",
    "    decade_dirs.sort()\n",
    "\n",
    "    total_books = 0\n",
    "    for decade_dir in decade_dirs:\n",
    "        clean_decade_path = os.path.join(clean_split_path, decade_dir)\n",
    "        print(f\"clean decade path: {clean_decade_path}\")\n",
    "        if not os.path.exists(clean_decade_path):\n",
    "            os.makedirs(clean_decade_path)\n",
    "        raw_decade_path = os.path.join(raw_split_path, decade_dir)\n",
    "        print(f\"raw decade path: {raw_decade_path}\")\n",
    "        text_files = [f for f in os.listdir(raw_decade_path) if f.endswith(\".txt\")]\n",
    "\n",
    "        for text_file in text_files:\n",
    "            total_books += 1\n",
    "            print(f\"books processed: {total_books}\")\n",
    "            raw_file_path = os.path.join(raw_decade_path, text_file)\n",
    "            # print(f\"raw data path: {raw_file_path}\")\n",
    "            clean_file_path = os.path.join(clean_decade_path, text_file)\n",
    "            # print(f\"clean file path: {clean_file_path}\")\n",
    "            print(\n",
    "                f\"read raw data: {raw_file_path} -> clean data -> write clean data: {clean_file_path}\"\n",
    "            )\n",
    "            with open(raw_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                raw_data = f.read()\n",
    "                print(f\"read raw data <- {raw_file_path}\")\n",
    "\n",
    "            cleaned_data = clean_text(raw_data)\n",
    "            with open(clean_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(cleaned_data)\n",
    "                print(f\"write clean data -> {clean_file_path}\")\n",
    "\n",
    "            print(f\"wrote cleaned data successfully!!!\")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98977dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stratified_split(raw_train_split_path, clean_train_split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778f4c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stratified_split(raw_test_split_path, clean_test_split_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05070628",
   "metadata": {},
   "source": [
    "##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306beaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This function creates paragraphs using the ideas mentioned in the paper Deep Learning for Period Classification of Historical\n",
    "Texts\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def create_paragraphs(text, min_words=10, max_words=210):\n",
    "    words = text.split()\n",
    "\n",
    "    paragraphs = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + max_words, len(words))\n",
    "        paragraph_words = words[start:end]\n",
    "        if len(paragraph_words) >= min_words:\n",
    "            paragraph_text = \" \".join(paragraph_words)\n",
    "            paragraphs.append(paragraph_text)\n",
    "\n",
    "        start = end\n",
    "\n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_paragraph_data(clean_split_path, raw_data_path):\n",
    "    years = sorted(os.listdir(clean_split_path))\n",
    "    print(f\"years in sorted order: {years}\")\n",
    "    # print(f\"dataset path: {os.listdir(clean_train_split_path)}\")\n",
    "    book_titles = get_book_titles(raw_data_path)\n",
    "    paragraph_data = []\n",
    "    total_books = 0\n",
    "\n",
    "    # all the years\n",
    "    for decade in years:\n",
    "        print(f\"process decade: {decade}\")\n",
    "        decade_path = f\"{clean_split_path}/{decade}\"\n",
    "        print(f\"decade_path: {decade_path}\")\n",
    "        decade_titles = book_titles[int(decade)]\n",
    "        print(f\"number of book titles in decade: {len(decade_titles)}\")\n",
    "\n",
    "        if os.path.exists(decade_path):\n",
    "            text_files = sorted(\n",
    "                [f for f in os.listdir(decade_path) if f.endswith(\".txt\")]\n",
    "            )\n",
    "            for index, text_filename in enumerate(text_files):\n",
    "                print(f\"current book: {index + 1}\")\n",
    "                total_books += 1\n",
    "                print(f\"book filename: {text_filename}\")\n",
    "                text_file_number = int(re.findall(r\"\\d+\", text_filename)[0])\n",
    "                print(f\"book number: {text_file_number}\")\n",
    "                book_title = decade_titles[text_file_number - 1]\n",
    "                text_filepath = os.path.join(decade_path, text_filename)\n",
    "                with open(text_filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    clean_book_data = f.read()\n",
    "                    print(f\"succesfully read book!!!\")\n",
    "                book_paragraphs = create_paragraphs(clean_book_data)\n",
    "                print(f\"number of paragraphs created: {len(book_paragraphs)}\")\n",
    "                paragraph_length = len(book_paragraphs[0].split())\n",
    "                print(f\"length of a paragraph: {paragraph_length}\")\n",
    "\n",
    "                paragraph_info = {\n",
    "                    \"paragraphs\": book_paragraphs,\n",
    "                    \"book_title\": book_title,\n",
    "                    \"decade\": decade,\n",
    "                    \"filepath\": text_filepath,\n",
    "                    \"book_id\": f\"{decade}_{book_title[:20].replace(' ', '_')}\",\n",
    "                }\n",
    "                paragraph_data.append(paragraph_info)\n",
    "\n",
    "        print(f\"total number of books processed in decade {decade} -> {total_books}\")\n",
    "        print()\n",
    "\n",
    "    print(f\"total number of books processed: {total_books}\")\n",
    "    return paragraph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99707082",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_train_data = create_paragraph_data(clean_train_split_path, raw_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f5527",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_test_data = create_paragraph_data(clean_test_split_path, raw_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4bab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_csv(paragraph_data, output_file):\n",
    "    print(f\"write {len(paragraph_data)} -> {output_file}\")\n",
    "\n",
    "    # map decade to label for classification\n",
    "    decades = sorted(set(int(item[\"decade\"]) for item in paragraph_data))\n",
    "    decade_to_label = {decade: idx for idx, decade in enumerate(decades)}\n",
    "\n",
    "    with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        header_fields = [\n",
    "            \"text\",\n",
    "            \"decade\",\n",
    "            \"decade_label\",\n",
    "            \"book_title\",\n",
    "            \"book_id\",\n",
    "            \"paragraph_id\",\n",
    "            \"word_count\",\n",
    "        ]\n",
    "        writer = csv.DictWriter(f, fieldnames=header_fields)\n",
    "\n",
    "        writer.writeheader()\n",
    "        total_paragraphs = 0\n",
    "        total_book_count = 0\n",
    "\n",
    "        for book_index, paragraph_info in enumerate(paragraph_data):\n",
    "            decade = int(paragraph_info[\"decade\"])\n",
    "            decade_label = decade_to_label[decade]\n",
    "            book_title = paragraph_info[\"book_title\"]\n",
    "            book_id = paragraph_info[\"book_id\"]\n",
    "            paragraphs = paragraph_info[\"paragraphs\"]\n",
    "\n",
    "            for index, text in enumerate(paragraphs):\n",
    "                clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "                word_count = len(clean_text.split())\n",
    "\n",
    "                # from the paper\n",
    "                if word_count < 10:\n",
    "                    continue\n",
    "\n",
    "                writer.writerow(\n",
    "                    {\n",
    "                        \"text\": clean_text,\n",
    "                        \"decade\": decade,\n",
    "                        \"decade_label\": decade_label,\n",
    "                        \"book_title\": book_title,\n",
    "                        \"book_id\": book_id,\n",
    "                        \"paragraph_id\": f\"{decade}_{book_id}_{index:03d}\",\n",
    "                        \"word_count\": word_count,\n",
    "                    }\n",
    "                )\n",
    "                total_paragraphs += 1\n",
    "\n",
    "            total_book_count += 1\n",
    "            print(f\"processed total books: {book_index + 1}\")\n",
    "\n",
    "    print(\n",
    "        f\"Succesfully wrote {total_paragraphs} paragraphs and processed {total_book_count} books -> {output_file}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f4e8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = os.path.join(model_dataset_path, \"train_data.csv\")\n",
    "write_data_to_csv(paragraph_train_data, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba23e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = os.path.join(model_dataset_path, \"test_data.csv\")\n",
    "write_data_to_csv(paragraph_test_data, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd4afcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleanup_model_data(model_dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
