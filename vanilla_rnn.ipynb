{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63287636",
   "metadata": {},
   "source": [
    "# RNN Architecture - Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd439f",
   "metadata": {},
   "source": [
    "## Setup - Libraries, Packages, Embeddings, Paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de086a78",
   "metadata": {},
   "source": [
    "### Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fee19ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import urllib.request\n",
    "import zipfile \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d60d68",
   "metadata": {},
   "source": [
    "### Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "482feee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"./Embeddings\"\n",
    "def download_progress(block_num, block_size, total_size):\n",
    "    if not hasattr(download_progress, \"pbar\"):\n",
    "        download_progress.pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True)\n",
    "    download_progress.pbar.update(block_size)\n",
    "\n",
    "if not os.path.exists(embeddings_path):\n",
    "    print(f\"create directory to store pre-trained glove embeddings\")\n",
    "    os.makedirs(embeddings_path)\n",
    "    print(f\"download pre-trained Glove Embeddings\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "        \"./Embeddings/glove.6B.zip\",\n",
    "        download_progress,\n",
    "    )\n",
    "    print(\"unpack embeddings\")\n",
    "    with zipfile.ZipFile(\"./Embeddings/glove.6B.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"./Embeddings/\")\n",
    "    os.remove(\"./Embeddings/glove.6B.zip\")\n",
    "    \n",
    "    print(\"embeddings download complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e3d2a5",
   "metadata": {},
   "source": [
    "### Paths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8294cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6b_50_path = \"./Embeddings/glove.6B.50d.txt\"\n",
    "train_data_path = \"./Datasets/model_data/train_data.csv\"\n",
    "test_data_path = \"./Datasets/model_data/test_data.csv\"\n",
    "clean_train_split_path = \"./Datasets/clean_train_split/\"\n",
    "clean_test_split_path = \"./Datasets/clean_test_split\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52462b95",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd07f76b",
   "metadata": {},
   "source": [
    "### Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d178c45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>decade</th>\n",
       "      <th>decade_label</th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Produced by Gary R. Young THE SCHOOL FOR SCAND...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>The School for Scandal</td>\n",
       "      <td>1770_The_School_for_Scand</td>\n",
       "      <td>1770_1770_The_School_for_Scand_000</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the works of Sheridan as he wrote them, I may ...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>The School for Scandal</td>\n",
       "      <td>1770_The_School_for_Scand</td>\n",
       "      <td>1770_1770_The_School_for_Scand_001</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he had been nineteen years endeavouring to sat...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>The School for Scandal</td>\n",
       "      <td>1770_The_School_for_Scand</td>\n",
       "      <td>1770_1770_The_School_for_Scand_002</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>That even you assist her fame to raise, Approv...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>The School for Scandal</td>\n",
       "      <td>1770_The_School_for_Scand</td>\n",
       "      <td>1770_1770_The_School_for_Scand_003</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>and face-- Poets would study the immortal line...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>The School for Scandal</td>\n",
       "      <td>1770_The_School_for_Scand</td>\n",
       "      <td>1770_1770_The_School_for_Scand_004</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>who the peril of her lips shall paint? Strip t...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>The School for Scandal</td>\n",
       "      <td>1770_The_School_for_Scand</td>\n",
       "      <td>1770_1770_The_School_for_Scand_005</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>might well be thought Prerogative in her, and ...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>The School for Scandal</td>\n",
       "      <td>1770_The_School_for_Scand</td>\n",
       "      <td>1770_1770_The_School_for_Scand_006</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>th' acknowledged praise Has spread conviction ...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>The School for Scandal</td>\n",
       "      <td>1770_The_School_for_Scand</td>\n",
       "      <td>1770_1770_The_School_for_Scand_007</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LAST NIGHT LORD L. [Sips] WAS CAUGHT WITH LADY...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>The School for Scandal</td>\n",
       "      <td>1770_The_School_for_Scand</td>\n",
       "      <td>1770_1770_The_School_for_Scand_008</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>he would through-- He'll fight--that's write--...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>The School for Scandal</td>\n",
       "      <td>1770_The_School_for_Scand</td>\n",
       "      <td>1770_1770_The_School_for_Scand_009</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  decade  decade_label  \\\n",
       "0  Produced by Gary R. Young THE SCHOOL FOR SCAND...    1770             0   \n",
       "1  the works of Sheridan as he wrote them, I may ...    1770             0   \n",
       "2  he had been nineteen years endeavouring to sat...    1770             0   \n",
       "3  That even you assist her fame to raise, Approv...    1770             0   \n",
       "4  and face-- Poets would study the immortal line...    1770             0   \n",
       "5  who the peril of her lips shall paint? Strip t...    1770             0   \n",
       "6  might well be thought Prerogative in her, and ...    1770             0   \n",
       "7  th' acknowledged praise Has spread conviction ...    1770             0   \n",
       "8  LAST NIGHT LORD L. [Sips] WAS CAUGHT WITH LADY...    1770             0   \n",
       "9  he would through-- He'll fight--that's write--...    1770             0   \n",
       "\n",
       "               book_title                    book_id  \\\n",
       "0  The School for Scandal  1770_The_School_for_Scand   \n",
       "1  The School for Scandal  1770_The_School_for_Scand   \n",
       "2  The School for Scandal  1770_The_School_for_Scand   \n",
       "3  The School for Scandal  1770_The_School_for_Scand   \n",
       "4  The School for Scandal  1770_The_School_for_Scand   \n",
       "5  The School for Scandal  1770_The_School_for_Scand   \n",
       "6  The School for Scandal  1770_The_School_for_Scand   \n",
       "7  The School for Scandal  1770_The_School_for_Scand   \n",
       "8  The School for Scandal  1770_The_School_for_Scand   \n",
       "9  The School for Scandal  1770_The_School_for_Scand   \n",
       "\n",
       "                         paragraph_id  word_count  \n",
       "0  1770_1770_The_School_for_Scand_000         210  \n",
       "1  1770_1770_The_School_for_Scand_001         210  \n",
       "2  1770_1770_The_School_for_Scand_002         210  \n",
       "3  1770_1770_The_School_for_Scand_003         210  \n",
       "4  1770_1770_The_School_for_Scand_004         210  \n",
       "5  1770_1770_The_School_for_Scand_005         210  \n",
       "6  1770_1770_The_School_for_Scand_006         210  \n",
       "7  1770_1770_The_School_for_Scand_007         210  \n",
       "8  1770_1770_The_School_for_Scand_008         210  \n",
       "9  1770_1770_The_School_for_Scand_009         210  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(train_data_path)\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76f5e340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 84860 entries, 0 to 84859\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   text          84860 non-null  object\n",
      " 1   decade        84860 non-null  int64 \n",
      " 2   decade_label  84860 non-null  int64 \n",
      " 3   book_title    84860 non-null  object\n",
      " 4   book_id       84860 non-null  object\n",
      " 5   paragraph_id  84860 non-null  object\n",
      " 6   word_count    84860 non-null  int64 \n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 4.5+ MB\n"
     ]
    }
   ],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cdff6f",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e2f35c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>decade</th>\n",
       "      <th>decade_label</th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>An Inquiry into the Nature and Causes of the W...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>An Inquiry into the Nature and Causes of the W...</td>\n",
       "      <td>1770_An_Inquiry_into_the_</td>\n",
       "      <td>1770_1770_An_Inquiry_into_the__000</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>THE EXPENSE OF MAINTAINING THE NATIONAL CAPITA...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>An Inquiry into the Nature and Causes of the W...</td>\n",
       "      <td>1770_An_Inquiry_into_the_</td>\n",
       "      <td>1770_1770_An_Inquiry_into_the__001</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRODUCE OF LAND, AS EITHER THE SOLE OR THE PRI...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>An Inquiry into the Nature and Causes of the W...</td>\n",
       "      <td>1770_An_Inquiry_into_the_</td>\n",
       "      <td>1770_1770_An_Inquiry_into_the__002</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Whatever be the soil, climate, or extent of te...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>An Inquiry into the Nature and Causes of the W...</td>\n",
       "      <td>1770_An_Inquiry_into_the_</td>\n",
       "      <td>1770_1770_An_Inquiry_into_the__003</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of those who work; yet the produce of the whol...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>An Inquiry into the Nature and Causes of the W...</td>\n",
       "      <td>1770_An_Inquiry_into_the_</td>\n",
       "      <td>1770_1770_An_Inquiry_into_the__004</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>of capital stock, of the manner in which it is...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>An Inquiry into the Nature and Causes of the W...</td>\n",
       "      <td>1770_An_Inquiry_into_the_</td>\n",
       "      <td>1770_1770_An_Inquiry_into_the__005</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>which some magnify the importance of that indu...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>An Inquiry into the Nature and Causes of the W...</td>\n",
       "      <td>1770_An_Inquiry_into_the_</td>\n",
       "      <td>1770_1770_An_Inquiry_into_the__006</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>the expenses incumbent on the whole society, a...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>An Inquiry into the Nature and Causes of the W...</td>\n",
       "      <td>1770_An_Inquiry_into_the_</td>\n",
       "      <td>1770_1770_An_Inquiry_into_the__007</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trifling manufactures which are destined to su...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>An Inquiry into the Nature and Causes of the W...</td>\n",
       "      <td>1770_An_Inquiry_into_the_</td>\n",
       "      <td>1770_1770_An_Inquiry_into_the__008</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>machinery employed in it (to the invention of ...</td>\n",
       "      <td>1770</td>\n",
       "      <td>0</td>\n",
       "      <td>An Inquiry into the Nature and Causes of the W...</td>\n",
       "      <td>1770_An_Inquiry_into_the_</td>\n",
       "      <td>1770_1770_An_Inquiry_into_the__009</td>\n",
       "      <td>210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  decade  decade_label  \\\n",
       "0  An Inquiry into the Nature and Causes of the W...    1770             0   \n",
       "1  THE EXPENSE OF MAINTAINING THE NATIONAL CAPITA...    1770             0   \n",
       "2  PRODUCE OF LAND, AS EITHER THE SOLE OR THE PRI...    1770             0   \n",
       "3  Whatever be the soil, climate, or extent of te...    1770             0   \n",
       "4  of those who work; yet the produce of the whol...    1770             0   \n",
       "5  of capital stock, of the manner in which it is...    1770             0   \n",
       "6  which some magnify the importance of that indu...    1770             0   \n",
       "7  the expenses incumbent on the whole society, a...    1770             0   \n",
       "8  trifling manufactures which are destined to su...    1770             0   \n",
       "9  machinery employed in it (to the invention of ...    1770             0   \n",
       "\n",
       "                                          book_title  \\\n",
       "0  An Inquiry into the Nature and Causes of the W...   \n",
       "1  An Inquiry into the Nature and Causes of the W...   \n",
       "2  An Inquiry into the Nature and Causes of the W...   \n",
       "3  An Inquiry into the Nature and Causes of the W...   \n",
       "4  An Inquiry into the Nature and Causes of the W...   \n",
       "5  An Inquiry into the Nature and Causes of the W...   \n",
       "6  An Inquiry into the Nature and Causes of the W...   \n",
       "7  An Inquiry into the Nature and Causes of the W...   \n",
       "8  An Inquiry into the Nature and Causes of the W...   \n",
       "9  An Inquiry into the Nature and Causes of the W...   \n",
       "\n",
       "                     book_id                        paragraph_id  word_count  \n",
       "0  1770_An_Inquiry_into_the_  1770_1770_An_Inquiry_into_the__000         210  \n",
       "1  1770_An_Inquiry_into_the_  1770_1770_An_Inquiry_into_the__001         210  \n",
       "2  1770_An_Inquiry_into_the_  1770_1770_An_Inquiry_into_the__002         210  \n",
       "3  1770_An_Inquiry_into_the_  1770_1770_An_Inquiry_into_the__003         210  \n",
       "4  1770_An_Inquiry_into_the_  1770_1770_An_Inquiry_into_the__004         210  \n",
       "5  1770_An_Inquiry_into_the_  1770_1770_An_Inquiry_into_the__005         210  \n",
       "6  1770_An_Inquiry_into_the_  1770_1770_An_Inquiry_into_the__006         210  \n",
       "7  1770_An_Inquiry_into_the_  1770_1770_An_Inquiry_into_the__007         210  \n",
       "8  1770_An_Inquiry_into_the_  1770_1770_An_Inquiry_into_the__008         210  \n",
       "9  1770_An_Inquiry_into_the_  1770_1770_An_Inquiry_into_the__009         210  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(test_data_path)\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca452fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25538 entries, 0 to 25537\n",
      "Data columns (total 7 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   text          25538 non-null  object\n",
      " 1   decade        25538 non-null  int64 \n",
      " 2   decade_label  25538 non-null  int64 \n",
      " 3   book_title    25538 non-null  object\n",
      " 4   book_id       25538 non-null  object\n",
      " 5   paragraph_id  25538 non-null  object\n",
      " 6   word_count    25538 non-null  int64 \n",
      "dtypes: int64(3), object(4)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3e026e",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96dc7ef",
   "metadata": {},
   "source": [
    "### NLTK Tokenizer\n",
    "This code was adapted from Professor Johan Boye's DD2417 assignment tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce87541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/pranavrajan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download(\"punkt_tab\")\n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "\n",
    "class HistoricalTextTokenizer:\n",
    "    \"\"\"\n",
    "    All of this code is adapted from Professor Johan Boye's DD2417 assignment tokenizers \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2id = defaultdict(lambda: None)\n",
    "        self.id2word = defaultdict(lambda: None)\n",
    "        self.latest_new_word = -1 \n",
    "        self.tokens_processed = 0 \n",
    "\n",
    "        self.UNKNOWN = '<unk>'\n",
    "        self.PADDING_WORD = '<pad>'\n",
    "\n",
    "        self.get_word_id(self.PADDING_WORD)\n",
    "        self.get_word_id(self.UNKNOWN)\n",
    "\n",
    "    def get_word_id(self, word):\n",
    "        word = word.lower()\n",
    "        if word in self.word2id:\n",
    "            return self.word2id[word]\n",
    "        else:\n",
    "            self.latest_new_word += 1\n",
    "            self.id2word[self.latest_new_word] = word\n",
    "            self.word2id[word] = self.latest_new_word\n",
    "            return self.latest_new_word\n",
    "\n",
    "    def process_files(self, file_or_dir):\n",
    "        all_texts = []\n",
    "        all_labels = []\n",
    "\n",
    "        if os.path.isdir(file_or_dir):\n",
    "            decade_dirs = sorted([d for d in os.listdir(file_or_dir) if os.path.isdir(os.path.join(file_or_dir, d))])\n",
    "            for decade_dir in decade_dirs:\n",
    "                decade_path = os.path.join(file_or_dir, decade_dir)\n",
    "                decade = int(decade_dir)\n",
    "                print(f\"Processing decade: {decade}\")\n",
    "                text_files = sorted([f for f in os.listdir(decade_path) if f.endswith(\".txt\")])\n",
    "                print(f\"number of files in {decade} directory: {len(text_files)}\")\n",
    "\n",
    "                for file in text_files:\n",
    "                    filepath = os.path.join(decade_path, file)\n",
    "                    print(f\"tokenize file {file}\")\n",
    "                    text, labels = self.process_file(filepath, decade)\n",
    "                    all_texts.extend(text)\n",
    "                    all_labels.extend(labels)\n",
    "        else:\n",
    "            texts, labels = self.process_file(file_or_dir, 0)\n",
    "            all_texts.extend(texts)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "        return all_texts, all_labels\n",
    "        # pass\n",
    "\n",
    "    def process_file(self, filepath, decade):\n",
    "        print(filepath)\n",
    "        stream = open(filepath, mode=\"r\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "        text = stream.read()\n",
    "        stream.close()\n",
    "\n",
    "        try:\n",
    "            self.tokens = nltk.word_tokenize(text)\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt\")\n",
    "            self.tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        for i, token in enumerate(self.tokens):\n",
    "            self.tokens_processed += 1\n",
    "            word_id = self.get_word_id(token)\n",
    "\n",
    "            if self.tokens_processed % 10000 == 0:\n",
    "                print(\"Processed\", \"{:,}\".format(self.tokens_processed), \"tokens\")\n",
    "\n",
    "        paragraphs = self.create_paragraphs(text)\n",
    "        labels = [decade] * len(paragraphs)\n",
    "\n",
    "        return paragraphs, labels\n",
    "        # pass\n",
    "\n",
    "    def create_paragraphs(self, text, min_words=10, max_words=210):\n",
    "        words = text.split()\n",
    "        paragraphs = []\n",
    "        start = 0\n",
    "\n",
    "        while start < len(words):\n",
    "            end = min(start + max_words, len(words))\n",
    "            paragraph_words = words[start:end]\n",
    "            if len(paragraph_words) >= min_words:\n",
    "                paragraph_text = \" \".join(paragraph_words)\n",
    "                paragraphs.append(paragraph_text)\n",
    "            start = end\n",
    "\n",
    "        return paragraphs \n",
    "        # pass\n",
    "\n",
    "    def tokenize_text_to_id(self, text):\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt\")\n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "        word_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.word2id:\n",
    "                word_ids.append(self.word2id[token])\n",
    "            else:\n",
    "                word_ids.append(self.word2id[self.UNKNOWN])\n",
    "        return word_ids\n",
    "\n",
    "        # pass\n",
    "\n",
    "    def pad_sequence_to_length(self, word_ids, max_length=220):\n",
    "        padding_id = self.word2id[self.PADDING_WORD]\n",
    "        if len(word_ids) > max_length:\n",
    "            word_ids = word_ids[:max_length]\n",
    "\n",
    "        while len(word_ids) < max_length:\n",
    "            word_ids.append(padding_id)\n",
    "        return word_ids\n",
    "        # pass\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.word2id)\n",
    "        # pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e98180",
   "metadata": {},
   "source": [
    "### Tokenize Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cccc4675",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = HistoricalTextTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e903fa",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87dede51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing decade: 1770\n",
      "number of files in 1770 directory: 9\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_train_split/1770/book_03.txt\n",
      "Processed 10,000 tokens\n",
      "Processed 20,000 tokens\n",
      "Processed 30,000 tokens\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_train_split/1770/book_04.txt\n",
      "Processed 40,000 tokens\n",
      "Processed 50,000 tokens\n",
      "Processed 60,000 tokens\n",
      "Processed 70,000 tokens\n",
      "Processed 80,000 tokens\n",
      "Processed 90,000 tokens\n",
      "Processed 100,000 tokens\n",
      "Processed 110,000 tokens\n",
      "Processed 120,000 tokens\n",
      "Processed 130,000 tokens\n",
      "Processed 140,000 tokens\n",
      "Processed 150,000 tokens\n",
      "Processed 160,000 tokens\n",
      "Processed 170,000 tokens\n",
      "Processed 180,000 tokens\n",
      "Processed 190,000 tokens\n",
      "Processed 200,000 tokens\n",
      "Processed 210,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_train_split/1770/book_05.txt\n",
      "Processed 220,000 tokens\n",
      "Processed 230,000 tokens\n",
      "Processed 240,000 tokens\n",
      "Processed 250,000 tokens\n",
      "Processed 260,000 tokens\n",
      "Processed 270,000 tokens\n",
      "Processed 280,000 tokens\n",
      "Processed 290,000 tokens\n",
      "Processed 300,000 tokens\n",
      "Processed 310,000 tokens\n",
      "Processed 320,000 tokens\n",
      "Processed 330,000 tokens\n",
      "Processed 340,000 tokens\n",
      "Processed 350,000 tokens\n",
      "Processed 360,000 tokens\n",
      "Processed 370,000 tokens\n",
      "Processed 380,000 tokens\n",
      "Processed 390,000 tokens\n",
      "Processed 400,000 tokens\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_train_split/1770/book_06.txt\n",
      "Processed 410,000 tokens\n",
      "Processed 420,000 tokens\n",
      "Processed 430,000 tokens\n",
      "Processed 440,000 tokens\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_train_split/1770/book_07.txt\n",
      "Processed 450,000 tokens\n",
      "Processed 460,000 tokens\n",
      "Processed 470,000 tokens\n",
      "Processed 480,000 tokens\n",
      "Processed 490,000 tokens\n",
      "Processed 500,000 tokens\n",
      "Processed 510,000 tokens\n",
      "Processed 520,000 tokens\n",
      "Processed 530,000 tokens\n",
      "Processed 540,000 tokens\n",
      "Processed 550,000 tokens\n",
      "Processed 560,000 tokens\n",
      "Processed 570,000 tokens\n",
      "Processed 580,000 tokens\n",
      "Processed 590,000 tokens\n",
      "Processed 600,000 tokens\n",
      "Processed 610,000 tokens\n",
      "Processed 620,000 tokens\n",
      "Processed 630,000 tokens\n",
      "Processed 640,000 tokens\n",
      "Processed 650,000 tokens\n",
      "Processed 660,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_train_split/1770/book_08.txt\n",
      "Processed 670,000 tokens\n",
      "Processed 680,000 tokens\n",
      "Processed 690,000 tokens\n",
      "Processed 700,000 tokens\n",
      "Processed 710,000 tokens\n",
      "Processed 720,000 tokens\n",
      "Processed 730,000 tokens\n",
      "Processed 740,000 tokens\n",
      "Processed 750,000 tokens\n",
      "Processed 760,000 tokens\n",
      "Processed 770,000 tokens\n",
      "Processed 780,000 tokens\n",
      "Processed 790,000 tokens\n",
      "Processed 800,000 tokens\n",
      "Processed 810,000 tokens\n",
      "Processed 820,000 tokens\n",
      "Processed 830,000 tokens\n",
      "Processed 840,000 tokens\n",
      "Processed 850,000 tokens\n",
      "Processed 860,000 tokens\n",
      "Processed 870,000 tokens\n",
      "Processed 880,000 tokens\n",
      "Processed 890,000 tokens\n",
      "Processed 900,000 tokens\n",
      "Processed 910,000 tokens\n",
      "Processed 920,000 tokens\n",
      "Processed 930,000 tokens\n",
      "Processed 940,000 tokens\n",
      "Processed 950,000 tokens\n",
      "Processed 960,000 tokens\n",
      "Processed 970,000 tokens\n",
      "Processed 980,000 tokens\n",
      "Processed 990,000 tokens\n",
      "Processed 1,000,000 tokens\n",
      "Processed 1,010,000 tokens\n",
      "Processed 1,020,000 tokens\n",
      "Processed 1,030,000 tokens\n",
      "Processed 1,040,000 tokens\n",
      "Processed 1,050,000 tokens\n",
      "Processed 1,060,000 tokens\n",
      "Processed 1,070,000 tokens\n",
      "Processed 1,080,000 tokens\n",
      "Processed 1,090,000 tokens\n",
      "Processed 1,100,000 tokens\n",
      "Processed 1,110,000 tokens\n",
      "Processed 1,120,000 tokens\n",
      "Processed 1,130,000 tokens\n",
      "Processed 1,140,000 tokens\n",
      "Processed 1,150,000 tokens\n",
      "Processed 1,160,000 tokens\n",
      "Processed 1,170,000 tokens\n",
      "Processed 1,180,000 tokens\n",
      "Processed 1,190,000 tokens\n",
      "Processed 1,200,000 tokens\n",
      "Processed 1,210,000 tokens\n",
      "Processed 1,220,000 tokens\n",
      "Processed 1,230,000 tokens\n",
      "Processed 1,240,000 tokens\n",
      "Processed 1,250,000 tokens\n",
      "Processed 1,260,000 tokens\n",
      "Processed 1,270,000 tokens\n",
      "Processed 1,280,000 tokens\n",
      "Processed 1,290,000 tokens\n",
      "Processed 1,300,000 tokens\n",
      "Processed 1,310,000 tokens\n",
      "Processed 1,320,000 tokens\n",
      "Processed 1,330,000 tokens\n",
      "Processed 1,340,000 tokens\n",
      "Processed 1,350,000 tokens\n",
      "Processed 1,360,000 tokens\n",
      "Processed 1,370,000 tokens\n",
      "Processed 1,380,000 tokens\n",
      "Processed 1,390,000 tokens\n",
      "Processed 1,400,000 tokens\n",
      "Processed 1,410,000 tokens\n",
      "Processed 1,420,000 tokens\n",
      "Processed 1,430,000 tokens\n",
      "Processed 1,440,000 tokens\n",
      "Processed 1,450,000 tokens\n",
      "Processed 1,460,000 tokens\n",
      "Processed 1,470,000 tokens\n",
      "Processed 1,480,000 tokens\n",
      "Processed 1,490,000 tokens\n",
      "Processed 1,500,000 tokens\n",
      "Processed 1,510,000 tokens\n",
      "Processed 1,520,000 tokens\n",
      "Processed 1,530,000 tokens\n",
      "Processed 1,540,000 tokens\n",
      "Processed 1,550,000 tokens\n",
      "Processed 1,560,000 tokens\n",
      "Processed 1,570,000 tokens\n",
      "Processed 1,580,000 tokens\n",
      "Processed 1,590,000 tokens\n",
      "Processed 1,600,000 tokens\n",
      "Processed 1,610,000 tokens\n",
      "Processed 1,620,000 tokens\n",
      "Processed 1,630,000 tokens\n",
      "Processed 1,640,000 tokens\n",
      "Processed 1,650,000 tokens\n",
      "Processed 1,660,000 tokens\n",
      "Processed 1,670,000 tokens\n",
      "Processed 1,680,000 tokens\n",
      "Processed 1,690,000 tokens\n",
      "Processed 1,700,000 tokens\n",
      "Processed 1,710,000 tokens\n",
      "Processed 1,720,000 tokens\n",
      "Processed 1,730,000 tokens\n",
      "Processed 1,740,000 tokens\n",
      "Processed 1,750,000 tokens\n",
      "Processed 1,760,000 tokens\n",
      "Processed 1,770,000 tokens\n",
      "Processed 1,780,000 tokens\n",
      "Processed 1,790,000 tokens\n",
      "Processed 1,800,000 tokens\n",
      "Processed 1,810,000 tokens\n",
      "Processed 1,820,000 tokens\n",
      "Processed 1,830,000 tokens\n",
      "Processed 1,840,000 tokens\n",
      "Processed 1,850,000 tokens\n",
      "Processed 1,860,000 tokens\n",
      "Processed 1,870,000 tokens\n",
      "Processed 1,880,000 tokens\n",
      "Processed 1,890,000 tokens\n",
      "Processed 1,900,000 tokens\n",
      "Processed 1,910,000 tokens\n",
      "Processed 1,920,000 tokens\n",
      "Processed 1,930,000 tokens\n",
      "Processed 1,940,000 tokens\n",
      "Processed 1,950,000 tokens\n",
      "Processed 1,960,000 tokens\n",
      "Processed 1,970,000 tokens\n",
      "Processed 1,980,000 tokens\n",
      "Processed 1,990,000 tokens\n",
      "Processed 2,000,000 tokens\n",
      "Processed 2,010,000 tokens\n",
      "Processed 2,020,000 tokens\n",
      "Processed 2,030,000 tokens\n",
      "Processed 2,040,000 tokens\n",
      "Processed 2,050,000 tokens\n",
      "Processed 2,060,000 tokens\n",
      "Processed 2,070,000 tokens\n",
      "Processed 2,080,000 tokens\n",
      "Processed 2,090,000 tokens\n",
      "Processed 2,100,000 tokens\n",
      "Processed 2,110,000 tokens\n",
      "Processed 2,120,000 tokens\n",
      "Processed 2,130,000 tokens\n",
      "Processed 2,140,000 tokens\n",
      "Processed 2,150,000 tokens\n",
      "Processed 2,160,000 tokens\n",
      "Processed 2,170,000 tokens\n",
      "Processed 2,180,000 tokens\n",
      "Processed 2,190,000 tokens\n",
      "Processed 2,200,000 tokens\n",
      "Processed 2,210,000 tokens\n",
      "Processed 2,220,000 tokens\n",
      "Processed 2,230,000 tokens\n",
      "Processed 2,240,000 tokens\n",
      "Processed 2,250,000 tokens\n",
      "Processed 2,260,000 tokens\n",
      "Processed 2,270,000 tokens\n",
      "Processed 2,280,000 tokens\n",
      "Processed 2,290,000 tokens\n",
      "Processed 2,300,000 tokens\n",
      "Processed 2,310,000 tokens\n",
      "Processed 2,320,000 tokens\n",
      "Processed 2,330,000 tokens\n",
      "Processed 2,340,000 tokens\n",
      "Processed 2,350,000 tokens\n",
      "Processed 2,360,000 tokens\n",
      "Processed 2,370,000 tokens\n",
      "Processed 2,380,000 tokens\n",
      "Processed 2,390,000 tokens\n",
      "Processed 2,400,000 tokens\n",
      "Processed 2,410,000 tokens\n",
      "Processed 2,420,000 tokens\n",
      "Processed 2,430,000 tokens\n",
      "Processed 2,440,000 tokens\n",
      "Processed 2,450,000 tokens\n",
      "Processed 2,460,000 tokens\n",
      "Processed 2,470,000 tokens\n",
      "Processed 2,480,000 tokens\n",
      "Processed 2,490,000 tokens\n",
      "Processed 2,500,000 tokens\n",
      "Processed 2,510,000 tokens\n",
      "Processed 2,520,000 tokens\n",
      "Processed 2,530,000 tokens\n",
      "Processed 2,540,000 tokens\n",
      "Processed 2,550,000 tokens\n",
      "Processed 2,560,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_train_split/1770/book_09.txt\n",
      "Processed 2,570,000 tokens\n",
      "Processed 2,580,000 tokens\n",
      "Processed 2,590,000 tokens\n",
      "Processed 2,600,000 tokens\n",
      "Processed 2,610,000 tokens\n",
      "Processed 2,620,000 tokens\n",
      "Processed 2,630,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_train_split/1770/book_10.txt\n",
      "Processed 2,640,000 tokens\n",
      "Processed 2,650,000 tokens\n",
      "tokenize file book_12.txt\n",
      "./Datasets/clean_train_split/1770/book_12.txt\n",
      "Processed 2,660,000 tokens\n",
      "Processing decade: 1780\n",
      "number of files in 1780 directory: 9\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_train_split/1780/book_03.txt\n",
      "Processed 2,670,000 tokens\n",
      "Processed 2,680,000 tokens\n",
      "Processed 2,690,000 tokens\n",
      "Processed 2,700,000 tokens\n",
      "Processed 2,710,000 tokens\n",
      "Processed 2,720,000 tokens\n",
      "Processed 2,730,000 tokens\n",
      "Processed 2,740,000 tokens\n",
      "Processed 2,750,000 tokens\n",
      "Processed 2,760,000 tokens\n",
      "Processed 2,770,000 tokens\n",
      "Processed 2,780,000 tokens\n",
      "Processed 2,790,000 tokens\n",
      "Processed 2,800,000 tokens\n",
      "Processed 2,810,000 tokens\n",
      "Processed 2,820,000 tokens\n",
      "Processed 2,830,000 tokens\n",
      "Processed 2,840,000 tokens\n",
      "Processed 2,850,000 tokens\n",
      "Processed 2,860,000 tokens\n",
      "Processed 2,870,000 tokens\n",
      "Processed 2,880,000 tokens\n",
      "Processed 2,890,000 tokens\n",
      "Processed 2,900,000 tokens\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_train_split/1780/book_04.txt\n",
      "Processed 2,910,000 tokens\n",
      "Processed 2,920,000 tokens\n",
      "Processed 2,930,000 tokens\n",
      "Processed 2,940,000 tokens\n",
      "Processed 2,950,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_train_split/1780/book_05.txt\n",
      "Processed 2,960,000 tokens\n",
      "Processed 2,970,000 tokens\n",
      "Processed 2,980,000 tokens\n",
      "Processed 2,990,000 tokens\n",
      "Processed 3,000,000 tokens\n",
      "Processed 3,010,000 tokens\n",
      "Processed 3,020,000 tokens\n",
      "Processed 3,030,000 tokens\n",
      "Processed 3,040,000 tokens\n",
      "Processed 3,050,000 tokens\n",
      "Processed 3,060,000 tokens\n",
      "Processed 3,070,000 tokens\n",
      "Processed 3,080,000 tokens\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_train_split/1780/book_06.txt\n",
      "Processed 3,090,000 tokens\n",
      "Processed 3,100,000 tokens\n",
      "Processed 3,110,000 tokens\n",
      "Processed 3,120,000 tokens\n",
      "Processed 3,130,000 tokens\n",
      "Processed 3,140,000 tokens\n",
      "Processed 3,150,000 tokens\n",
      "Processed 3,160,000 tokens\n",
      "Processed 3,170,000 tokens\n",
      "Processed 3,180,000 tokens\n",
      "Processed 3,190,000 tokens\n",
      "Processed 3,200,000 tokens\n",
      "Processed 3,210,000 tokens\n",
      "Processed 3,220,000 tokens\n",
      "Processed 3,230,000 tokens\n",
      "Processed 3,240,000 tokens\n",
      "Processed 3,250,000 tokens\n",
      "Processed 3,260,000 tokens\n",
      "Processed 3,270,000 tokens\n",
      "Processed 3,280,000 tokens\n",
      "Processed 3,290,000 tokens\n",
      "Processed 3,300,000 tokens\n",
      "Processed 3,310,000 tokens\n",
      "Processed 3,320,000 tokens\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_train_split/1780/book_07.txt\n",
      "Processed 3,330,000 tokens\n",
      "Processed 3,340,000 tokens\n",
      "Processed 3,350,000 tokens\n",
      "Processed 3,360,000 tokens\n",
      "Processed 3,370,000 tokens\n",
      "Processed 3,380,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_train_split/1780/book_08.txt\n",
      "Processed 3,390,000 tokens\n",
      "Processed 3,400,000 tokens\n",
      "Processed 3,410,000 tokens\n",
      "Processed 3,420,000 tokens\n",
      "Processed 3,430,000 tokens\n",
      "Processed 3,440,000 tokens\n",
      "Processed 3,450,000 tokens\n",
      "Processed 3,460,000 tokens\n",
      "Processed 3,470,000 tokens\n",
      "Processed 3,480,000 tokens\n",
      "Processed 3,490,000 tokens\n",
      "Processed 3,500,000 tokens\n",
      "Processed 3,510,000 tokens\n",
      "Processed 3,520,000 tokens\n",
      "Processed 3,530,000 tokens\n",
      "Processed 3,540,000 tokens\n",
      "Processed 3,550,000 tokens\n",
      "Processed 3,560,000 tokens\n",
      "Processed 3,570,000 tokens\n",
      "Processed 3,580,000 tokens\n",
      "Processed 3,590,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_train_split/1780/book_09.txt\n",
      "Processed 3,600,000 tokens\n",
      "Processed 3,610,000 tokens\n",
      "Processed 3,620,000 tokens\n",
      "Processed 3,630,000 tokens\n",
      "Processed 3,640,000 tokens\n",
      "Processed 3,650,000 tokens\n",
      "Processed 3,660,000 tokens\n",
      "Processed 3,670,000 tokens\n",
      "Processed 3,680,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_train_split/1780/book_10.txt\n",
      "Processed 3,690,000 tokens\n",
      "Processed 3,700,000 tokens\n",
      "Processed 3,710,000 tokens\n",
      "Processed 3,720,000 tokens\n",
      "Processed 3,730,000 tokens\n",
      "Processed 3,740,000 tokens\n",
      "Processed 3,750,000 tokens\n",
      "Processed 3,760,000 tokens\n",
      "Processed 3,770,000 tokens\n",
      "Processed 3,780,000 tokens\n",
      "tokenize file book_11.txt\n",
      "./Datasets/clean_train_split/1780/book_11.txt\n",
      "Processed 3,790,000 tokens\n",
      "Processed 3,800,000 tokens\n",
      "Processed 3,810,000 tokens\n",
      "Processed 3,820,000 tokens\n",
      "Processed 3,830,000 tokens\n",
      "Processed 3,840,000 tokens\n",
      "Processed 3,850,000 tokens\n",
      "Processed 3,860,000 tokens\n",
      "Processed 3,870,000 tokens\n",
      "Processed 3,880,000 tokens\n",
      "Processing decade: 1790\n",
      "number of files in 1790 directory: 8\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_train_split/1790/book_01.txt\n",
      "Processed 3,890,000 tokens\n",
      "Processed 3,900,000 tokens\n",
      "Processed 3,910,000 tokens\n",
      "Processed 3,920,000 tokens\n",
      "Processed 3,930,000 tokens\n",
      "Processed 3,940,000 tokens\n",
      "Processed 3,950,000 tokens\n",
      "Processed 3,960,000 tokens\n",
      "Processed 3,970,000 tokens\n",
      "Processed 3,980,000 tokens\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_train_split/1790/book_02.txt\n",
      "Processed 3,990,000 tokens\n",
      "Processed 4,000,000 tokens\n",
      "Processed 4,010,000 tokens\n",
      "Processed 4,020,000 tokens\n",
      "Processed 4,030,000 tokens\n",
      "Processed 4,040,000 tokens\n",
      "Processed 4,050,000 tokens\n",
      "Processed 4,060,000 tokens\n",
      "Processed 4,070,000 tokens\n",
      "Processed 4,080,000 tokens\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_train_split/1790/book_03.txt\n",
      "Processed 4,090,000 tokens\n",
      "Processed 4,100,000 tokens\n",
      "Processed 4,110,000 tokens\n",
      "Processed 4,120,000 tokens\n",
      "Processed 4,130,000 tokens\n",
      "Processed 4,140,000 tokens\n",
      "Processed 4,150,000 tokens\n",
      "Processed 4,160,000 tokens\n",
      "Processed 4,170,000 tokens\n",
      "Processed 4,180,000 tokens\n",
      "Processed 4,190,000 tokens\n",
      "Processed 4,200,000 tokens\n",
      "Processed 4,210,000 tokens\n",
      "Processed 4,220,000 tokens\n",
      "Processed 4,230,000 tokens\n",
      "Processed 4,240,000 tokens\n",
      "Processed 4,250,000 tokens\n",
      "Processed 4,260,000 tokens\n",
      "Processed 4,270,000 tokens\n",
      "Processed 4,280,000 tokens\n",
      "Processed 4,290,000 tokens\n",
      "Processed 4,300,000 tokens\n",
      "Processed 4,310,000 tokens\n",
      "Processed 4,320,000 tokens\n",
      "Processed 4,330,000 tokens\n",
      "Processed 4,340,000 tokens\n",
      "Processed 4,350,000 tokens\n",
      "Processed 4,360,000 tokens\n",
      "Processed 4,370,000 tokens\n",
      "Processed 4,380,000 tokens\n",
      "Processed 4,390,000 tokens\n",
      "Processed 4,400,000 tokens\n",
      "Processed 4,410,000 tokens\n",
      "Processed 4,420,000 tokens\n",
      "Processed 4,430,000 tokens\n",
      "Processed 4,440,000 tokens\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_train_split/1790/book_06.txt\n",
      "Processed 4,450,000 tokens\n",
      "Processed 4,460,000 tokens\n",
      "Processed 4,470,000 tokens\n",
      "Processed 4,480,000 tokens\n",
      "Processed 4,490,000 tokens\n",
      "Processed 4,500,000 tokens\n",
      "Processed 4,510,000 tokens\n",
      "Processed 4,520,000 tokens\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_train_split/1790/book_07.txt\n",
      "Processed 4,530,000 tokens\n",
      "Processed 4,540,000 tokens\n",
      "Processed 4,550,000 tokens\n",
      "Processed 4,560,000 tokens\n",
      "Processed 4,570,000 tokens\n",
      "Processed 4,580,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_train_split/1790/book_09.txt\n",
      "Processed 4,590,000 tokens\n",
      "Processed 4,600,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_train_split/1790/book_10.txt\n",
      "Processed 4,610,000 tokens\n",
      "Processed 4,620,000 tokens\n",
      "Processed 4,630,000 tokens\n",
      "Processed 4,640,000 tokens\n",
      "Processed 4,650,000 tokens\n",
      "Processed 4,660,000 tokens\n",
      "Processed 4,670,000 tokens\n",
      "Processed 4,680,000 tokens\n",
      "Processed 4,690,000 tokens\n",
      "Processed 4,700,000 tokens\n",
      "Processed 4,710,000 tokens\n",
      "Processed 4,720,000 tokens\n",
      "Processed 4,730,000 tokens\n",
      "Processed 4,740,000 tokens\n",
      "Processed 4,750,000 tokens\n",
      "Processed 4,760,000 tokens\n",
      "Processed 4,770,000 tokens\n",
      "Processed 4,780,000 tokens\n",
      "Processed 4,790,000 tokens\n",
      "Processed 4,800,000 tokens\n",
      "Processed 4,810,000 tokens\n",
      "Processed 4,820,000 tokens\n",
      "Processed 4,830,000 tokens\n",
      "Processed 4,840,000 tokens\n",
      "Processed 4,850,000 tokens\n",
      "Processed 4,860,000 tokens\n",
      "Processed 4,870,000 tokens\n",
      "Processed 4,880,000 tokens\n",
      "Processed 4,890,000 tokens\n",
      "Processed 4,900,000 tokens\n",
      "Processed 4,910,000 tokens\n",
      "Processed 4,920,000 tokens\n",
      "Processed 4,930,000 tokens\n",
      "Processed 4,940,000 tokens\n",
      "Processed 4,950,000 tokens\n",
      "Processed 4,960,000 tokens\n",
      "Processed 4,970,000 tokens\n",
      "Processed 4,980,000 tokens\n",
      "Processed 4,990,000 tokens\n",
      "Processed 5,000,000 tokens\n",
      "Processed 5,010,000 tokens\n",
      "Processed 5,020,000 tokens\n",
      "Processed 5,030,000 tokens\n",
      "tokenize file book_11.txt\n",
      "./Datasets/clean_train_split/1790/book_11.txt\n",
      "Processed 5,040,000 tokens\n",
      "Processed 5,050,000 tokens\n",
      "Processed 5,060,000 tokens\n",
      "Processed 5,070,000 tokens\n",
      "Processed 5,080,000 tokens\n",
      "Processed 5,090,000 tokens\n",
      "Processed 5,100,000 tokens\n",
      "Processed 5,110,000 tokens\n",
      "Processed 5,120,000 tokens\n",
      "Processed 5,130,000 tokens\n",
      "Processed 5,140,000 tokens\n",
      "Processed 5,150,000 tokens\n",
      "Processed 5,160,000 tokens\n",
      "Processed 5,170,000 tokens\n",
      "Processed 5,180,000 tokens\n",
      "Processed 5,190,000 tokens\n",
      "Processed 5,200,000 tokens\n",
      "Processed 5,210,000 tokens\n",
      "Processed 5,220,000 tokens\n",
      "Processed 5,230,000 tokens\n",
      "Processed 5,240,000 tokens\n",
      "Processed 5,250,000 tokens\n",
      "Processed 5,260,000 tokens\n",
      "Processed 5,270,000 tokens\n",
      "Processed 5,280,000 tokens\n",
      "Processed 5,290,000 tokens\n",
      "Processing decade: 1800\n",
      "number of files in 1800 directory: 8\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_train_split/1800/book_01.txt\n",
      "Processed 5,300,000 tokens\n",
      "Processed 5,310,000 tokens\n",
      "Processed 5,320,000 tokens\n",
      "Processed 5,330,000 tokens\n",
      "Processed 5,340,000 tokens\n",
      "Processed 5,350,000 tokens\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_train_split/1800/book_02.txt\n",
      "Processed 5,360,000 tokens\n",
      "Processed 5,370,000 tokens\n",
      "Processed 5,380,000 tokens\n",
      "Processed 5,390,000 tokens\n",
      "Processed 5,400,000 tokens\n",
      "Processed 5,410,000 tokens\n",
      "Processed 5,420,000 tokens\n",
      "Processed 5,430,000 tokens\n",
      "Processed 5,440,000 tokens\n",
      "Processed 5,450,000 tokens\n",
      "Processed 5,460,000 tokens\n",
      "Processed 5,470,000 tokens\n",
      "Processed 5,480,000 tokens\n",
      "Processed 5,490,000 tokens\n",
      "Processed 5,500,000 tokens\n",
      "Processed 5,510,000 tokens\n",
      "Processed 5,520,000 tokens\n",
      "Processed 5,530,000 tokens\n",
      "Processed 5,540,000 tokens\n",
      "Processed 5,550,000 tokens\n",
      "Processed 5,560,000 tokens\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_train_split/1800/book_03.txt\n",
      "Processed 5,570,000 tokens\n",
      "Processed 5,580,000 tokens\n",
      "Processed 5,590,000 tokens\n",
      "Processed 5,600,000 tokens\n",
      "Processed 5,610,000 tokens\n",
      "Processed 5,620,000 tokens\n",
      "Processed 5,630,000 tokens\n",
      "Processed 5,640,000 tokens\n",
      "Processed 5,650,000 tokens\n",
      "Processed 5,660,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_train_split/1800/book_05.txt\n",
      "Processed 5,670,000 tokens\n",
      "Processed 5,680,000 tokens\n",
      "Processed 5,690,000 tokens\n",
      "Processed 5,700,000 tokens\n",
      "Processed 5,710,000 tokens\n",
      "Processed 5,720,000 tokens\n",
      "Processed 5,730,000 tokens\n",
      "Processed 5,740,000 tokens\n",
      "Processed 5,750,000 tokens\n",
      "Processed 5,760,000 tokens\n",
      "Processed 5,770,000 tokens\n",
      "Processed 5,780,000 tokens\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_train_split/1800/book_07.txt\n",
      "Processed 5,790,000 tokens\n",
      "Processed 5,800,000 tokens\n",
      "Processed 5,810,000 tokens\n",
      "Processed 5,820,000 tokens\n",
      "Processed 5,830,000 tokens\n",
      "Processed 5,840,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_train_split/1800/book_08.txt\n",
      "Processed 5,850,000 tokens\n",
      "Processed 5,860,000 tokens\n",
      "Processed 5,870,000 tokens\n",
      "Processed 5,880,000 tokens\n",
      "Processed 5,890,000 tokens\n",
      "Processed 5,900,000 tokens\n",
      "Processed 5,910,000 tokens\n",
      "Processed 5,920,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_train_split/1800/book_09.txt\n",
      "Processed 5,930,000 tokens\n",
      "Processed 5,940,000 tokens\n",
      "Processed 5,950,000 tokens\n",
      "Processed 5,960,000 tokens\n",
      "Processed 5,970,000 tokens\n",
      "Processed 5,980,000 tokens\n",
      "Processed 5,990,000 tokens\n",
      "Processed 6,000,000 tokens\n",
      "Processed 6,010,000 tokens\n",
      "Processed 6,020,000 tokens\n",
      "Processed 6,030,000 tokens\n",
      "Processed 6,040,000 tokens\n",
      "Processed 6,050,000 tokens\n",
      "Processed 6,060,000 tokens\n",
      "Processed 6,070,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_train_split/1800/book_10.txt\n",
      "Processed 6,080,000 tokens\n",
      "Processed 6,090,000 tokens\n",
      "Processed 6,100,000 tokens\n",
      "Processed 6,110,000 tokens\n",
      "Processed 6,120,000 tokens\n",
      "Processed 6,130,000 tokens\n",
      "Processed 6,140,000 tokens\n",
      "Processed 6,150,000 tokens\n",
      "Processed 6,160,000 tokens\n",
      "Processed 6,170,000 tokens\n",
      "Processed 6,180,000 tokens\n",
      "Processed 6,190,000 tokens\n",
      "Processed 6,200,000 tokens\n",
      "Processed 6,210,000 tokens\n",
      "Processed 6,220,000 tokens\n",
      "Processed 6,230,000 tokens\n",
      "Processing decade: 1810\n",
      "number of files in 1810 directory: 12\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_train_split/1810/book_02.txt\n",
      "Processed 6,240,000 tokens\n",
      "Processed 6,250,000 tokens\n",
      "Processed 6,260,000 tokens\n",
      "Processed 6,270,000 tokens\n",
      "Processed 6,280,000 tokens\n",
      "Processed 6,290,000 tokens\n",
      "Processed 6,300,000 tokens\n",
      "Processed 6,310,000 tokens\n",
      "Processed 6,320,000 tokens\n",
      "Processed 6,330,000 tokens\n",
      "Processed 6,340,000 tokens\n",
      "Processed 6,350,000 tokens\n",
      "Processed 6,360,000 tokens\n",
      "Processed 6,370,000 tokens\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_train_split/1810/book_03.txt\n",
      "Processed 6,380,000 tokens\n",
      "Processed 6,390,000 tokens\n",
      "Processed 6,400,000 tokens\n",
      "Processed 6,410,000 tokens\n",
      "Processed 6,420,000 tokens\n",
      "Processed 6,430,000 tokens\n",
      "Processed 6,440,000 tokens\n",
      "Processed 6,450,000 tokens\n",
      "Processed 6,460,000 tokens\n",
      "Processed 6,470,000 tokens\n",
      "Processed 6,480,000 tokens\n",
      "Processed 6,490,000 tokens\n",
      "Processed 6,500,000 tokens\n",
      "Processed 6,510,000 tokens\n",
      "Processed 6,520,000 tokens\n",
      "Processed 6,530,000 tokens\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_train_split/1810/book_04.txt\n",
      "Processed 6,540,000 tokens\n",
      "Processed 6,550,000 tokens\n",
      "Processed 6,560,000 tokens\n",
      "Processed 6,570,000 tokens\n",
      "Processed 6,580,000 tokens\n",
      "Processed 6,590,000 tokens\n",
      "Processed 6,600,000 tokens\n",
      "Processed 6,610,000 tokens\n",
      "Processed 6,620,000 tokens\n",
      "Processed 6,630,000 tokens\n",
      "Processed 6,640,000 tokens\n",
      "Processed 6,650,000 tokens\n",
      "Processed 6,660,000 tokens\n",
      "Processed 6,670,000 tokens\n",
      "Processed 6,680,000 tokens\n",
      "Processed 6,690,000 tokens\n",
      "Processed 6,700,000 tokens\n",
      "Processed 6,710,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_train_split/1810/book_05.txt\n",
      "Processed 6,720,000 tokens\n",
      "Processed 6,730,000 tokens\n",
      "Processed 6,740,000 tokens\n",
      "Processed 6,750,000 tokens\n",
      "Processed 6,760,000 tokens\n",
      "Processed 6,770,000 tokens\n",
      "Processed 6,780,000 tokens\n",
      "Processed 6,790,000 tokens\n",
      "Processed 6,800,000 tokens\n",
      "Processed 6,810,000 tokens\n",
      "Processed 6,820,000 tokens\n",
      "Processed 6,830,000 tokens\n",
      "Processed 6,840,000 tokens\n",
      "Processed 6,850,000 tokens\n",
      "Processed 6,860,000 tokens\n",
      "Processed 6,870,000 tokens\n",
      "Processed 6,880,000 tokens\n",
      "Processed 6,890,000 tokens\n",
      "Processed 6,900,000 tokens\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_train_split/1810/book_06.txt\n",
      "Processed 6,910,000 tokens\n",
      "Processed 6,920,000 tokens\n",
      "Processed 6,930,000 tokens\n",
      "Processed 6,940,000 tokens\n",
      "Processed 6,950,000 tokens\n",
      "Processed 6,960,000 tokens\n",
      "Processed 6,970,000 tokens\n",
      "Processed 6,980,000 tokens\n",
      "Processed 6,990,000 tokens\n",
      "Processed 7,000,000 tokens\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_train_split/1810/book_07.txt\n",
      "Processed 7,010,000 tokens\n",
      "Processed 7,020,000 tokens\n",
      "Processed 7,030,000 tokens\n",
      "Processed 7,040,000 tokens\n",
      "Processed 7,050,000 tokens\n",
      "Processed 7,060,000 tokens\n",
      "Processed 7,070,000 tokens\n",
      "Processed 7,080,000 tokens\n",
      "Processed 7,090,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_train_split/1810/book_08.txt\n",
      "Processed 7,100,000 tokens\n",
      "Processed 7,110,000 tokens\n",
      "Processed 7,120,000 tokens\n",
      "Processed 7,130,000 tokens\n",
      "Processed 7,140,000 tokens\n",
      "Processed 7,150,000 tokens\n",
      "Processed 7,160,000 tokens\n",
      "Processed 7,170,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_train_split/1810/book_09.txt\n",
      "Processed 7,180,000 tokens\n",
      "Processed 7,190,000 tokens\n",
      "Processed 7,200,000 tokens\n",
      "Processed 7,210,000 tokens\n",
      "Processed 7,220,000 tokens\n",
      "Processed 7,230,000 tokens\n",
      "Processed 7,240,000 tokens\n",
      "Processed 7,250,000 tokens\n",
      "Processed 7,260,000 tokens\n",
      "Processed 7,270,000 tokens\n",
      "Processed 7,280,000 tokens\n",
      "Processed 7,290,000 tokens\n",
      "Processed 7,300,000 tokens\n",
      "Processed 7,310,000 tokens\n",
      "Processed 7,320,000 tokens\n",
      "Processed 7,330,000 tokens\n",
      "Processed 7,340,000 tokens\n",
      "Processed 7,350,000 tokens\n",
      "Processed 7,360,000 tokens\n",
      "Processed 7,370,000 tokens\n",
      "Processed 7,380,000 tokens\n",
      "Processed 7,390,000 tokens\n",
      "Processed 7,400,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_train_split/1810/book_10.txt\n",
      "Processed 7,410,000 tokens\n",
      "Processed 7,420,000 tokens\n",
      "Processed 7,430,000 tokens\n",
      "Processed 7,440,000 tokens\n",
      "Processed 7,450,000 tokens\n",
      "Processed 7,460,000 tokens\n",
      "Processed 7,470,000 tokens\n",
      "Processed 7,480,000 tokens\n",
      "Processed 7,490,000 tokens\n",
      "Processed 7,500,000 tokens\n",
      "Processed 7,510,000 tokens\n",
      "Processed 7,520,000 tokens\n",
      "Processed 7,530,000 tokens\n",
      "Processed 7,540,000 tokens\n",
      "Processed 7,550,000 tokens\n",
      "Processed 7,560,000 tokens\n",
      "Processed 7,570,000 tokens\n",
      "Processed 7,580,000 tokens\n",
      "Processed 7,590,000 tokens\n",
      "Processed 7,600,000 tokens\n",
      "Processed 7,610,000 tokens\n",
      "Processed 7,620,000 tokens\n",
      "Processed 7,630,000 tokens\n",
      "tokenize file book_11.txt\n",
      "./Datasets/clean_train_split/1810/book_11.txt\n",
      "Processed 7,640,000 tokens\n",
      "Processed 7,650,000 tokens\n",
      "Processed 7,660,000 tokens\n",
      "Processed 7,670,000 tokens\n",
      "Processed 7,680,000 tokens\n",
      "Processed 7,690,000 tokens\n",
      "Processed 7,700,000 tokens\n",
      "Processed 7,710,000 tokens\n",
      "Processed 7,720,000 tokens\n",
      "Processed 7,730,000 tokens\n",
      "Processed 7,740,000 tokens\n",
      "Processed 7,750,000 tokens\n",
      "Processed 7,760,000 tokens\n",
      "Processed 7,770,000 tokens\n",
      "Processed 7,780,000 tokens\n",
      "Processed 7,790,000 tokens\n",
      "Processed 7,800,000 tokens\n",
      "Processed 7,810,000 tokens\n",
      "Processed 7,820,000 tokens\n",
      "Processed 7,830,000 tokens\n",
      "Processed 7,840,000 tokens\n",
      "Processed 7,850,000 tokens\n",
      "tokenize file book_14.txt\n",
      "./Datasets/clean_train_split/1810/book_14.txt\n",
      "Processed 7,860,000 tokens\n",
      "Processed 7,870,000 tokens\n",
      "Processed 7,880,000 tokens\n",
      "Processed 7,890,000 tokens\n",
      "Processed 7,900,000 tokens\n",
      "Processed 7,910,000 tokens\n",
      "Processed 7,920,000 tokens\n",
      "Processed 7,930,000 tokens\n",
      "Processed 7,940,000 tokens\n",
      "Processed 7,950,000 tokens\n",
      "Processed 7,960,000 tokens\n",
      "tokenize file book_15.txt\n",
      "./Datasets/clean_train_split/1810/book_15.txt\n",
      "Processed 7,970,000 tokens\n",
      "Processed 7,980,000 tokens\n",
      "Processed 7,990,000 tokens\n",
      "Processed 8,000,000 tokens\n",
      "Processed 8,010,000 tokens\n",
      "Processed 8,020,000 tokens\n",
      "Processed 8,030,000 tokens\n",
      "Processed 8,040,000 tokens\n",
      "Processed 8,050,000 tokens\n",
      "Processed 8,060,000 tokens\n",
      "Processed 8,070,000 tokens\n",
      "Processed 8,080,000 tokens\n",
      "Processed 8,090,000 tokens\n",
      "Processed 8,100,000 tokens\n",
      "Processing decade: 1820\n",
      "number of files in 1820 directory: 11\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_train_split/1820/book_03.txt\n",
      "Processed 8,110,000 tokens\n",
      "Processed 8,120,000 tokens\n",
      "Processed 8,130,000 tokens\n",
      "Processed 8,140,000 tokens\n",
      "Processed 8,150,000 tokens\n",
      "Processed 8,160,000 tokens\n",
      "Processed 8,170,000 tokens\n",
      "Processed 8,180,000 tokens\n",
      "Processed 8,190,000 tokens\n",
      "Processed 8,200,000 tokens\n",
      "Processed 8,210,000 tokens\n",
      "Processed 8,220,000 tokens\n",
      "Processed 8,230,000 tokens\n",
      "Processed 8,240,000 tokens\n",
      "Processed 8,250,000 tokens\n",
      "Processed 8,260,000 tokens\n",
      "Processed 8,270,000 tokens\n",
      "Processed 8,280,000 tokens\n",
      "Processed 8,290,000 tokens\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_train_split/1820/book_04.txt\n",
      "Processed 8,300,000 tokens\n",
      "Processed 8,310,000 tokens\n",
      "Processed 8,320,000 tokens\n",
      "Processed 8,330,000 tokens\n",
      "Processed 8,340,000 tokens\n",
      "Processed 8,350,000 tokens\n",
      "Processed 8,360,000 tokens\n",
      "Processed 8,370,000 tokens\n",
      "Processed 8,380,000 tokens\n",
      "Processed 8,390,000 tokens\n",
      "Processed 8,400,000 tokens\n",
      "Processed 8,410,000 tokens\n",
      "Processed 8,420,000 tokens\n",
      "Processed 8,430,000 tokens\n",
      "Processed 8,440,000 tokens\n",
      "Processed 8,450,000 tokens\n",
      "Processed 8,460,000 tokens\n",
      "Processed 8,470,000 tokens\n",
      "Processed 8,480,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_train_split/1820/book_05.txt\n",
      "Processed 8,490,000 tokens\n",
      "Processed 8,500,000 tokens\n",
      "Processed 8,510,000 tokens\n",
      "Processed 8,520,000 tokens\n",
      "Processed 8,530,000 tokens\n",
      "Processed 8,540,000 tokens\n",
      "Processed 8,550,000 tokens\n",
      "Processed 8,560,000 tokens\n",
      "Processed 8,570,000 tokens\n",
      "Processed 8,580,000 tokens\n",
      "Processed 8,590,000 tokens\n",
      "Processed 8,600,000 tokens\n",
      "Processed 8,610,000 tokens\n",
      "Processed 8,620,000 tokens\n",
      "Processed 8,630,000 tokens\n",
      "Processed 8,640,000 tokens\n",
      "Processed 8,650,000 tokens\n",
      "Processed 8,660,000 tokens\n",
      "Processed 8,670,000 tokens\n",
      "Processed 8,680,000 tokens\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_train_split/1820/book_06.txt\n",
      "Processed 8,690,000 tokens\n",
      "Processed 8,700,000 tokens\n",
      "Processed 8,710,000 tokens\n",
      "Processed 8,720,000 tokens\n",
      "Processed 8,730,000 tokens\n",
      "Processed 8,740,000 tokens\n",
      "Processed 8,750,000 tokens\n",
      "Processed 8,760,000 tokens\n",
      "Processed 8,770,000 tokens\n",
      "Processed 8,780,000 tokens\n",
      "Processed 8,790,000 tokens\n",
      "Processed 8,800,000 tokens\n",
      "Processed 8,810,000 tokens\n",
      "Processed 8,820,000 tokens\n",
      "Processed 8,830,000 tokens\n",
      "Processed 8,840,000 tokens\n",
      "Processed 8,850,000 tokens\n",
      "Processed 8,860,000 tokens\n",
      "Processed 8,870,000 tokens\n",
      "Processed 8,880,000 tokens\n",
      "Processed 8,890,000 tokens\n",
      "Processed 8,900,000 tokens\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_train_split/1820/book_07.txt\n",
      "Processed 8,910,000 tokens\n",
      "Processed 8,920,000 tokens\n",
      "Processed 8,930,000 tokens\n",
      "Processed 8,940,000 tokens\n",
      "Processed 8,950,000 tokens\n",
      "Processed 8,960,000 tokens\n",
      "Processed 8,970,000 tokens\n",
      "Processed 8,980,000 tokens\n",
      "Processed 8,990,000 tokens\n",
      "Processed 9,000,000 tokens\n",
      "Processed 9,010,000 tokens\n",
      "Processed 9,020,000 tokens\n",
      "Processed 9,030,000 tokens\n",
      "Processed 9,040,000 tokens\n",
      "Processed 9,050,000 tokens\n",
      "Processed 9,060,000 tokens\n",
      "Processed 9,070,000 tokens\n",
      "Processed 9,080,000 tokens\n",
      "Processed 9,090,000 tokens\n",
      "Processed 9,100,000 tokens\n",
      "Processed 9,110,000 tokens\n",
      "Processed 9,120,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_train_split/1820/book_08.txt\n",
      "Processed 9,130,000 tokens\n",
      "Processed 9,140,000 tokens\n",
      "Processed 9,150,000 tokens\n",
      "Processed 9,160,000 tokens\n",
      "Processed 9,170,000 tokens\n",
      "Processed 9,180,000 tokens\n",
      "Processed 9,190,000 tokens\n",
      "Processed 9,200,000 tokens\n",
      "Processed 9,210,000 tokens\n",
      "Processed 9,220,000 tokens\n",
      "Processed 9,230,000 tokens\n",
      "Processed 9,240,000 tokens\n",
      "Processed 9,250,000 tokens\n",
      "Processed 9,260,000 tokens\n",
      "Processed 9,270,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_train_split/1820/book_09.txt\n",
      "Processed 9,280,000 tokens\n",
      "Processed 9,290,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_train_split/1820/book_10.txt\n",
      "Processed 9,300,000 tokens\n",
      "tokenize file book_11.txt\n",
      "./Datasets/clean_train_split/1820/book_11.txt\n",
      "Processed 9,310,000 tokens\n",
      "Processed 9,320,000 tokens\n",
      "Processed 9,330,000 tokens\n",
      "Processed 9,340,000 tokens\n",
      "Processed 9,350,000 tokens\n",
      "Processed 9,360,000 tokens\n",
      "Processed 9,370,000 tokens\n",
      "Processed 9,380,000 tokens\n",
      "Processed 9,390,000 tokens\n",
      "Processed 9,400,000 tokens\n",
      "Processed 9,410,000 tokens\n",
      "Processed 9,420,000 tokens\n",
      "Processed 9,430,000 tokens\n",
      "Processed 9,440,000 tokens\n",
      "Processed 9,450,000 tokens\n",
      "tokenize file book_13.txt\n",
      "./Datasets/clean_train_split/1820/book_13.txt\n",
      "Processed 9,460,000 tokens\n",
      "Processed 9,470,000 tokens\n",
      "Processed 9,480,000 tokens\n",
      "Processed 9,490,000 tokens\n",
      "Processed 9,500,000 tokens\n",
      "tokenize file book_14.txt\n",
      "./Datasets/clean_train_split/1820/book_14.txt\n",
      "Processed 9,510,000 tokens\n",
      "Processed 9,520,000 tokens\n",
      "Processed 9,530,000 tokens\n",
      "Processed 9,540,000 tokens\n",
      "Processed 9,550,000 tokens\n",
      "Processed 9,560,000 tokens\n",
      "Processed 9,570,000 tokens\n",
      "Processed 9,580,000 tokens\n",
      "Processed 9,590,000 tokens\n",
      "Processed 9,600,000 tokens\n",
      "Processed 9,610,000 tokens\n",
      "Processed 9,620,000 tokens\n",
      "Processed 9,630,000 tokens\n",
      "Processed 9,640,000 tokens\n",
      "Processed 9,650,000 tokens\n",
      "Processed 9,660,000 tokens\n",
      "Processed 9,670,000 tokens\n",
      "Processed 9,680,000 tokens\n",
      "Processed 9,690,000 tokens\n",
      "Processed 9,700,000 tokens\n",
      "Processed 9,710,000 tokens\n",
      "Processing decade: 1830\n",
      "number of files in 1830 directory: 9\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_train_split/1830/book_01.txt\n",
      "Processed 9,720,000 tokens\n",
      "Processed 9,730,000 tokens\n",
      "Processed 9,740,000 tokens\n",
      "Processed 9,750,000 tokens\n",
      "Processed 9,760,000 tokens\n",
      "Processed 9,770,000 tokens\n",
      "Processed 9,780,000 tokens\n",
      "Processed 9,790,000 tokens\n",
      "Processed 9,800,000 tokens\n",
      "Processed 9,810,000 tokens\n",
      "Processed 9,820,000 tokens\n",
      "Processed 9,830,000 tokens\n",
      "Processed 9,840,000 tokens\n",
      "Processed 9,850,000 tokens\n",
      "Processed 9,860,000 tokens\n",
      "Processed 9,870,000 tokens\n",
      "Processed 9,880,000 tokens\n",
      "Processed 9,890,000 tokens\n",
      "Processed 9,900,000 tokens\n",
      "Processed 9,910,000 tokens\n",
      "Processed 9,920,000 tokens\n",
      "Processed 9,930,000 tokens\n",
      "Processed 9,940,000 tokens\n",
      "Processed 9,950,000 tokens\n",
      "Processed 9,960,000 tokens\n",
      "Processed 9,970,000 tokens\n",
      "Processed 9,980,000 tokens\n",
      "Processed 9,990,000 tokens\n",
      "Processed 10,000,000 tokens\n",
      "Processed 10,010,000 tokens\n",
      "Processed 10,020,000 tokens\n",
      "Processed 10,030,000 tokens\n",
      "Processed 10,040,000 tokens\n",
      "Processed 10,050,000 tokens\n",
      "Processed 10,060,000 tokens\n",
      "Processed 10,070,000 tokens\n",
      "Processed 10,080,000 tokens\n",
      "Processed 10,090,000 tokens\n",
      "Processed 10,100,000 tokens\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_train_split/1830/book_02.txt\n",
      "Processed 10,110,000 tokens\n",
      "Processed 10,120,000 tokens\n",
      "Processed 10,130,000 tokens\n",
      "Processed 10,140,000 tokens\n",
      "Processed 10,150,000 tokens\n",
      "Processed 10,160,000 tokens\n",
      "Processed 10,170,000 tokens\n",
      "Processed 10,180,000 tokens\n",
      "Processed 10,190,000 tokens\n",
      "Processed 10,200,000 tokens\n",
      "Processed 10,210,000 tokens\n",
      "Processed 10,220,000 tokens\n",
      "Processed 10,230,000 tokens\n",
      "Processed 10,240,000 tokens\n",
      "Processed 10,250,000 tokens\n",
      "Processed 10,260,000 tokens\n",
      "Processed 10,270,000 tokens\n",
      "Processed 10,280,000 tokens\n",
      "Processed 10,290,000 tokens\n",
      "Processed 10,300,000 tokens\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_train_split/1830/book_04.txt\n",
      "Processed 10,310,000 tokens\n",
      "Processed 10,320,000 tokens\n",
      "Processed 10,330,000 tokens\n",
      "Processed 10,340,000 tokens\n",
      "Processed 10,350,000 tokens\n",
      "Processed 10,360,000 tokens\n",
      "Processed 10,370,000 tokens\n",
      "Processed 10,380,000 tokens\n",
      "Processed 10,390,000 tokens\n",
      "Processed 10,400,000 tokens\n",
      "Processed 10,410,000 tokens\n",
      "Processed 10,420,000 tokens\n",
      "Processed 10,430,000 tokens\n",
      "Processed 10,440,000 tokens\n",
      "Processed 10,450,000 tokens\n",
      "Processed 10,460,000 tokens\n",
      "Processed 10,470,000 tokens\n",
      "Processed 10,480,000 tokens\n",
      "Processed 10,490,000 tokens\n",
      "Processed 10,500,000 tokens\n",
      "Processed 10,510,000 tokens\n",
      "Processed 10,520,000 tokens\n",
      "Processed 10,530,000 tokens\n",
      "Processed 10,540,000 tokens\n",
      "Processed 10,550,000 tokens\n",
      "Processed 10,560,000 tokens\n",
      "Processed 10,570,000 tokens\n",
      "Processed 10,580,000 tokens\n",
      "Processed 10,590,000 tokens\n",
      "Processed 10,600,000 tokens\n",
      "Processed 10,610,000 tokens\n",
      "Processed 10,620,000 tokens\n",
      "Processed 10,630,000 tokens\n",
      "Processed 10,640,000 tokens\n",
      "Processed 10,650,000 tokens\n",
      "Processed 10,660,000 tokens\n",
      "Processed 10,670,000 tokens\n",
      "Processed 10,680,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_train_split/1830/book_05.txt\n",
      "Processed 10,690,000 tokens\n",
      "Processed 10,700,000 tokens\n",
      "Processed 10,710,000 tokens\n",
      "Processed 10,720,000 tokens\n",
      "Processed 10,730,000 tokens\n",
      "Processed 10,740,000 tokens\n",
      "Processed 10,750,000 tokens\n",
      "Processed 10,760,000 tokens\n",
      "Processed 10,770,000 tokens\n",
      "Processed 10,780,000 tokens\n",
      "Processed 10,790,000 tokens\n",
      "Processed 10,800,000 tokens\n",
      "Processed 10,810,000 tokens\n",
      "Processed 10,820,000 tokens\n",
      "Processed 10,830,000 tokens\n",
      "Processed 10,840,000 tokens\n",
      "Processed 10,850,000 tokens\n",
      "Processed 10,860,000 tokens\n",
      "Processed 10,870,000 tokens\n",
      "Processed 10,880,000 tokens\n",
      "Processed 10,890,000 tokens\n",
      "Processed 10,900,000 tokens\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_train_split/1830/book_07.txt\n",
      "Processed 10,910,000 tokens\n",
      "Processed 10,920,000 tokens\n",
      "Processed 10,930,000 tokens\n",
      "Processed 10,940,000 tokens\n",
      "Processed 10,950,000 tokens\n",
      "Processed 10,960,000 tokens\n",
      "Processed 10,970,000 tokens\n",
      "Processed 10,980,000 tokens\n",
      "Processed 10,990,000 tokens\n",
      "Processed 11,000,000 tokens\n",
      "Processed 11,010,000 tokens\n",
      "Processed 11,020,000 tokens\n",
      "Processed 11,030,000 tokens\n",
      "Processed 11,040,000 tokens\n",
      "Processed 11,050,000 tokens\n",
      "Processed 11,060,000 tokens\n",
      "Processed 11,070,000 tokens\n",
      "Processed 11,080,000 tokens\n",
      "Processed 11,090,000 tokens\n",
      "Processed 11,100,000 tokens\n",
      "Processed 11,110,000 tokens\n",
      "Processed 11,120,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_train_split/1830/book_08.txt\n",
      "Processed 11,130,000 tokens\n",
      "Processed 11,140,000 tokens\n",
      "Processed 11,150,000 tokens\n",
      "Processed 11,160,000 tokens\n",
      "Processed 11,170,000 tokens\n",
      "Processed 11,180,000 tokens\n",
      "Processed 11,190,000 tokens\n",
      "Processed 11,200,000 tokens\n",
      "Processed 11,210,000 tokens\n",
      "Processed 11,220,000 tokens\n",
      "Processed 11,230,000 tokens\n",
      "Processed 11,240,000 tokens\n",
      "Processed 11,250,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_train_split/1830/book_09.txt\n",
      "Processed 11,260,000 tokens\n",
      "Processed 11,270,000 tokens\n",
      "Processed 11,280,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_train_split/1830/book_10.txt\n",
      "Processed 11,290,000 tokens\n",
      "Processed 11,300,000 tokens\n",
      "Processed 11,310,000 tokens\n",
      "Processed 11,320,000 tokens\n",
      "Processed 11,330,000 tokens\n",
      "Processed 11,340,000 tokens\n",
      "Processed 11,350,000 tokens\n",
      "Processed 11,360,000 tokens\n",
      "Processed 11,370,000 tokens\n",
      "Processed 11,380,000 tokens\n",
      "Processed 11,390,000 tokens\n",
      "Processed 11,400,000 tokens\n",
      "Processed 11,410,000 tokens\n",
      "tokenize file book_12.txt\n",
      "./Datasets/clean_train_split/1830/book_12.txt\n",
      "Processed 11,420,000 tokens\n",
      "Processed 11,430,000 tokens\n",
      "Processed 11,440,000 tokens\n",
      "Processed 11,450,000 tokens\n",
      "Processed 11,460,000 tokens\n",
      "Processed 11,470,000 tokens\n",
      "Processed 11,480,000 tokens\n",
      "Processed 11,490,000 tokens\n",
      "Processed 11,500,000 tokens\n",
      "Processed 11,510,000 tokens\n",
      "Processed 11,520,000 tokens\n",
      "Processed 11,530,000 tokens\n",
      "Processed 11,540,000 tokens\n",
      "Processed 11,550,000 tokens\n",
      "Processed 11,560,000 tokens\n",
      "Processed 11,570,000 tokens\n",
      "Processed 11,580,000 tokens\n",
      "Processing decade: 1840\n",
      "number of files in 1840 directory: 10\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_train_split/1840/book_01.txt\n",
      "Processed 11,590,000 tokens\n",
      "Processed 11,600,000 tokens\n",
      "Processed 11,610,000 tokens\n",
      "Processed 11,620,000 tokens\n",
      "Processed 11,630,000 tokens\n",
      "Processed 11,640,000 tokens\n",
      "Processed 11,650,000 tokens\n",
      "Processed 11,660,000 tokens\n",
      "Processed 11,670,000 tokens\n",
      "Processed 11,680,000 tokens\n",
      "Processed 11,690,000 tokens\n",
      "Processed 11,700,000 tokens\n",
      "Processed 11,710,000 tokens\n",
      "Processed 11,720,000 tokens\n",
      "Processed 11,730,000 tokens\n",
      "Processed 11,740,000 tokens\n",
      "Processed 11,750,000 tokens\n",
      "Processed 11,760,000 tokens\n",
      "Processed 11,770,000 tokens\n",
      "Processed 11,780,000 tokens\n",
      "Processed 11,790,000 tokens\n",
      "Processed 11,800,000 tokens\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_train_split/1840/book_02.txt\n",
      "Processed 11,810,000 tokens\n",
      "Processed 11,820,000 tokens\n",
      "Processed 11,830,000 tokens\n",
      "Processed 11,840,000 tokens\n",
      "Processed 11,850,000 tokens\n",
      "Processed 11,860,000 tokens\n",
      "Processed 11,870,000 tokens\n",
      "Processed 11,880,000 tokens\n",
      "Processed 11,890,000 tokens\n",
      "Processed 11,900,000 tokens\n",
      "Processed 11,910,000 tokens\n",
      "Processed 11,920,000 tokens\n",
      "Processed 11,930,000 tokens\n",
      "Processed 11,940,000 tokens\n",
      "Processed 11,950,000 tokens\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_train_split/1840/book_03.txt\n",
      "Processed 11,960,000 tokens\n",
      "Processed 11,970,000 tokens\n",
      "Processed 11,980,000 tokens\n",
      "Processed 11,990,000 tokens\n",
      "Processed 12,000,000 tokens\n",
      "Processed 12,010,000 tokens\n",
      "Processed 12,020,000 tokens\n",
      "Processed 12,030,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_train_split/1840/book_05.txt\n",
      "Processed 12,040,000 tokens\n",
      "Processed 12,050,000 tokens\n",
      "Processed 12,060,000 tokens\n",
      "Processed 12,070,000 tokens\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_train_split/1840/book_06.txt\n",
      "Processed 12,080,000 tokens\n",
      "Processed 12,090,000 tokens\n",
      "Processed 12,100,000 tokens\n",
      "Processed 12,110,000 tokens\n",
      "Processed 12,120,000 tokens\n",
      "Processed 12,130,000 tokens\n",
      "Processed 12,140,000 tokens\n",
      "Processed 12,150,000 tokens\n",
      "Processed 12,160,000 tokens\n",
      "Processed 12,170,000 tokens\n",
      "Processed 12,180,000 tokens\n",
      "Processed 12,190,000 tokens\n",
      "Processed 12,200,000 tokens\n",
      "Processed 12,210,000 tokens\n",
      "Processed 12,220,000 tokens\n",
      "Processed 12,230,000 tokens\n",
      "Processed 12,240,000 tokens\n",
      "Processed 12,250,000 tokens\n",
      "Processed 12,260,000 tokens\n",
      "Processed 12,270,000 tokens\n",
      "Processed 12,280,000 tokens\n",
      "Processed 12,290,000 tokens\n",
      "Processed 12,300,000 tokens\n",
      "Processed 12,310,000 tokens\n",
      "Processed 12,320,000 tokens\n",
      "Processed 12,330,000 tokens\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_train_split/1840/book_07.txt\n",
      "Processed 12,340,000 tokens\n",
      "Processed 12,350,000 tokens\n",
      "Processed 12,360,000 tokens\n",
      "Processed 12,370,000 tokens\n",
      "Processed 12,380,000 tokens\n",
      "Processed 12,390,000 tokens\n",
      "Processed 12,400,000 tokens\n",
      "Processed 12,410,000 tokens\n",
      "Processed 12,420,000 tokens\n",
      "Processed 12,430,000 tokens\n",
      "Processed 12,440,000 tokens\n",
      "Processed 12,450,000 tokens\n",
      "Processed 12,460,000 tokens\n",
      "Processed 12,470,000 tokens\n",
      "Processed 12,480,000 tokens\n",
      "Processed 12,490,000 tokens\n",
      "Processed 12,500,000 tokens\n",
      "Processed 12,510,000 tokens\n",
      "Processed 12,520,000 tokens\n",
      "Processed 12,530,000 tokens\n",
      "Processed 12,540,000 tokens\n",
      "Processed 12,550,000 tokens\n",
      "Processed 12,560,000 tokens\n",
      "Processed 12,570,000 tokens\n",
      "Processed 12,580,000 tokens\n",
      "Processed 12,590,000 tokens\n",
      "Processed 12,600,000 tokens\n",
      "Processed 12,610,000 tokens\n",
      "Processed 12,620,000 tokens\n",
      "Processed 12,630,000 tokens\n",
      "Processed 12,640,000 tokens\n",
      "Processed 12,650,000 tokens\n",
      "Processed 12,660,000 tokens\n",
      "Processed 12,670,000 tokens\n",
      "Processed 12,680,000 tokens\n",
      "Processed 12,690,000 tokens\n",
      "Processed 12,700,000 tokens\n",
      "Processed 12,710,000 tokens\n",
      "Processed 12,720,000 tokens\n",
      "Processed 12,730,000 tokens\n",
      "Processed 12,740,000 tokens\n",
      "Processed 12,750,000 tokens\n",
      "Processed 12,760,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_train_split/1840/book_08.txt\n",
      "Processed 12,770,000 tokens\n",
      "Processed 12,780,000 tokens\n",
      "Processed 12,790,000 tokens\n",
      "Processed 12,800,000 tokens\n",
      "Processed 12,810,000 tokens\n",
      "Processed 12,820,000 tokens\n",
      "Processed 12,830,000 tokens\n",
      "Processed 12,840,000 tokens\n",
      "Processed 12,850,000 tokens\n",
      "Processed 12,860,000 tokens\n",
      "Processed 12,870,000 tokens\n",
      "Processed 12,880,000 tokens\n",
      "Processed 12,890,000 tokens\n",
      "Processed 12,900,000 tokens\n",
      "Processed 12,910,000 tokens\n",
      "Processed 12,920,000 tokens\n",
      "Processed 12,930,000 tokens\n",
      "Processed 12,940,000 tokens\n",
      "Processed 12,950,000 tokens\n",
      "Processed 12,960,000 tokens\n",
      "Processed 12,970,000 tokens\n",
      "Processed 12,980,000 tokens\n",
      "Processed 12,990,000 tokens\n",
      "Processed 13,000,000 tokens\n",
      "Processed 13,010,000 tokens\n",
      "Processed 13,020,000 tokens\n",
      "Processed 13,030,000 tokens\n",
      "Processed 13,040,000 tokens\n",
      "Processed 13,050,000 tokens\n",
      "Processed 13,060,000 tokens\n",
      "Processed 13,070,000 tokens\n",
      "Processed 13,080,000 tokens\n",
      "Processed 13,090,000 tokens\n",
      "Processed 13,100,000 tokens\n",
      "Processed 13,110,000 tokens\n",
      "Processed 13,120,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_train_split/1840/book_10.txt\n",
      "Processed 13,130,000 tokens\n",
      "Processed 13,140,000 tokens\n",
      "Processed 13,150,000 tokens\n",
      "Processed 13,160,000 tokens\n",
      "Processed 13,170,000 tokens\n",
      "Processed 13,180,000 tokens\n",
      "Processed 13,190,000 tokens\n",
      "Processed 13,200,000 tokens\n",
      "Processed 13,210,000 tokens\n",
      "Processed 13,220,000 tokens\n",
      "Processed 13,230,000 tokens\n",
      "Processed 13,240,000 tokens\n",
      "Processed 13,250,000 tokens\n",
      "Processed 13,260,000 tokens\n",
      "Processed 13,270,000 tokens\n",
      "Processed 13,280,000 tokens\n",
      "Processed 13,290,000 tokens\n",
      "Processed 13,300,000 tokens\n",
      "Processed 13,310,000 tokens\n",
      "Processed 13,320,000 tokens\n",
      "Processed 13,330,000 tokens\n",
      "Processed 13,340,000 tokens\n",
      "Processed 13,350,000 tokens\n",
      "Processed 13,360,000 tokens\n",
      "Processed 13,370,000 tokens\n",
      "Processed 13,380,000 tokens\n",
      "Processed 13,390,000 tokens\n",
      "Processed 13,400,000 tokens\n",
      "Processed 13,410,000 tokens\n",
      "tokenize file book_11.txt\n",
      "./Datasets/clean_train_split/1840/book_11.txt\n",
      "Processed 13,420,000 tokens\n",
      "Processed 13,430,000 tokens\n",
      "Processed 13,440,000 tokens\n",
      "Processed 13,450,000 tokens\n",
      "tokenize file book_13.txt\n",
      "./Datasets/clean_train_split/1840/book_13.txt\n",
      "Processed 13,460,000 tokens\n",
      "Processed 13,470,000 tokens\n",
      "Processed 13,480,000 tokens\n",
      "Processed 13,490,000 tokens\n",
      "Processed 13,500,000 tokens\n",
      "Processed 13,510,000 tokens\n",
      "Processed 13,520,000 tokens\n",
      "Processing decade: 1850\n",
      "number of files in 1850 directory: 10\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_train_split/1850/book_02.txt\n",
      "Processed 13,530,000 tokens\n",
      "Processed 13,540,000 tokens\n",
      "Processed 13,550,000 tokens\n",
      "Processed 13,560,000 tokens\n",
      "Processed 13,570,000 tokens\n",
      "Processed 13,580,000 tokens\n",
      "Processed 13,590,000 tokens\n",
      "Processed 13,600,000 tokens\n",
      "Processed 13,610,000 tokens\n",
      "Processed 13,620,000 tokens\n",
      "Processed 13,630,000 tokens\n",
      "Processed 13,640,000 tokens\n",
      "Processed 13,650,000 tokens\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_train_split/1850/book_03.txt\n",
      "Processed 13,660,000 tokens\n",
      "Processed 13,670,000 tokens\n",
      "Processed 13,680,000 tokens\n",
      "Processed 13,690,000 tokens\n",
      "Processed 13,700,000 tokens\n",
      "Processed 13,710,000 tokens\n",
      "Processed 13,720,000 tokens\n",
      "Processed 13,730,000 tokens\n",
      "Processed 13,740,000 tokens\n",
      "Processed 13,750,000 tokens\n",
      "Processed 13,760,000 tokens\n",
      "Processed 13,770,000 tokens\n",
      "Processed 13,780,000 tokens\n",
      "Processed 13,790,000 tokens\n",
      "Processed 13,800,000 tokens\n",
      "Processed 13,810,000 tokens\n",
      "Processed 13,820,000 tokens\n",
      "Processed 13,830,000 tokens\n",
      "Processed 13,840,000 tokens\n",
      "Processed 13,850,000 tokens\n",
      "Processed 13,860,000 tokens\n",
      "Processed 13,870,000 tokens\n",
      "Processed 13,880,000 tokens\n",
      "Processed 13,890,000 tokens\n",
      "Processed 13,900,000 tokens\n",
      "Processed 13,910,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_train_split/1850/book_05.txt\n",
      "Processed 13,920,000 tokens\n",
      "Processed 13,930,000 tokens\n",
      "Processed 13,940,000 tokens\n",
      "Processed 13,950,000 tokens\n",
      "Processed 13,960,000 tokens\n",
      "Processed 13,970,000 tokens\n",
      "Processed 13,980,000 tokens\n",
      "Processed 13,990,000 tokens\n",
      "Processed 14,000,000 tokens\n",
      "Processed 14,010,000 tokens\n",
      "Processed 14,020,000 tokens\n",
      "Processed 14,030,000 tokens\n",
      "Processed 14,040,000 tokens\n",
      "Processed 14,050,000 tokens\n",
      "Processed 14,060,000 tokens\n",
      "Processed 14,070,000 tokens\n",
      "Processed 14,080,000 tokens\n",
      "Processed 14,090,000 tokens\n",
      "Processed 14,100,000 tokens\n",
      "Processed 14,110,000 tokens\n",
      "Processed 14,120,000 tokens\n",
      "Processed 14,130,000 tokens\n",
      "Processed 14,140,000 tokens\n",
      "Processed 14,150,000 tokens\n",
      "Processed 14,160,000 tokens\n",
      "Processed 14,170,000 tokens\n",
      "Processed 14,180,000 tokens\n",
      "Processed 14,190,000 tokens\n",
      "Processed 14,200,000 tokens\n",
      "Processed 14,210,000 tokens\n",
      "Processed 14,220,000 tokens\n",
      "Processed 14,230,000 tokens\n",
      "Processed 14,240,000 tokens\n",
      "Processed 14,250,000 tokens\n",
      "Processed 14,260,000 tokens\n",
      "Processed 14,270,000 tokens\n",
      "Processed 14,280,000 tokens\n",
      "Processed 14,290,000 tokens\n",
      "Processed 14,300,000 tokens\n",
      "Processed 14,310,000 tokens\n",
      "Processed 14,320,000 tokens\n",
      "Processed 14,330,000 tokens\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_train_split/1850/book_06.txt\n",
      "Processed 14,340,000 tokens\n",
      "Processed 14,350,000 tokens\n",
      "Processed 14,360,000 tokens\n",
      "Processed 14,370,000 tokens\n",
      "Processed 14,380,000 tokens\n",
      "Processed 14,390,000 tokens\n",
      "Processed 14,400,000 tokens\n",
      "Processed 14,410,000 tokens\n",
      "Processed 14,420,000 tokens\n",
      "Processed 14,430,000 tokens\n",
      "Processed 14,440,000 tokens\n",
      "Processed 14,450,000 tokens\n",
      "Processed 14,460,000 tokens\n",
      "Processed 14,470,000 tokens\n",
      "Processed 14,480,000 tokens\n",
      "Processed 14,490,000 tokens\n",
      "Processed 14,500,000 tokens\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_train_split/1850/book_07.txt\n",
      "Processed 14,510,000 tokens\n",
      "Processed 14,520,000 tokens\n",
      "Processed 14,530,000 tokens\n",
      "Processed 14,540,000 tokens\n",
      "Processed 14,550,000 tokens\n",
      "Processed 14,560,000 tokens\n",
      "Processed 14,570,000 tokens\n",
      "Processed 14,580,000 tokens\n",
      "Processed 14,590,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_train_split/1850/book_08.txt\n",
      "Processed 14,600,000 tokens\n",
      "Processed 14,610,000 tokens\n",
      "Processed 14,620,000 tokens\n",
      "Processed 14,630,000 tokens\n",
      "Processed 14,640,000 tokens\n",
      "Processed 14,650,000 tokens\n",
      "Processed 14,660,000 tokens\n",
      "Processed 14,670,000 tokens\n",
      "Processed 14,680,000 tokens\n",
      "Processed 14,690,000 tokens\n",
      "Processed 14,700,000 tokens\n",
      "Processed 14,710,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_train_split/1850/book_09.txt\n",
      "Processed 14,720,000 tokens\n",
      "Processed 14,730,000 tokens\n",
      "Processed 14,740,000 tokens\n",
      "Processed 14,750,000 tokens\n",
      "Processed 14,760,000 tokens\n",
      "Processed 14,770,000 tokens\n",
      "Processed 14,780,000 tokens\n",
      "Processed 14,790,000 tokens\n",
      "Processed 14,800,000 tokens\n",
      "Processed 14,810,000 tokens\n",
      "Processed 14,820,000 tokens\n",
      "Processed 14,830,000 tokens\n",
      "Processed 14,840,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_train_split/1850/book_10.txt\n",
      "Processed 14,850,000 tokens\n",
      "Processed 14,860,000 tokens\n",
      "Processed 14,870,000 tokens\n",
      "Processed 14,880,000 tokens\n",
      "Processed 14,890,000 tokens\n",
      "Processed 14,900,000 tokens\n",
      "Processed 14,910,000 tokens\n",
      "Processed 14,920,000 tokens\n",
      "Processed 14,930,000 tokens\n",
      "Processed 14,940,000 tokens\n",
      "Processed 14,950,000 tokens\n",
      "Processed 14,960,000 tokens\n",
      "Processed 14,970,000 tokens\n",
      "Processed 14,980,000 tokens\n",
      "Processed 14,990,000 tokens\n",
      "Processed 15,000,000 tokens\n",
      "Processed 15,010,000 tokens\n",
      "Processed 15,020,000 tokens\n",
      "Processed 15,030,000 tokens\n",
      "Processed 15,040,000 tokens\n",
      "Processed 15,050,000 tokens\n",
      "Processed 15,060,000 tokens\n",
      "Processed 15,070,000 tokens\n",
      "Processed 15,080,000 tokens\n",
      "Processed 15,090,000 tokens\n",
      "Processed 15,100,000 tokens\n",
      "Processed 15,110,000 tokens\n",
      "Processed 15,120,000 tokens\n",
      "Processed 15,130,000 tokens\n",
      "Processed 15,140,000 tokens\n",
      "Processed 15,150,000 tokens\n",
      "Processed 15,160,000 tokens\n",
      "Processed 15,170,000 tokens\n",
      "Processed 15,180,000 tokens\n",
      "Processed 15,190,000 tokens\n",
      "Processed 15,200,000 tokens\n",
      "Processed 15,210,000 tokens\n",
      "Processed 15,220,000 tokens\n",
      "Processed 15,230,000 tokens\n",
      "Processed 15,240,000 tokens\n",
      "Processed 15,250,000 tokens\n",
      "Processed 15,260,000 tokens\n",
      "Processed 15,270,000 tokens\n",
      "Processed 15,280,000 tokens\n",
      "tokenize file book_11.txt\n",
      "./Datasets/clean_train_split/1850/book_11.txt\n",
      "Processed 15,290,000 tokens\n",
      "Processed 15,300,000 tokens\n",
      "Processed 15,310,000 tokens\n",
      "Processed 15,320,000 tokens\n",
      "Processed 15,330,000 tokens\n",
      "Processed 15,340,000 tokens\n",
      "Processed 15,350,000 tokens\n",
      "Processed 15,360,000 tokens\n",
      "Processed 15,370,000 tokens\n",
      "Processed 15,380,000 tokens\n",
      "Processed 15,390,000 tokens\n",
      "Processed 15,400,000 tokens\n",
      "Processed 15,410,000 tokens\n",
      "Processed 15,420,000 tokens\n",
      "Processed 15,430,000 tokens\n",
      "Processed 15,440,000 tokens\n",
      "Processed 15,450,000 tokens\n",
      "Processed 15,460,000 tokens\n",
      "Processed 15,470,000 tokens\n",
      "Processed 15,480,000 tokens\n",
      "Processed 15,490,000 tokens\n",
      "Processed 15,500,000 tokens\n",
      "tokenize file book_12.txt\n",
      "./Datasets/clean_train_split/1850/book_12.txt\n",
      "Processed 15,510,000 tokens\n",
      "Processed 15,520,000 tokens\n",
      "Processed 15,530,000 tokens\n",
      "Processed 15,540,000 tokens\n",
      "Processed 15,550,000 tokens\n",
      "Processed 15,560,000 tokens\n",
      "Processed 15,570,000 tokens\n",
      "Processed 15,580,000 tokens\n",
      "Processed 15,590,000 tokens\n",
      "Processed 15,600,000 tokens\n",
      "Processed 15,610,000 tokens\n",
      "Processed 15,620,000 tokens\n",
      "Processed 15,630,000 tokens\n",
      "Processed 15,640,000 tokens\n",
      "Processed 15,650,000 tokens\n",
      "Processing decade: 1860\n",
      "number of files in 1860 directory: 9\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_train_split/1860/book_01.txt\n",
      "Processed 15,660,000 tokens\n",
      "Processed 15,670,000 tokens\n",
      "Processed 15,680,000 tokens\n",
      "Processed 15,690,000 tokens\n",
      "Processed 15,700,000 tokens\n",
      "Processed 15,710,000 tokens\n",
      "Processed 15,720,000 tokens\n",
      "Processed 15,730,000 tokens\n",
      "Processed 15,740,000 tokens\n",
      "Processed 15,750,000 tokens\n",
      "Processed 15,760,000 tokens\n",
      "Processed 15,770,000 tokens\n",
      "Processed 15,780,000 tokens\n",
      "Processed 15,790,000 tokens\n",
      "Processed 15,800,000 tokens\n",
      "Processed 15,810,000 tokens\n",
      "Processed 15,820,000 tokens\n",
      "Processed 15,830,000 tokens\n",
      "Processed 15,840,000 tokens\n",
      "Processed 15,850,000 tokens\n",
      "Processed 15,860,000 tokens\n",
      "Processed 15,870,000 tokens\n",
      "Processed 15,880,000 tokens\n",
      "Processed 15,890,000 tokens\n",
      "Processed 15,900,000 tokens\n",
      "Processed 15,910,000 tokens\n",
      "Processed 15,920,000 tokens\n",
      "Processed 15,930,000 tokens\n",
      "Processed 15,940,000 tokens\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_train_split/1860/book_02.txt\n",
      "Processed 15,950,000 tokens\n",
      "Processed 15,960,000 tokens\n",
      "Processed 15,970,000 tokens\n",
      "Processed 15,980,000 tokens\n",
      "Processed 15,990,000 tokens\n",
      "Processed 16,000,000 tokens\n",
      "Processed 16,010,000 tokens\n",
      "Processed 16,020,000 tokens\n",
      "Processed 16,030,000 tokens\n",
      "Processed 16,040,000 tokens\n",
      "Processed 16,050,000 tokens\n",
      "Processed 16,060,000 tokens\n",
      "Processed 16,070,000 tokens\n",
      "Processed 16,080,000 tokens\n",
      "Processed 16,090,000 tokens\n",
      "Processed 16,100,000 tokens\n",
      "Processed 16,110,000 tokens\n",
      "Processed 16,120,000 tokens\n",
      "Processed 16,130,000 tokens\n",
      "Processed 16,140,000 tokens\n",
      "Processed 16,150,000 tokens\n",
      "Processed 16,160,000 tokens\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_train_split/1860/book_03.txt\n",
      "Processed 16,170,000 tokens\n",
      "Processed 16,180,000 tokens\n",
      "Processed 16,190,000 tokens\n",
      "Processed 16,200,000 tokens\n",
      "Processed 16,210,000 tokens\n",
      "Processed 16,220,000 tokens\n",
      "Processed 16,230,000 tokens\n",
      "Processed 16,240,000 tokens\n",
      "Processed 16,250,000 tokens\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_train_split/1860/book_04.txt\n",
      "Processed 16,260,000 tokens\n",
      "Processed 16,270,000 tokens\n",
      "Processed 16,280,000 tokens\n",
      "Processed 16,290,000 tokens\n",
      "Processed 16,300,000 tokens\n",
      "Processed 16,310,000 tokens\n",
      "Processed 16,320,000 tokens\n",
      "Processed 16,330,000 tokens\n",
      "Processed 16,340,000 tokens\n",
      "Processed 16,350,000 tokens\n",
      "Processed 16,360,000 tokens\n",
      "Processed 16,370,000 tokens\n",
      "Processed 16,380,000 tokens\n",
      "Processed 16,390,000 tokens\n",
      "Processed 16,400,000 tokens\n",
      "Processed 16,410,000 tokens\n",
      "Processed 16,420,000 tokens\n",
      "Processed 16,430,000 tokens\n",
      "Processed 16,440,000 tokens\n",
      "Processed 16,450,000 tokens\n",
      "Processed 16,460,000 tokens\n",
      "Processed 16,470,000 tokens\n",
      "Processed 16,480,000 tokens\n",
      "Processed 16,490,000 tokens\n",
      "Processed 16,500,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_train_split/1860/book_05.txt\n",
      "Processed 16,510,000 tokens\n",
      "Processed 16,520,000 tokens\n",
      "Processed 16,530,000 tokens\n",
      "Processed 16,540,000 tokens\n",
      "Processed 16,550,000 tokens\n",
      "Processed 16,560,000 tokens\n",
      "Processed 16,570,000 tokens\n",
      "Processed 16,580,000 tokens\n",
      "Processed 16,590,000 tokens\n",
      "Processed 16,600,000 tokens\n",
      "Processed 16,610,000 tokens\n",
      "Processed 16,620,000 tokens\n",
      "Processed 16,630,000 tokens\n",
      "Processed 16,640,000 tokens\n",
      "Processed 16,650,000 tokens\n",
      "Processed 16,660,000 tokens\n",
      "Processed 16,670,000 tokens\n",
      "Processed 16,680,000 tokens\n",
      "Processed 16,690,000 tokens\n",
      "Processed 16,700,000 tokens\n",
      "Processed 16,710,000 tokens\n",
      "Processed 16,720,000 tokens\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_train_split/1860/book_06.txt\n",
      "Processed 16,730,000 tokens\n",
      "Processed 16,740,000 tokens\n",
      "Processed 16,750,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_train_split/1860/book_09.txt\n",
      "Processed 16,760,000 tokens\n",
      "Processed 16,770,000 tokens\n",
      "Processed 16,780,000 tokens\n",
      "Processed 16,790,000 tokens\n",
      "Processed 16,800,000 tokens\n",
      "Processed 16,810,000 tokens\n",
      "Processed 16,820,000 tokens\n",
      "Processed 16,830,000 tokens\n",
      "Processed 16,840,000 tokens\n",
      "Processed 16,850,000 tokens\n",
      "Processed 16,860,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_train_split/1860/book_10.txt\n",
      "Processed 16,870,000 tokens\n",
      "Processed 16,880,000 tokens\n",
      "Processed 16,890,000 tokens\n",
      "Processed 16,900,000 tokens\n",
      "Processed 16,910,000 tokens\n",
      "Processed 16,920,000 tokens\n",
      "Processed 16,930,000 tokens\n",
      "Processed 16,940,000 tokens\n",
      "Processed 16,950,000 tokens\n",
      "Processed 16,960,000 tokens\n",
      "Processed 16,970,000 tokens\n",
      "Processed 16,980,000 tokens\n",
      "Processed 16,990,000 tokens\n",
      "tokenize file book_12.txt\n",
      "./Datasets/clean_train_split/1860/book_12.txt\n",
      "Processed 17,000,000 tokens\n",
      "Processed 17,010,000 tokens\n",
      "Processed 17,020,000 tokens\n",
      "Processed 17,030,000 tokens\n",
      "Processed 17,040,000 tokens\n",
      "Processed 17,050,000 tokens\n",
      "Processed 17,060,000 tokens\n",
      "Processed 17,070,000 tokens\n",
      "Processed 17,080,000 tokens\n",
      "Processed 17,090,000 tokens\n",
      "Processed 17,100,000 tokens\n",
      "Processed 17,110,000 tokens\n",
      "Processed 17,120,000 tokens\n",
      "Processed 17,130,000 tokens\n",
      "Processed 17,140,000 tokens\n",
      "Processed 17,150,000 tokens\n",
      "Processed 17,160,000 tokens\n",
      "Processed 17,170,000 tokens\n",
      "Processed 17,180,000 tokens\n",
      "Processed 17,190,000 tokens\n",
      "Processed 17,200,000 tokens\n",
      "Processed 17,210,000 tokens\n",
      "Processed 17,220,000 tokens\n",
      "Processed 17,230,000 tokens\n",
      "Processed 17,240,000 tokens\n",
      "Processed 17,250,000 tokens\n",
      "Processed 17,260,000 tokens\n",
      "Processed 17,270,000 tokens\n",
      "Processed 17,280,000 tokens\n",
      "Processed 17,290,000 tokens\n",
      "Processed 17,300,000 tokens\n",
      "Processed 17,310,000 tokens\n",
      "Processed 17,320,000 tokens\n",
      "Processed 17,330,000 tokens\n",
      "Processed 17,340,000 tokens\n",
      "Processed 17,350,000 tokens\n",
      "Processed 17,360,000 tokens\n",
      "Processed 17,370,000 tokens\n",
      "Processed 17,380,000 tokens\n",
      "Processed 17,390,000 tokens\n",
      "Processed 17,400,000 tokens\n",
      "Processed 17,410,000 tokens\n",
      "Processed 17,420,000 tokens\n",
      "Processed 17,430,000 tokens\n",
      "Processed 17,440,000 tokens\n",
      "Processed 17,450,000 tokens\n",
      "Processed 17,460,000 tokens\n",
      "Processed 17,470,000 tokens\n",
      "Processed 17,480,000 tokens\n",
      "Processed 17,490,000 tokens\n",
      "Processed 17,500,000 tokens\n",
      "Processed 17,510,000 tokens\n",
      "Processed 17,520,000 tokens\n",
      "Processed 17,530,000 tokens\n",
      "Processed 17,540,000 tokens\n",
      "Processed 17,550,000 tokens\n",
      "Processed 17,560,000 tokens\n",
      "Processed 17,570,000 tokens\n",
      "Processed 17,580,000 tokens\n",
      "Processed 17,590,000 tokens\n",
      "Processed 17,600,000 tokens\n",
      "Processed 17,610,000 tokens\n",
      "Processed 17,620,000 tokens\n",
      "Processed 17,630,000 tokens\n",
      "Processed 17,640,000 tokens\n",
      "Processed 17,650,000 tokens\n",
      "Processed 17,660,000 tokens\n",
      "Processed 17,670,000 tokens\n",
      "Processing decade: 1870\n",
      "number of files in 1870 directory: 9\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_train_split/1870/book_01.txt\n",
      "Processed 17,680,000 tokens\n",
      "Processed 17,690,000 tokens\n",
      "Processed 17,700,000 tokens\n",
      "Processed 17,710,000 tokens\n",
      "Processed 17,720,000 tokens\n",
      "Processed 17,730,000 tokens\n",
      "Processed 17,740,000 tokens\n",
      "Processed 17,750,000 tokens\n",
      "Processed 17,760,000 tokens\n",
      "Processed 17,770,000 tokens\n",
      "Processed 17,780,000 tokens\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_train_split/1870/book_02.txt\n",
      "Processed 17,790,000 tokens\n",
      "Processed 17,800,000 tokens\n",
      "Processed 17,810,000 tokens\n",
      "Processed 17,820,000 tokens\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_train_split/1870/book_03.txt\n",
      "Processed 17,830,000 tokens\n",
      "Processed 17,840,000 tokens\n",
      "Processed 17,850,000 tokens\n",
      "Processed 17,860,000 tokens\n",
      "Processed 17,870,000 tokens\n",
      "Processed 17,880,000 tokens\n",
      "Processed 17,890,000 tokens\n",
      "Processed 17,900,000 tokens\n",
      "Processed 17,910,000 tokens\n",
      "Processed 17,920,000 tokens\n",
      "Processed 17,930,000 tokens\n",
      "Processed 17,940,000 tokens\n",
      "Processed 17,950,000 tokens\n",
      "Processed 17,960,000 tokens\n",
      "Processed 17,970,000 tokens\n",
      "Processed 17,980,000 tokens\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_train_split/1870/book_04.txt\n",
      "Processed 17,990,000 tokens\n",
      "Processed 18,000,000 tokens\n",
      "Processed 18,010,000 tokens\n",
      "Processed 18,020,000 tokens\n",
      "Processed 18,030,000 tokens\n",
      "Processed 18,040,000 tokens\n",
      "Processed 18,050,000 tokens\n",
      "Processed 18,060,000 tokens\n",
      "Processed 18,070,000 tokens\n",
      "Processed 18,080,000 tokens\n",
      "Processed 18,090,000 tokens\n",
      "Processed 18,100,000 tokens\n",
      "Processed 18,110,000 tokens\n",
      "Processed 18,120,000 tokens\n",
      "Processed 18,130,000 tokens\n",
      "Processed 18,140,000 tokens\n",
      "Processed 18,150,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_train_split/1870/book_05.txt\n",
      "Processed 18,160,000 tokens\n",
      "Processed 18,170,000 tokens\n",
      "Processed 18,180,000 tokens\n",
      "Processed 18,190,000 tokens\n",
      "Processed 18,200,000 tokens\n",
      "Processed 18,210,000 tokens\n",
      "Processed 18,220,000 tokens\n",
      "Processed 18,230,000 tokens\n",
      "Processed 18,240,000 tokens\n",
      "Processed 18,250,000 tokens\n",
      "Processed 18,260,000 tokens\n",
      "Processed 18,270,000 tokens\n",
      "Processed 18,280,000 tokens\n",
      "Processed 18,290,000 tokens\n",
      "Processed 18,300,000 tokens\n",
      "Processed 18,310,000 tokens\n",
      "Processed 18,320,000 tokens\n",
      "Processed 18,330,000 tokens\n",
      "Processed 18,340,000 tokens\n",
      "Processed 18,350,000 tokens\n",
      "Processed 18,360,000 tokens\n",
      "Processed 18,370,000 tokens\n",
      "Processed 18,380,000 tokens\n",
      "Processed 18,390,000 tokens\n",
      "Processed 18,400,000 tokens\n",
      "Processed 18,410,000 tokens\n",
      "Processed 18,420,000 tokens\n",
      "Processed 18,430,000 tokens\n",
      "Processed 18,440,000 tokens\n",
      "Processed 18,450,000 tokens\n",
      "Processed 18,460,000 tokens\n",
      "Processed 18,470,000 tokens\n",
      "Processed 18,480,000 tokens\n",
      "Processed 18,490,000 tokens\n",
      "Processed 18,500,000 tokens\n",
      "Processed 18,510,000 tokens\n",
      "Processed 18,520,000 tokens\n",
      "Processed 18,530,000 tokens\n",
      "Processed 18,540,000 tokens\n",
      "Processed 18,550,000 tokens\n",
      "Processed 18,560,000 tokens\n",
      "Processed 18,570,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_train_split/1870/book_08.txt\n",
      "Processed 18,580,000 tokens\n",
      "Processed 18,590,000 tokens\n",
      "Processed 18,600,000 tokens\n",
      "Processed 18,610,000 tokens\n",
      "Processed 18,620,000 tokens\n",
      "Processed 18,630,000 tokens\n",
      "Processed 18,640,000 tokens\n",
      "Processed 18,650,000 tokens\n",
      "Processed 18,660,000 tokens\n",
      "Processed 18,670,000 tokens\n",
      "Processed 18,680,000 tokens\n",
      "Processed 18,690,000 tokens\n",
      "Processed 18,700,000 tokens\n",
      "Processed 18,710,000 tokens\n",
      "Processed 18,720,000 tokens\n",
      "Processed 18,730,000 tokens\n",
      "Processed 18,740,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_train_split/1870/book_09.txt\n",
      "Processed 18,750,000 tokens\n",
      "Processed 18,760,000 tokens\n",
      "Processed 18,770,000 tokens\n",
      "Processed 18,780,000 tokens\n",
      "Processed 18,790,000 tokens\n",
      "Processed 18,800,000 tokens\n",
      "Processed 18,810,000 tokens\n",
      "Processed 18,820,000 tokens\n",
      "Processed 18,830,000 tokens\n",
      "Processed 18,840,000 tokens\n",
      "Processed 18,850,000 tokens\n",
      "Processed 18,860,000 tokens\n",
      "Processed 18,870,000 tokens\n",
      "Processed 18,880,000 tokens\n",
      "Processed 18,890,000 tokens\n",
      "Processed 18,900,000 tokens\n",
      "Processed 18,910,000 tokens\n",
      "Processed 18,920,000 tokens\n",
      "Processed 18,930,000 tokens\n",
      "Processed 18,940,000 tokens\n",
      "Processed 18,950,000 tokens\n",
      "Processed 18,960,000 tokens\n",
      "Processed 18,970,000 tokens\n",
      "Processed 18,980,000 tokens\n",
      "Processed 18,990,000 tokens\n",
      "Processed 19,000,000 tokens\n",
      "Processed 19,010,000 tokens\n",
      "Processed 19,020,000 tokens\n",
      "Processed 19,030,000 tokens\n",
      "Processed 19,040,000 tokens\n",
      "Processed 19,050,000 tokens\n",
      "Processed 19,060,000 tokens\n",
      "Processed 19,070,000 tokens\n",
      "Processed 19,080,000 tokens\n",
      "Processed 19,090,000 tokens\n",
      "Processed 19,100,000 tokens\n",
      "Processed 19,110,000 tokens\n",
      "Processed 19,120,000 tokens\n",
      "Processed 19,130,000 tokens\n",
      "Processed 19,140,000 tokens\n",
      "Processed 19,150,000 tokens\n",
      "Processed 19,160,000 tokens\n",
      "Processed 19,170,000 tokens\n",
      "tokenize file book_11.txt\n",
      "./Datasets/clean_train_split/1870/book_11.txt\n",
      "Processed 19,180,000 tokens\n",
      "Processed 19,190,000 tokens\n",
      "Processed 19,200,000 tokens\n",
      "Processed 19,210,000 tokens\n",
      "Processed 19,220,000 tokens\n",
      "Processed 19,230,000 tokens\n",
      "Processed 19,240,000 tokens\n",
      "Processed 19,250,000 tokens\n",
      "Processed 19,260,000 tokens\n",
      "Processed 19,270,000 tokens\n",
      "Processed 19,280,000 tokens\n",
      "Processed 19,290,000 tokens\n",
      "Processed 19,300,000 tokens\n",
      "Processed 19,310,000 tokens\n",
      "Processed 19,320,000 tokens\n",
      "Processed 19,330,000 tokens\n",
      "tokenize file book_12.txt\n",
      "./Datasets/clean_train_split/1870/book_12.txt\n",
      "Processed 19,340,000 tokens\n",
      "Processed 19,350,000 tokens\n",
      "Processed 19,360,000 tokens\n",
      "Processed 19,370,000 tokens\n",
      "Processed 19,380,000 tokens\n",
      "Processed 19,390,000 tokens\n",
      "Processed 19,400,000 tokens\n",
      "Processed 19,410,000 tokens\n",
      "Processed 19,420,000 tokens\n",
      "Processed 19,430,000 tokens\n",
      "Processed 19,440,000 tokens\n",
      "Processed 19,450,000 tokens\n",
      "Processed 19,460,000 tokens\n",
      "Processed 19,470,000 tokens\n",
      "Processed 19,480,000 tokens\n",
      "Processed 19,490,000 tokens\n",
      "Processed 19,500,000 tokens\n",
      "Processed 19,510,000 tokens\n",
      "Processed 19,520,000 tokens\n",
      "Processed 19,530,000 tokens\n",
      "Processed 19,540,000 tokens\n",
      "Processed 19,550,000 tokens\n",
      "Processed 19,560,000 tokens\n",
      "Processed 19,570,000 tokens\n",
      "Processed 19,580,000 tokens\n",
      "Processed 19,590,000 tokens\n",
      "Processed 19,600,000 tokens\n",
      "Processed 19,610,000 tokens\n",
      "Processed 19,620,000 tokens\n",
      "Processed 19,630,000 tokens\n",
      "Processing decade: 1880\n",
      "number of files in 1880 directory: 9\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_train_split/1880/book_01.txt\n",
      "Processed 19,640,000 tokens\n",
      "Processed 19,650,000 tokens\n",
      "Processed 19,660,000 tokens\n",
      "Processed 19,670,000 tokens\n",
      "Processed 19,680,000 tokens\n",
      "Processed 19,690,000 tokens\n",
      "Processed 19,700,000 tokens\n",
      "Processed 19,710,000 tokens\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_train_split/1880/book_02.txt\n",
      "Processed 19,720,000 tokens\n",
      "Processed 19,730,000 tokens\n",
      "Processed 19,740,000 tokens\n",
      "Processed 19,750,000 tokens\n",
      "Processed 19,760,000 tokens\n",
      "Processed 19,770,000 tokens\n",
      "Processed 19,780,000 tokens\n",
      "Processed 19,790,000 tokens\n",
      "Processed 19,800,000 tokens\n",
      "Processed 19,810,000 tokens\n",
      "Processed 19,820,000 tokens\n",
      "Processed 19,830,000 tokens\n",
      "Processed 19,840,000 tokens\n",
      "Processed 19,850,000 tokens\n",
      "Processed 19,860,000 tokens\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_train_split/1880/book_04.txt\n",
      "Processed 19,870,000 tokens\n",
      "Processed 19,880,000 tokens\n",
      "Processed 19,890,000 tokens\n",
      "Processed 19,900,000 tokens\n",
      "Processed 19,910,000 tokens\n",
      "Processed 19,920,000 tokens\n",
      "Processed 19,930,000 tokens\n",
      "Processed 19,940,000 tokens\n",
      "Processed 19,950,000 tokens\n",
      "Processed 19,960,000 tokens\n",
      "Processed 19,970,000 tokens\n",
      "Processed 19,980,000 tokens\n",
      "Processed 19,990,000 tokens\n",
      "Processed 20,000,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_train_split/1880/book_05.txt\n",
      "Processed 20,010,000 tokens\n",
      "Processed 20,020,000 tokens\n",
      "Processed 20,030,000 tokens\n",
      "Processed 20,040,000 tokens\n",
      "Processed 20,050,000 tokens\n",
      "Processed 20,060,000 tokens\n",
      "Processed 20,070,000 tokens\n",
      "Processed 20,080,000 tokens\n",
      "Processed 20,090,000 tokens\n",
      "Processed 20,100,000 tokens\n",
      "Processed 20,110,000 tokens\n",
      "Processed 20,120,000 tokens\n",
      "Processed 20,130,000 tokens\n",
      "Processed 20,140,000 tokens\n",
      "Processed 20,150,000 tokens\n",
      "Processed 20,160,000 tokens\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_train_split/1880/book_06.txt\n",
      "Processed 20,170,000 tokens\n",
      "Processed 20,180,000 tokens\n",
      "Processed 20,190,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_train_split/1880/book_08.txt\n",
      "Processed 20,200,000 tokens\n",
      "Processed 20,210,000 tokens\n",
      "Processed 20,220,000 tokens\n",
      "Processed 20,230,000 tokens\n",
      "Processed 20,240,000 tokens\n",
      "Processed 20,250,000 tokens\n",
      "Processed 20,260,000 tokens\n",
      "Processed 20,270,000 tokens\n",
      "Processed 20,280,000 tokens\n",
      "Processed 20,290,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_train_split/1880/book_09.txt\n",
      "Processed 20,300,000 tokens\n",
      "Processed 20,310,000 tokens\n",
      "Processed 20,320,000 tokens\n",
      "Processed 20,330,000 tokens\n",
      "Processed 20,340,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_train_split/1880/book_10.txt\n",
      "Processed 20,350,000 tokens\n",
      "Processed 20,360,000 tokens\n",
      "Processed 20,370,000 tokens\n",
      "Processed 20,380,000 tokens\n",
      "Processed 20,390,000 tokens\n",
      "Processed 20,400,000 tokens\n",
      "Processed 20,410,000 tokens\n",
      "Processed 20,420,000 tokens\n",
      "Processed 20,430,000 tokens\n",
      "Processed 20,440,000 tokens\n",
      "Processed 20,450,000 tokens\n",
      "Processed 20,460,000 tokens\n",
      "Processed 20,470,000 tokens\n",
      "Processed 20,480,000 tokens\n",
      "tokenize file book_12.txt\n",
      "./Datasets/clean_train_split/1880/book_12.txt\n",
      "Processed 20,490,000 tokens\n",
      "Processed 20,500,000 tokens\n",
      "Processed 20,510,000 tokens\n",
      "Processed 20,520,000 tokens\n",
      "Processed 20,530,000 tokens\n",
      "Processed 20,540,000 tokens\n",
      "Processed 20,550,000 tokens\n",
      "Processed 20,560,000 tokens\n",
      "Processed 20,570,000 tokens\n",
      "Processed 20,580,000 tokens\n",
      "Processing decade: 1890\n",
      "number of files in 1890 directory: 9\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_train_split/1890/book_01.txt\n",
      "Processed 20,590,000 tokens\n",
      "Processed 20,600,000 tokens\n",
      "Processed 20,610,000 tokens\n",
      "Processed 20,620,000 tokens\n",
      "Processed 20,630,000 tokens\n",
      "Processed 20,640,000 tokens\n",
      "Processed 20,650,000 tokens\n",
      "Processed 20,660,000 tokens\n",
      "Processed 20,670,000 tokens\n",
      "Processed 20,680,000 tokens\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_train_split/1890/book_03.txt\n",
      "Processed 20,690,000 tokens\n",
      "Processed 20,700,000 tokens\n",
      "Processed 20,710,000 tokens\n",
      "Processed 20,720,000 tokens\n",
      "Processed 20,730,000 tokens\n",
      "Processed 20,740,000 tokens\n",
      "Processed 20,750,000 tokens\n",
      "Processed 20,760,000 tokens\n",
      "Processed 20,770,000 tokens\n",
      "Processed 20,780,000 tokens\n",
      "Processed 20,790,000 tokens\n",
      "Processed 20,800,000 tokens\n",
      "Processed 20,810,000 tokens\n",
      "Processed 20,820,000 tokens\n",
      "Processed 20,830,000 tokens\n",
      "Processed 20,840,000 tokens\n",
      "Processed 20,850,000 tokens\n",
      "Processed 20,860,000 tokens\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_train_split/1890/book_04.txt\n",
      "Processed 20,870,000 tokens\n",
      "Processed 20,880,000 tokens\n",
      "Processed 20,890,000 tokens\n",
      "Processed 20,900,000 tokens\n",
      "Processed 20,910,000 tokens\n",
      "Processed 20,920,000 tokens\n",
      "Processed 20,930,000 tokens\n",
      "Processed 20,940,000 tokens\n",
      "Processed 20,950,000 tokens\n",
      "Processed 20,960,000 tokens\n",
      "Processed 20,970,000 tokens\n",
      "Processed 20,980,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_train_split/1890/book_05.txt\n",
      "Processed 20,990,000 tokens\n",
      "Processed 21,000,000 tokens\n",
      "Processed 21,010,000 tokens\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_train_split/1890/book_06.txt\n",
      "Processed 21,020,000 tokens\n",
      "Processed 21,030,000 tokens\n",
      "Processed 21,040,000 tokens\n",
      "Processed 21,050,000 tokens\n",
      "Processed 21,060,000 tokens\n",
      "Processed 21,070,000 tokens\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_train_split/1890/book_07.txt\n",
      "Processed 21,080,000 tokens\n",
      "Processed 21,090,000 tokens\n",
      "Processed 21,100,000 tokens\n",
      "Processed 21,110,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_train_split/1890/book_08.txt\n",
      "Processed 21,120,000 tokens\n",
      "Processed 21,130,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_train_split/1890/book_10.txt\n",
      "Processed 21,140,000 tokens\n",
      "Processed 21,150,000 tokens\n",
      "Processed 21,160,000 tokens\n",
      "Processed 21,170,000 tokens\n",
      "Processed 21,180,000 tokens\n",
      "Processed 21,190,000 tokens\n",
      "Processed 21,200,000 tokens\n",
      "Processed 21,210,000 tokens\n",
      "Processed 21,220,000 tokens\n",
      "Processed 21,230,000 tokens\n",
      "Processed 21,240,000 tokens\n",
      "Processed 21,250,000 tokens\n",
      "Processed 21,260,000 tokens\n",
      "Processed 21,270,000 tokens\n",
      "Processed 21,280,000 tokens\n",
      "Processed 21,290,000 tokens\n",
      "Processed 21,300,000 tokens\n",
      "Processed 21,310,000 tokens\n",
      "Processed 21,320,000 tokens\n",
      "tokenize file book_12.txt\n",
      "./Datasets/clean_train_split/1890/book_12.txt\n",
      "Processed 21,330,000 tokens\n",
      "Processed 21,340,000 tokens\n",
      "Processed 21,350,000 tokens\n",
      "Processed 21,360,000 tokens\n",
      "Processed 21,370,000 tokens\n",
      "Processed 21,380,000 tokens\n",
      "Processed 21,390,000 tokens\n",
      "Processed 21,400,000 tokens\n",
      "Processed 21,410,000 tokens\n",
      "Processed 21,420,000 tokens\n"
     ]
    }
   ],
   "source": [
    "train_text_data, train_labels = text_tokenizer.process_files(clean_train_split_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91abec7",
   "metadata": {},
   "source": [
    "#### Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e6bca4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing decade: 1770\n",
      "number of files in 1770 directory: 3\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_test_split/1770/book_01.txt\n",
      "Processed 21,430,000 tokens\n",
      "Processed 21,440,000 tokens\n",
      "Processed 21,450,000 tokens\n",
      "Processed 21,460,000 tokens\n",
      "Processed 21,470,000 tokens\n",
      "Processed 21,480,000 tokens\n",
      "Processed 21,490,000 tokens\n",
      "Processed 21,500,000 tokens\n",
      "Processed 21,510,000 tokens\n",
      "Processed 21,520,000 tokens\n",
      "Processed 21,530,000 tokens\n",
      "Processed 21,540,000 tokens\n",
      "Processed 21,550,000 tokens\n",
      "Processed 21,560,000 tokens\n",
      "Processed 21,570,000 tokens\n",
      "Processed 21,580,000 tokens\n",
      "Processed 21,590,000 tokens\n",
      "Processed 21,600,000 tokens\n",
      "Processed 21,610,000 tokens\n",
      "Processed 21,620,000 tokens\n",
      "Processed 21,630,000 tokens\n",
      "Processed 21,640,000 tokens\n",
      "Processed 21,650,000 tokens\n",
      "Processed 21,660,000 tokens\n",
      "Processed 21,670,000 tokens\n",
      "Processed 21,680,000 tokens\n",
      "Processed 21,690,000 tokens\n",
      "Processed 21,700,000 tokens\n",
      "Processed 21,710,000 tokens\n",
      "Processed 21,720,000 tokens\n",
      "Processed 21,730,000 tokens\n",
      "Processed 21,740,000 tokens\n",
      "Processed 21,750,000 tokens\n",
      "Processed 21,760,000 tokens\n",
      "Processed 21,770,000 tokens\n",
      "Processed 21,780,000 tokens\n",
      "Processed 21,790,000 tokens\n",
      "Processed 21,800,000 tokens\n",
      "Processed 21,810,000 tokens\n",
      "Processed 21,820,000 tokens\n",
      "Processed 21,830,000 tokens\n",
      "Processed 21,840,000 tokens\n",
      "Processed 21,850,000 tokens\n",
      "Processed 21,860,000 tokens\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_test_split/1770/book_02.txt\n",
      "Processed 21,870,000 tokens\n",
      "Processed 21,880,000 tokens\n",
      "Processed 21,890,000 tokens\n",
      "tokenize file book_11.txt\n",
      "./Datasets/clean_test_split/1770/book_11.txt\n",
      "Processed 21,900,000 tokens\n",
      "Processed 21,910,000 tokens\n",
      "Processed 21,920,000 tokens\n",
      "Processed 21,930,000 tokens\n",
      "Processing decade: 1780\n",
      "number of files in 1780 directory: 3\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_test_split/1780/book_01.txt\n",
      "Processed 21,940,000 tokens\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_test_split/1780/book_02.txt\n",
      "Processed 21,950,000 tokens\n",
      "Processed 21,960,000 tokens\n",
      "Processed 21,970,000 tokens\n",
      "Processed 21,980,000 tokens\n",
      "Processed 21,990,000 tokens\n",
      "Processed 22,000,000 tokens\n",
      "Processed 22,010,000 tokens\n",
      "Processed 22,020,000 tokens\n",
      "Processed 22,030,000 tokens\n",
      "Processed 22,040,000 tokens\n",
      "Processed 22,050,000 tokens\n",
      "Processed 22,060,000 tokens\n",
      "Processed 22,070,000 tokens\n",
      "Processed 22,080,000 tokens\n",
      "tokenize file book_12.txt\n",
      "./Datasets/clean_test_split/1780/book_12.txt\n",
      "Processed 22,090,000 tokens\n",
      "Processed 22,100,000 tokens\n",
      "Processed 22,110,000 tokens\n",
      "Processed 22,120,000 tokens\n",
      "Processed 22,130,000 tokens\n",
      "Processing decade: 1790\n",
      "number of files in 1790 directory: 3\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_test_split/1790/book_04.txt\n",
      "Processed 22,140,000 tokens\n",
      "Processed 22,150,000 tokens\n",
      "Processed 22,160,000 tokens\n",
      "Processed 22,170,000 tokens\n",
      "Processed 22,180,000 tokens\n",
      "Processed 22,190,000 tokens\n",
      "Processed 22,200,000 tokens\n",
      "Processed 22,210,000 tokens\n",
      "Processed 22,220,000 tokens\n",
      "Processed 22,230,000 tokens\n",
      "Processed 22,240,000 tokens\n",
      "Processed 22,250,000 tokens\n",
      "Processed 22,260,000 tokens\n",
      "Processed 22,270,000 tokens\n",
      "Processed 22,280,000 tokens\n",
      "Processed 22,290,000 tokens\n",
      "tokenize file book_05.txt\n",
      "./Datasets/clean_test_split/1790/book_05.txt\n",
      "Processed 22,300,000 tokens\n",
      "Processed 22,310,000 tokens\n",
      "Processed 22,320,000 tokens\n",
      "Processed 22,330,000 tokens\n",
      "Processed 22,340,000 tokens\n",
      "Processed 22,350,000 tokens\n",
      "Processed 22,360,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_test_split/1790/book_08.txt\n",
      "Processed 22,370,000 tokens\n",
      "Processed 22,380,000 tokens\n",
      "Processed 22,390,000 tokens\n",
      "Processed 22,400,000 tokens\n",
      "Processed 22,410,000 tokens\n",
      "Processing decade: 1800\n",
      "number of files in 1800 directory: 2\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_test_split/1800/book_04.txt\n",
      "Processed 22,420,000 tokens\n",
      "Processed 22,430,000 tokens\n",
      "Processed 22,440,000 tokens\n",
      "Processed 22,450,000 tokens\n",
      "Processed 22,460,000 tokens\n",
      "Processed 22,470,000 tokens\n",
      "Processed 22,480,000 tokens\n",
      "Processed 22,490,000 tokens\n",
      "Processed 22,500,000 tokens\n",
      "Processed 22,510,000 tokens\n",
      "Processed 22,520,000 tokens\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_test_split/1800/book_06.txt\n",
      "Processed 22,530,000 tokens\n",
      "Processed 22,540,000 tokens\n",
      "Processed 22,550,000 tokens\n",
      "Processed 22,560,000 tokens\n",
      "Processed 22,570,000 tokens\n",
      "Processed 22,580,000 tokens\n",
      "Processed 22,590,000 tokens\n",
      "Processed 22,600,000 tokens\n",
      "Processed 22,610,000 tokens\n",
      "Processed 22,620,000 tokens\n",
      "Processed 22,630,000 tokens\n",
      "Processed 22,640,000 tokens\n",
      "Processing decade: 1810\n",
      "number of files in 1810 directory: 3\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_test_split/1810/book_01.txt\n",
      "Processed 22,650,000 tokens\n",
      "Processed 22,660,000 tokens\n",
      "Processed 22,670,000 tokens\n",
      "Processed 22,680,000 tokens\n",
      "Processed 22,690,000 tokens\n",
      "Processed 22,700,000 tokens\n",
      "Processed 22,710,000 tokens\n",
      "Processed 22,720,000 tokens\n",
      "Processed 22,730,000 tokens\n",
      "Processed 22,740,000 tokens\n",
      "tokenize file book_12.txt\n",
      "./Datasets/clean_test_split/1810/book_12.txt\n",
      "Processed 22,750,000 tokens\n",
      "Processed 22,760,000 tokens\n",
      "Processed 22,770,000 tokens\n",
      "Processed 22,780,000 tokens\n",
      "Processed 22,790,000 tokens\n",
      "Processed 22,800,000 tokens\n",
      "Processed 22,810,000 tokens\n",
      "Processed 22,820,000 tokens\n",
      "Processed 22,830,000 tokens\n",
      "Processed 22,840,000 tokens\n",
      "Processed 22,850,000 tokens\n",
      "Processed 22,860,000 tokens\n",
      "Processed 22,870,000 tokens\n",
      "Processed 22,880,000 tokens\n",
      "Processed 22,890,000 tokens\n",
      "Processed 22,900,000 tokens\n",
      "Processed 22,910,000 tokens\n",
      "Processed 22,920,000 tokens\n",
      "Processed 22,930,000 tokens\n",
      "Processed 22,940,000 tokens\n",
      "Processed 22,950,000 tokens\n",
      "Processed 22,960,000 tokens\n",
      "tokenize file book_13.txt\n",
      "./Datasets/clean_test_split/1810/book_13.txt\n",
      "Processed 22,970,000 tokens\n",
      "Processed 22,980,000 tokens\n",
      "Processed 22,990,000 tokens\n",
      "Processed 23,000,000 tokens\n",
      "Processed 23,010,000 tokens\n",
      "Processed 23,020,000 tokens\n",
      "Processed 23,030,000 tokens\n",
      "Processing decade: 1820\n",
      "number of files in 1820 directory: 3\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_test_split/1820/book_01.txt\n",
      "Processed 23,040,000 tokens\n",
      "Processed 23,050,000 tokens\n",
      "Processed 23,060,000 tokens\n",
      "Processed 23,070,000 tokens\n",
      "Processed 23,080,000 tokens\n",
      "Processed 23,090,000 tokens\n",
      "Processed 23,100,000 tokens\n",
      "Processed 23,110,000 tokens\n",
      "Processed 23,120,000 tokens\n",
      "Processed 23,130,000 tokens\n",
      "Processed 23,140,000 tokens\n",
      "Processed 23,150,000 tokens\n",
      "Processed 23,160,000 tokens\n",
      "Processed 23,170,000 tokens\n",
      "Processed 23,180,000 tokens\n",
      "Processed 23,190,000 tokens\n",
      "Processed 23,200,000 tokens\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_test_split/1820/book_02.txt\n",
      "Processed 23,210,000 tokens\n",
      "Processed 23,220,000 tokens\n",
      "Processed 23,230,000 tokens\n",
      "Processed 23,240,000 tokens\n",
      "Processed 23,250,000 tokens\n",
      "Processed 23,260,000 tokens\n",
      "Processed 23,270,000 tokens\n",
      "Processed 23,280,000 tokens\n",
      "Processed 23,290,000 tokens\n",
      "Processed 23,300,000 tokens\n",
      "Processed 23,310,000 tokens\n",
      "Processed 23,320,000 tokens\n",
      "Processed 23,330,000 tokens\n",
      "Processed 23,340,000 tokens\n",
      "Processed 23,350,000 tokens\n",
      "Processed 23,360,000 tokens\n",
      "Processed 23,370,000 tokens\n",
      "Processed 23,380,000 tokens\n",
      "Processed 23,390,000 tokens\n",
      "Processed 23,400,000 tokens\n",
      "Processed 23,410,000 tokens\n",
      "tokenize file book_12.txt\n",
      "./Datasets/clean_test_split/1820/book_12.txt\n",
      "Processed 23,420,000 tokens\n",
      "Processed 23,430,000 tokens\n",
      "Processed 23,440,000 tokens\n",
      "Processed 23,450,000 tokens\n",
      "Processed 23,460,000 tokens\n",
      "Processed 23,470,000 tokens\n",
      "Processed 23,480,000 tokens\n",
      "Processed 23,490,000 tokens\n",
      "Processed 23,500,000 tokens\n",
      "Processed 23,510,000 tokens\n",
      "Processed 23,520,000 tokens\n",
      "Processed 23,530,000 tokens\n",
      "Processed 23,540,000 tokens\n",
      "Processed 23,550,000 tokens\n",
      "Processed 23,560,000 tokens\n",
      "Processed 23,570,000 tokens\n",
      "Processed 23,580,000 tokens\n",
      "Processed 23,590,000 tokens\n",
      "Processed 23,600,000 tokens\n",
      "Processed 23,610,000 tokens\n",
      "Processed 23,620,000 tokens\n",
      "Processed 23,630,000 tokens\n",
      "Processed 23,640,000 tokens\n",
      "Processing decade: 1830\n",
      "number of files in 1830 directory: 3\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_test_split/1830/book_03.txt\n",
      "Processed 23,650,000 tokens\n",
      "Processed 23,660,000 tokens\n",
      "Processed 23,670,000 tokens\n",
      "Processed 23,680,000 tokens\n",
      "Processed 23,690,000 tokens\n",
      "Processed 23,700,000 tokens\n",
      "Processed 23,710,000 tokens\n",
      "Processed 23,720,000 tokens\n",
      "Processed 23,730,000 tokens\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_test_split/1830/book_06.txt\n",
      "Processed 23,740,000 tokens\n",
      "Processed 23,750,000 tokens\n",
      "Processed 23,760,000 tokens\n",
      "Processed 23,770,000 tokens\n",
      "Processed 23,780,000 tokens\n",
      "Processed 23,790,000 tokens\n",
      "Processed 23,800,000 tokens\n",
      "Processed 23,810,000 tokens\n",
      "Processed 23,820,000 tokens\n",
      "Processed 23,830,000 tokens\n",
      "Processed 23,840,000 tokens\n",
      "Processed 23,850,000 tokens\n",
      "Processed 23,860,000 tokens\n",
      "Processed 23,870,000 tokens\n",
      "Processed 23,880,000 tokens\n",
      "Processed 23,890,000 tokens\n",
      "Processed 23,900,000 tokens\n",
      "Processed 23,910,000 tokens\n",
      "Processed 23,920,000 tokens\n",
      "Processed 23,930,000 tokens\n",
      "tokenize file book_11.txt\n",
      "./Datasets/clean_test_split/1830/book_11.txt\n",
      "Processed 23,940,000 tokens\n",
      "Processed 23,950,000 tokens\n",
      "Processed 23,960,000 tokens\n",
      "Processed 23,970,000 tokens\n",
      "Processed 23,980,000 tokens\n",
      "Processed 23,990,000 tokens\n",
      "Processed 24,000,000 tokens\n",
      "Processed 24,010,000 tokens\n",
      "Processed 24,020,000 tokens\n",
      "Processed 24,030,000 tokens\n",
      "Processed 24,040,000 tokens\n",
      "Processed 24,050,000 tokens\n",
      "Processed 24,060,000 tokens\n",
      "Processed 24,070,000 tokens\n",
      "Processed 24,080,000 tokens\n",
      "Processed 24,090,000 tokens\n",
      "Processed 24,100,000 tokens\n",
      "Processed 24,110,000 tokens\n",
      "Processed 24,120,000 tokens\n",
      "Processing decade: 1840\n",
      "number of files in 1840 directory: 3\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_test_split/1840/book_04.txt\n",
      "Processed 24,130,000 tokens\n",
      "Processed 24,140,000 tokens\n",
      "Processed 24,150,000 tokens\n",
      "Processed 24,160,000 tokens\n",
      "Processed 24,170,000 tokens\n",
      "Processed 24,180,000 tokens\n",
      "Processed 24,190,000 tokens\n",
      "Processed 24,200,000 tokens\n",
      "Processed 24,210,000 tokens\n",
      "Processed 24,220,000 tokens\n",
      "Processed 24,230,000 tokens\n",
      "Processed 24,240,000 tokens\n",
      "Processed 24,250,000 tokens\n",
      "Processed 24,260,000 tokens\n",
      "Processed 24,270,000 tokens\n",
      "Processed 24,280,000 tokens\n",
      "Processed 24,290,000 tokens\n",
      "Processed 24,300,000 tokens\n",
      "Processed 24,310,000 tokens\n",
      "Processed 24,320,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_test_split/1840/book_09.txt\n",
      "Processed 24,330,000 tokens\n",
      "Processed 24,340,000 tokens\n",
      "Processed 24,350,000 tokens\n",
      "Processed 24,360,000 tokens\n",
      "Processed 24,370,000 tokens\n",
      "Processed 24,380,000 tokens\n",
      "Processed 24,390,000 tokens\n",
      "Processed 24,400,000 tokens\n",
      "Processed 24,410,000 tokens\n",
      "Processed 24,420,000 tokens\n",
      "Processed 24,430,000 tokens\n",
      "Processed 24,440,000 tokens\n",
      "Processed 24,450,000 tokens\n",
      "Processed 24,460,000 tokens\n",
      "Processed 24,470,000 tokens\n",
      "Processed 24,480,000 tokens\n",
      "Processed 24,490,000 tokens\n",
      "Processed 24,500,000 tokens\n",
      "Processed 24,510,000 tokens\n",
      "Processed 24,520,000 tokens\n",
      "Processed 24,530,000 tokens\n",
      "Processed 24,540,000 tokens\n",
      "Processed 24,550,000 tokens\n",
      "Processed 24,560,000 tokens\n",
      "Processed 24,570,000 tokens\n",
      "Processed 24,580,000 tokens\n",
      "Processed 24,590,000 tokens\n",
      "Processed 24,600,000 tokens\n",
      "Processed 24,610,000 tokens\n",
      "Processed 24,620,000 tokens\n",
      "Processed 24,630,000 tokens\n",
      "Processed 24,640,000 tokens\n",
      "Processed 24,650,000 tokens\n",
      "Processed 24,660,000 tokens\n",
      "Processed 24,670,000 tokens\n",
      "Processed 24,680,000 tokens\n",
      "Processed 24,690,000 tokens\n",
      "Processed 24,700,000 tokens\n",
      "Processed 24,710,000 tokens\n",
      "Processed 24,720,000 tokens\n",
      "Processed 24,730,000 tokens\n",
      "Processed 24,740,000 tokens\n",
      "Processed 24,750,000 tokens\n",
      "Processed 24,760,000 tokens\n",
      "Processed 24,770,000 tokens\n",
      "Processed 24,780,000 tokens\n",
      "Processed 24,790,000 tokens\n",
      "Processed 24,800,000 tokens\n",
      "Processed 24,810,000 tokens\n",
      "Processed 24,820,000 tokens\n",
      "Processed 24,830,000 tokens\n",
      "Processed 24,840,000 tokens\n",
      "Processed 24,850,000 tokens\n",
      "Processed 24,860,000 tokens\n",
      "Processed 24,870,000 tokens\n",
      "Processed 24,880,000 tokens\n",
      "Processed 24,890,000 tokens\n",
      "tokenize file book_12.txt\n",
      "./Datasets/clean_test_split/1840/book_12.txt\n",
      "Processed 24,900,000 tokens\n",
      "Processed 24,910,000 tokens\n",
      "Processed 24,920,000 tokens\n",
      "Processed 24,930,000 tokens\n",
      "Processed 24,940,000 tokens\n",
      "Processed 24,950,000 tokens\n",
      "Processed 24,960,000 tokens\n",
      "Processed 24,970,000 tokens\n",
      "Processed 24,980,000 tokens\n",
      "Processed 24,990,000 tokens\n",
      "Processed 25,000,000 tokens\n",
      "Processed 25,010,000 tokens\n",
      "Processed 25,020,000 tokens\n",
      "Processed 25,030,000 tokens\n",
      "Processed 25,040,000 tokens\n",
      "Processed 25,050,000 tokens\n",
      "Processed 25,060,000 tokens\n",
      "Processed 25,070,000 tokens\n",
      "Processed 25,080,000 tokens\n",
      "Processed 25,090,000 tokens\n",
      "Processing decade: 1850\n",
      "number of files in 1850 directory: 3\n",
      "tokenize file book_01.txt\n",
      "./Datasets/clean_test_split/1850/book_01.txt\n",
      "Processed 25,100,000 tokens\n",
      "Processed 25,110,000 tokens\n",
      "Processed 25,120,000 tokens\n",
      "Processed 25,130,000 tokens\n",
      "Processed 25,140,000 tokens\n",
      "Processed 25,150,000 tokens\n",
      "Processed 25,160,000 tokens\n",
      "tokenize file book_04.txt\n",
      "./Datasets/clean_test_split/1850/book_04.txt\n",
      "Processed 25,170,000 tokens\n",
      "Processed 25,180,000 tokens\n",
      "Processed 25,190,000 tokens\n",
      "Processed 25,200,000 tokens\n",
      "Processed 25,210,000 tokens\n",
      "Processed 25,220,000 tokens\n",
      "Processed 25,230,000 tokens\n",
      "Processed 25,240,000 tokens\n",
      "Processed 25,250,000 tokens\n",
      "Processed 25,260,000 tokens\n",
      "Processed 25,270,000 tokens\n",
      "Processed 25,280,000 tokens\n",
      "Processed 25,290,000 tokens\n",
      "Processed 25,300,000 tokens\n",
      "Processed 25,310,000 tokens\n",
      "Processed 25,320,000 tokens\n",
      "Processed 25,330,000 tokens\n",
      "Processed 25,340,000 tokens\n",
      "Processed 25,350,000 tokens\n",
      "Processed 25,360,000 tokens\n",
      "Processed 25,370,000 tokens\n",
      "Processed 25,380,000 tokens\n",
      "Processed 25,390,000 tokens\n",
      "Processed 25,400,000 tokens\n",
      "tokenize file book_13.txt\n",
      "./Datasets/clean_test_split/1850/book_13.txt\n",
      "Processed 25,410,000 tokens\n",
      "Processed 25,420,000 tokens\n",
      "Processed 25,430,000 tokens\n",
      "Processed 25,440,000 tokens\n",
      "Processed 25,450,000 tokens\n",
      "Processed 25,460,000 tokens\n",
      "Processed 25,470,000 tokens\n",
      "Processed 25,480,000 tokens\n",
      "Processed 25,490,000 tokens\n",
      "Processed 25,500,000 tokens\n",
      "Processed 25,510,000 tokens\n",
      "Processed 25,520,000 tokens\n",
      "Processed 25,530,000 tokens\n",
      "Processed 25,540,000 tokens\n",
      "Processed 25,550,000 tokens\n",
      "Processed 25,560,000 tokens\n",
      "Processed 25,570,000 tokens\n",
      "Processed 25,580,000 tokens\n",
      "Processed 25,590,000 tokens\n",
      "Processed 25,600,000 tokens\n",
      "Processed 25,610,000 tokens\n",
      "Processed 25,620,000 tokens\n",
      "Processed 25,630,000 tokens\n",
      "Processed 25,640,000 tokens\n",
      "Processed 25,650,000 tokens\n",
      "Processed 25,660,000 tokens\n",
      "Processing decade: 1860\n",
      "number of files in 1860 directory: 3\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_test_split/1860/book_07.txt\n",
      "Processed 25,670,000 tokens\n",
      "Processed 25,680,000 tokens\n",
      "Processed 25,690,000 tokens\n",
      "Processed 25,700,000 tokens\n",
      "Processed 25,710,000 tokens\n",
      "Processed 25,720,000 tokens\n",
      "Processed 25,730,000 tokens\n",
      "Processed 25,740,000 tokens\n",
      "Processed 25,750,000 tokens\n",
      "Processed 25,760,000 tokens\n",
      "Processed 25,770,000 tokens\n",
      "Processed 25,780,000 tokens\n",
      "Processed 25,790,000 tokens\n",
      "Processed 25,800,000 tokens\n",
      "Processed 25,810,000 tokens\n",
      "Processed 25,820,000 tokens\n",
      "Processed 25,830,000 tokens\n",
      "Processed 25,840,000 tokens\n",
      "Processed 25,850,000 tokens\n",
      "Processed 25,860,000 tokens\n",
      "Processed 25,870,000 tokens\n",
      "Processed 25,880,000 tokens\n",
      "Processed 25,890,000 tokens\n",
      "tokenize file book_08.txt\n",
      "./Datasets/clean_test_split/1860/book_08.txt\n",
      "Processed 25,900,000 tokens\n",
      "Processed 25,910,000 tokens\n",
      "Processed 25,920,000 tokens\n",
      "Processed 25,930,000 tokens\n",
      "Processed 25,940,000 tokens\n",
      "Processed 25,950,000 tokens\n",
      "Processed 25,960,000 tokens\n",
      "Processed 25,970,000 tokens\n",
      "Processed 25,980,000 tokens\n",
      "Processed 25,990,000 tokens\n",
      "Processed 26,000,000 tokens\n",
      "Processed 26,010,000 tokens\n",
      "Processed 26,020,000 tokens\n",
      "Processed 26,030,000 tokens\n",
      "Processed 26,040,000 tokens\n",
      "Processed 26,050,000 tokens\n",
      "Processed 26,060,000 tokens\n",
      "Processed 26,070,000 tokens\n",
      "Processed 26,080,000 tokens\n",
      "Processed 26,090,000 tokens\n",
      "Processed 26,100,000 tokens\n",
      "Processed 26,110,000 tokens\n",
      "Processed 26,120,000 tokens\n",
      "Processed 26,130,000 tokens\n",
      "tokenize file book_11.txt\n",
      "./Datasets/clean_test_split/1860/book_11.txt\n",
      "Processed 26,140,000 tokens\n",
      "Processed 26,150,000 tokens\n",
      "Processed 26,160,000 tokens\n",
      "Processed 26,170,000 tokens\n",
      "Processed 26,180,000 tokens\n",
      "Processed 26,190,000 tokens\n",
      "Processed 26,200,000 tokens\n",
      "Processed 26,210,000 tokens\n",
      "Processed 26,220,000 tokens\n",
      "Processed 26,230,000 tokens\n",
      "Processed 26,240,000 tokens\n",
      "Processed 26,250,000 tokens\n",
      "Processed 26,260,000 tokens\n",
      "Processed 26,270,000 tokens\n",
      "Processed 26,280,000 tokens\n",
      "Processed 26,290,000 tokens\n",
      "Processed 26,300,000 tokens\n",
      "Processed 26,310,000 tokens\n",
      "Processed 26,320,000 tokens\n",
      "Processed 26,330,000 tokens\n",
      "Processed 26,340,000 tokens\n",
      "Processed 26,350,000 tokens\n",
      "Processed 26,360,000 tokens\n",
      "Processed 26,370,000 tokens\n",
      "Processed 26,380,000 tokens\n",
      "Processed 26,390,000 tokens\n",
      "Processing decade: 1870\n",
      "number of files in 1870 directory: 3\n",
      "tokenize file book_06.txt\n",
      "./Datasets/clean_test_split/1870/book_06.txt\n",
      "Processed 26,400,000 tokens\n",
      "Processed 26,410,000 tokens\n",
      "Processed 26,420,000 tokens\n",
      "Processed 26,430,000 tokens\n",
      "Processed 26,440,000 tokens\n",
      "Processed 26,450,000 tokens\n",
      "Processed 26,460,000 tokens\n",
      "Processed 26,470,000 tokens\n",
      "Processed 26,480,000 tokens\n",
      "Processed 26,490,000 tokens\n",
      "Processed 26,500,000 tokens\n",
      "Processed 26,510,000 tokens\n",
      "Processed 26,520,000 tokens\n",
      "Processed 26,530,000 tokens\n",
      "Processed 26,540,000 tokens\n",
      "Processed 26,550,000 tokens\n",
      "Processed 26,560,000 tokens\n",
      "Processed 26,570,000 tokens\n",
      "Processed 26,580,000 tokens\n",
      "Processed 26,590,000 tokens\n",
      "Processed 26,600,000 tokens\n",
      "Processed 26,610,000 tokens\n",
      "Processed 26,620,000 tokens\n",
      "Processed 26,630,000 tokens\n",
      "Processed 26,640,000 tokens\n",
      "Processed 26,650,000 tokens\n",
      "Processed 26,660,000 tokens\n",
      "Processed 26,670,000 tokens\n",
      "Processed 26,680,000 tokens\n",
      "Processed 26,690,000 tokens\n",
      "Processed 26,700,000 tokens\n",
      "Processed 26,710,000 tokens\n",
      "Processed 26,720,000 tokens\n",
      "Processed 26,730,000 tokens\n",
      "Processed 26,740,000 tokens\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_test_split/1870/book_07.txt\n",
      "Processed 26,750,000 tokens\n",
      "Processed 26,760,000 tokens\n",
      "Processed 26,770,000 tokens\n",
      "Processed 26,780,000 tokens\n",
      "Processed 26,790,000 tokens\n",
      "Processed 26,800,000 tokens\n",
      "Processed 26,810,000 tokens\n",
      "Processed 26,820,000 tokens\n",
      "Processed 26,830,000 tokens\n",
      "tokenize file book_10.txt\n",
      "./Datasets/clean_test_split/1870/book_10.txt\n",
      "Processed 26,840,000 tokens\n",
      "Processed 26,850,000 tokens\n",
      "Processed 26,860,000 tokens\n",
      "Processed 26,870,000 tokens\n",
      "Processed 26,880,000 tokens\n",
      "Processed 26,890,000 tokens\n",
      "Processed 26,900,000 tokens\n",
      "Processed 26,910,000 tokens\n",
      "Processed 26,920,000 tokens\n",
      "Processed 26,930,000 tokens\n",
      "Processed 26,940,000 tokens\n",
      "Processed 26,950,000 tokens\n",
      "Processed 26,960,000 tokens\n",
      "Processed 26,970,000 tokens\n",
      "Processed 26,980,000 tokens\n",
      "Processed 26,990,000 tokens\n",
      "Processed 27,000,000 tokens\n",
      "Processed 27,010,000 tokens\n",
      "Processed 27,020,000 tokens\n",
      "Processed 27,030,000 tokens\n",
      "Processed 27,040,000 tokens\n",
      "Processed 27,050,000 tokens\n",
      "Processed 27,060,000 tokens\n",
      "Processed 27,070,000 tokens\n",
      "Processed 27,080,000 tokens\n",
      "Processed 27,090,000 tokens\n",
      "Processed 27,100,000 tokens\n",
      "Processed 27,110,000 tokens\n",
      "Processed 27,120,000 tokens\n",
      "Processed 27,130,000 tokens\n",
      "Processed 27,140,000 tokens\n",
      "Processed 27,150,000 tokens\n",
      "Processed 27,160,000 tokens\n",
      "Processed 27,170,000 tokens\n",
      "Processed 27,180,000 tokens\n",
      "Processed 27,190,000 tokens\n",
      "Processed 27,200,000 tokens\n",
      "Processed 27,210,000 tokens\n",
      "Processed 27,220,000 tokens\n",
      "Processed 27,230,000 tokens\n",
      "Processed 27,240,000 tokens\n",
      "Processed 27,250,000 tokens\n",
      "Processed 27,260,000 tokens\n",
      "Processed 27,270,000 tokens\n",
      "Processing decade: 1880\n",
      "number of files in 1880 directory: 3\n",
      "tokenize file book_03.txt\n",
      "./Datasets/clean_test_split/1880/book_03.txt\n",
      "Processed 27,280,000 tokens\n",
      "Processed 27,290,000 tokens\n",
      "Processed 27,300,000 tokens\n",
      "Processed 27,310,000 tokens\n",
      "Processed 27,320,000 tokens\n",
      "Processed 27,330,000 tokens\n",
      "Processed 27,340,000 tokens\n",
      "Processed 27,350,000 tokens\n",
      "tokenize file book_07.txt\n",
      "./Datasets/clean_test_split/1880/book_07.txt\n",
      "Processed 27,360,000 tokens\n",
      "Processed 27,370,000 tokens\n",
      "Processed 27,380,000 tokens\n",
      "Processed 27,390,000 tokens\n",
      "Processed 27,400,000 tokens\n",
      "Processed 27,410,000 tokens\n",
      "Processed 27,420,000 tokens\n",
      "Processed 27,430,000 tokens\n",
      "Processed 27,440,000 tokens\n",
      "Processed 27,450,000 tokens\n",
      "Processed 27,460,000 tokens\n",
      "Processed 27,470,000 tokens\n",
      "Processed 27,480,000 tokens\n",
      "Processed 27,490,000 tokens\n",
      "tokenize file book_11.txt\n",
      "./Datasets/clean_test_split/1880/book_11.txt\n",
      "Processed 27,500,000 tokens\n",
      "Processed 27,510,000 tokens\n",
      "Processed 27,520,000 tokens\n",
      "Processed 27,530,000 tokens\n",
      "Processed 27,540,000 tokens\n",
      "Processed 27,550,000 tokens\n",
      "Processed 27,560,000 tokens\n",
      "Processed 27,570,000 tokens\n",
      "Processed 27,580,000 tokens\n",
      "Processed 27,590,000 tokens\n",
      "Processed 27,600,000 tokens\n",
      "Processed 27,610,000 tokens\n",
      "Processed 27,620,000 tokens\n",
      "Processed 27,630,000 tokens\n",
      "Processed 27,640,000 tokens\n",
      "Processed 27,650,000 tokens\n",
      "Processed 27,660,000 tokens\n",
      "Processed 27,670,000 tokens\n",
      "Processed 27,680,000 tokens\n",
      "Processed 27,690,000 tokens\n",
      "Processed 27,700,000 tokens\n",
      "Processed 27,710,000 tokens\n",
      "Processed 27,720,000 tokens\n",
      "Processed 27,730,000 tokens\n",
      "Processing decade: 1890\n",
      "number of files in 1890 directory: 3\n",
      "tokenize file book_02.txt\n",
      "./Datasets/clean_test_split/1890/book_02.txt\n",
      "Processed 27,740,000 tokens\n",
      "Processed 27,750,000 tokens\n",
      "Processed 27,760,000 tokens\n",
      "Processed 27,770,000 tokens\n",
      "Processed 27,780,000 tokens\n",
      "tokenize file book_09.txt\n",
      "./Datasets/clean_test_split/1890/book_09.txt\n",
      "Processed 27,790,000 tokens\n",
      "Processed 27,800,000 tokens\n",
      "Processed 27,810,000 tokens\n",
      "Processed 27,820,000 tokens\n",
      "Processed 27,830,000 tokens\n",
      "Processed 27,840,000 tokens\n",
      "Processed 27,850,000 tokens\n",
      "tokenize file book_11.txt\n",
      "./Datasets/clean_test_split/1890/book_11.txt\n",
      "Processed 27,860,000 tokens\n",
      "Processed 27,870,000 tokens\n",
      "Processed 27,880,000 tokens\n",
      "Processed 27,890,000 tokens\n",
      "Processed 27,900,000 tokens\n",
      "Processed 27,910,000 tokens\n"
     ]
    }
   ],
   "source": [
    "test_text_data, test_labels = text_tokenizer.process_files(clean_test_split_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28973e84",
   "metadata": {},
   "source": [
    "### Create Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33dd020f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1770: 0, 1780: 1, 1790: 2, 1800: 3, 1810: 4, 1820: 5, 1830: 6, 1840: 7, 1850: 8, 1860: 9, 1870: 10, 1880: 11, 1890: 12}\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(set(train_labels + test_labels))\n",
    "decade2label = {decade: i for i,decade in enumerate(labels)}\n",
    "print(f\"{decade2label}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c1ad2e",
   "metadata": {},
   "source": [
    "### Check Tokenizer + Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f774a958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train labels -> 84860\n",
      "length of train text(paragraphs) -> 84860\n",
      "\n",
      "number of test labels -> 25538\n",
      "length of test text -> 25538\n",
      "\n",
      "train text Produced by Gary R. Young THE SCHOOL FOR SCANDAL A COMEDY A PORTRAIT<1> BY R. B. SHERIDAN, ESQ. Transcriber's Comments on the preparation of this E-Text: SQUARE BRACKETS: The square brackets, i.e. [ ] are copied from the printed book, without change, except that a closing bracket \"]\" has been added to the stage directions. FOOTNOTES: For this E-Text version of the book, the footnotes have been consolidated at the end of the play. Numbering of the footnotes has been changed, and each footnote is given a unique identity in the form <X>. CHANGES TO THE TEXT: Character names have been expanded. For Example, SIR BENJAMIN was SIR BEN. THE TEXT OF THE SCHOOL FOR SCANDAL The text of THE SCHOOL FOR SCANDAL in this edition is taken, by Mr. Fraser Rae's generous permission, from his SHERIDAN'S PLAYS NOW PRINTED AS HE WROTE THEM. In his Prefatory Notes (xxxvii), Mr. Rae writes: \"The manuscript of it [THE SCHOOL FOR SCANDAL] in Sheridan's own handwriting is preserved at Frampton Court and is now printed in this volume. This version differs in many respects from that which is generally known, and I think it is even better than that which has hitherto been read and acted. As I have endeavoured to reproduce\n",
      "train label 1770\n",
      "\n",
      "test text(paragraphs) An Inquiry into the Nature and Causes of the Wealth of Nations by Adam Smith Contents INTRODUCTION AND PLAN OF THE WORK. BOOK I. OF THE CAUSES OF IMPROVEMENT IN THE PRODUCTIVE POWERS OF LABOUR, AND OF THE ORDER ACCORDING TO WHICH ITS PRODUCE IS NATURALLY DISTRIBUTED AMONG THE DIFFERENT RANKS OF THE PEOPLE. CHAPTER I. OF THE DIVISION OF LABOUR. CHAPTER II. OF THE PRINCIPLE WHICH GIVES OCCASION TO THE DIVISION OF LABOUR. CHAPTER III. THAT THE DIVISION OF LABOUR IS LIMITED BY THE EXTENT OF THE MARKET. CHAPTER IV. OF THE ORIGIN AND USE OF MONEY. CHAPTER V. OF THE REAL AND NOMINAL PRICE OF COMMODITIES, OR OF THEIR PRICE IN LABOUR, AND THEIR PRICE IN MONEY. CHAPTER VI. OF THE COMPONENT PART OF THE PRICE OF COMMODITIES. CHAPTER VII. OF THE NATURAL AND MARKET PRICE OF COMMODITIES. CHAPTER VIII. OF THE WAGES OF LABOUR. CHAPTER IX. OF THE PROFITS OF STOCK. CHAPTER X. OF WAGES AND PROFIT IN THE DIFFERENT EMPLOYMENTS OF LABOUR AND STOCK. CHAPTER XI. OF THE RENT OF LAND. BOOK II. OF THE NATURE, ACCUMULATION, AND EMPLOYMENT OF STOCK. CHAPTER I. OF THE DIVISION OF STOCK. CHAPTER II. OF MONEY, CONSIDERED AS A PARTICULAR BRANCH OF THE GENERAL STOCK OF THE SOCIETY, OR OF\n",
      "test label 1770\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of train labels -> {len(train_labels)}\")\n",
    "print(f\"length of train text(paragraphs) -> {len(train_text_data)}\")\n",
    "print()\n",
    "\n",
    "print(f\"number of test labels -> {len(test_labels)}\")\n",
    "print(f\"length of test text -> {len(test_text_data)}\")\n",
    "print()\n",
    "\n",
    "print(f\"train text {train_text_data[0]}\")\n",
    "print(f\"train label {train_labels[0]}\")\n",
    "print()\n",
    "\n",
    "print(f\"test text(paragraphs) {test_text_data[0]}\")\n",
    "print(f\"test label {test_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "636e3c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample -> Produced by Gary R. Young THE SCHOOL FOR SCANDAL A COMEDY A PORTRAIT<1> BY R. B. SHERIDAN, ESQ. Transcriber's Comments on the preparation of this E-Text: SQUARE BRACKETS: The square brackets, i.e. [ ] are copied from the printed book, without change, except that a closing bracket \"]\" has been added to the stage directions. FOOTNOTES: For this E-Text version of the book, the footnotes have been consolidated at the end of the play. Numbering of the footnotes has been changed, and each footnote is given a unique identity in the form <X>. CHANGES TO THE TEXT: Character names have been expanded. For Example, SIR BENJAMIN was SIR BEN. THE TEXT OF THE SCHOOL FOR SCANDAL The text of THE SCHOOL FOR SCANDAL in this edition is taken, by Mr. Fraser Rae's generous permission, from his SHERIDAN'S PLAYS NOW PRINTED AS HE WROTE THEM. In his Prefatory Notes (xxxvii), Mr. Rae writes: \"The manuscript of it [THE SCHOOL FOR SCANDAL] in Sheridan's own handwriting is preserved at Frampton Court and is now printed in this volume. This version differs in many respects from that which is generally known, and I think it is even better than that which has hitherto been read and acted. As I have endeavoured to reproduce\n",
      "train sample labe -> 1770\n",
      "tokenized train_sample -> [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 11, 13, 14, 15, 16, 3, 5, 17, 18, 19, 20, 21, 22, 23, 24, 25, 7, 26, 27, 28, 29, 30, 31, 32, 30, 7, 31, 32, 19, 33, 21, 34, 35, 36, 37, 38, 7, 39, 40, 19, 41, 42, 19, 43, 44, 11, 45, 46, 47, 35, 48, 49, 50, 51, 52, 7, 53, 54, 21, 55, 30, 9, 28, 29, 56, 27, 7, 40, 19, 7, 55, 57, 50, 58, 59, 7, 60, 27, 7, 61, 21, 62, 27, 7, 55, 49, 50, 63, 19, 64, 65, 66, 67, 68, 11, 69, 70, 71, 7, 72, 14, 73, 16, 21, 74, 52, 7, 75, 30, 76, 77, 57, 50, 78, 21, 9, 79, 19, 80, 81, 82, 80, 83, 21, 7, 75, 27, 7, 8, 9, 10, 7, 75, 27, 7, 8, 9, 10, 71, 28, 84, 67, 85, 19, 3, 86, 87, 88, 23, 89, 90, 19, 38, 91, 18, 23, 92, 93, 39, 94, 95, 96, 97, 21, 71, 91, 98, 99, 100, 101, 102, 19, 86, 88, 103, 30, 47, 7, 104, 27, 105, 34, 7, 8, 9, 10, 35, 71, 18, 23, 106, 107, 67, 108, 59, 109, 110, 64, 67, 93, 39, 71, 28, 111, 21, 28, 56, 112, 71, 113, 114, 38, 44, 115, 67, 116, 117, 19, 64, 118, 119, 105, 67, 120, 121, 122, 44, 115, 49, 123, 50, 124, 64, 125, 21, 94, 118, 57, 126, 52, 127]\n",
      "length of tokenized word 252\n"
     ]
    }
   ],
   "source": [
    "train_sample = train_text_data[0]\n",
    "train_sample_label = train_labels[0]\n",
    "word_ids = text_tokenizer.tokenize_text_to_id(train_sample)\n",
    "\n",
    "print(f\"train sample -> {train_sample}\")\n",
    "print(f\"train sample labe -> {train_sample_label}\")\n",
    "print(f\"tokenized train_sample -> {word_ids}\")\n",
    "print(f\"length of tokenized word {len(word_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65d8f952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sample -> An Inquiry into the Nature and Causes of the Wealth of Nations by Adam Smith Contents INTRODUCTION AND PLAN OF THE WORK. BOOK I. OF THE CAUSES OF IMPROVEMENT IN THE PRODUCTIVE POWERS OF LABOUR, AND OF THE ORDER ACCORDING TO WHICH ITS PRODUCE IS NATURALLY DISTRIBUTED AMONG THE DIFFERENT RANKS OF THE PEOPLE. CHAPTER I. OF THE DIVISION OF LABOUR. CHAPTER II. OF THE PRINCIPLE WHICH GIVES OCCASION TO THE DIVISION OF LABOUR. CHAPTER III. THAT THE DIVISION OF LABOUR IS LIMITED BY THE EXTENT OF THE MARKET. CHAPTER IV. OF THE ORIGIN AND USE OF MONEY. CHAPTER V. OF THE REAL AND NOMINAL PRICE OF COMMODITIES, OR OF THEIR PRICE IN LABOUR, AND THEIR PRICE IN MONEY. CHAPTER VI. OF THE COMPONENT PART OF THE PRICE OF COMMODITIES. CHAPTER VII. OF THE NATURAL AND MARKET PRICE OF COMMODITIES. CHAPTER VIII. OF THE WAGES OF LABOUR. CHAPTER IX. OF THE PROFITS OF STOCK. CHAPTER X. OF WAGES AND PROFIT IN THE DIFFERENT EMPLOYMENTS OF LABOUR AND STOCK. CHAPTER XI. OF THE RENT OF LAND. BOOK II. OF THE NATURE, ACCUMULATION, AND EMPLOYMENT OF STOCK. CHAPTER I. OF THE DIVISION OF STOCK. CHAPTER II. OF MONEY, CONSIDERED AS A PARTICULAR BRANCH OF THE GENERAL STOCK OF THE SOCIETY, OR OF\n",
      "test sample label -> 1770\n",
      "tokenized test_sample -> [213, 4503, 1171, 7, 346, 64, 5959, 27, 7, 5805, 27, 4787, 3, 37690, 729, 4827, 9485, 64, 3338, 27, 7, 4017, 21, 40, 210, 27, 7, 5959, 27, 8492, 71, 7, 7859, 13668, 27, 4546, 19, 64, 27, 7, 3374, 207, 52, 115, 850, 2461, 67, 1970, 8448, 1243, 7, 167, 12025, 27, 7, 1186, 21, 4006, 210, 27, 7, 11304, 27, 4546, 21, 4006, 1825, 21, 27, 7, 1607, 115, 2343, 3429, 52, 7, 11304, 27, 4546, 21, 4006, 2403, 21, 44, 7, 11304, 27, 4546, 67, 18473, 3, 7, 8134, 27, 7, 4142, 21, 4006, 2934, 21, 27, 7, 12512, 64, 177, 27, 2425, 21, 4006, 14814, 27, 7, 1115, 64, 34090, 3011, 27, 36241, 19, 143, 27, 537, 3011, 71, 4546, 19, 64, 537, 3011, 71, 2425, 21, 4006, 12946, 21, 27, 7, 70920, 169, 27, 7, 3011, 27, 36241, 21, 4006, 14815, 21, 27, 7, 1039, 64, 4142, 3011, 27, 36241, 21, 4006, 14816, 21, 27, 7, 2669, 27, 4546, 21, 4006, 14817, 21, 27, 7, 4054, 27, 2522, 21, 4006, 14818, 27, 2669, 64, 1123, 71, 7, 167, 19097, 27, 4546, 64, 2522, 21, 4006, 14819, 21, 27, 7, 7593, 27, 2847, 21, 40, 1825, 21, 27, 7, 346, 19, 14077, 19, 64, 9218, 27, 2522, 21, 4006, 210, 27, 7, 11304, 27, 2522, 21, 4006, 1825, 21, 27, 2425, 19, 4865, 94, 11, 2195, 7525, 27, 7, 2184, 2522, 27, 7, 1997, 19, 143, 27]\n",
      "length of tokenized word 242\n"
     ]
    }
   ],
   "source": [
    "test_sample = test_text_data[0]\n",
    "test_sample_label = test_labels[0]\n",
    "word_ids = text_tokenizer.tokenize_text_to_id(test_sample)\n",
    "\n",
    "print(f\"test sample -> {test_sample}\")\n",
    "print(f\"test sample label -> {test_sample_label}\")\n",
    "print(f\"tokenized test_sample -> {word_ids}\")\n",
    "print(f\"length of tokenized word {len(word_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ad33f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workbench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
