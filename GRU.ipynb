{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38cde6a-5334-400f-b277-27ffefff7cc5",
   "metadata": {},
   "source": [
    "# RNN Architecture - GRU RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b8eb6-40a5-4b13-be6e-1a4030a24ca0",
   "metadata": {},
   "source": [
    "## Setup - Libraries, Packages, Embeddings, Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9734d7f-45ae-4d7a-aed6-4b052c750249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import urllib.request\n",
    "import zipfile \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e7ae1d9f-73ba-48eb-8666-4197fc8f60ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"./Embeddings\"\n",
    "def download_progress(block_num, block_size, total_size):\n",
    "    if not hasattr(download_progress, \"pbar\"):\n",
    "        download_progress.pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True)\n",
    "    download_progress.pbar.update(block_size)\n",
    "\n",
    "if not os.path.exists(embeddings_path):\n",
    "    print(f\"create directory to store pre-trained glove embeddings\")\n",
    "    os.makedirs(embeddings_path)\n",
    "    print(f\"download pre-trained Glove Embeddings\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "        \"./Embeddings/glove.6B.zip\",\n",
    "        download_progress,\n",
    "    )\n",
    "    print(\"unpack embeddings\")\n",
    "    with zipfile.ZipFile(\"./Embeddings/glove.6B.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"./Embeddings/\")\n",
    "    os.remove(\"./Embeddings/glove.6B.zip\")\n",
    "    \n",
    "    print(\"embeddings download complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "664b703a-92c8-4051-b031-32242ab9bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6b_50_path = \"./Embeddings/glove.6B.50d.txt\"\n",
    "glove_6b_100_path = \"./Embeddings/glove.6B.100d.txt\"\n",
    "glove_6b_200_path = \"./Embeddings/glove.6B.200d.txt\"\n",
    "glove_6b_300_path = \"./Embeddings/glove.6B.300d.txt\"\n",
    "clean_train_split_path = \"./Datasets/clean_train_split/\"\n",
    "clean_test_split_path = \"./Datasets/clean_test_split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "72149524-7cac-4343-8bbf-6599e361e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download(\"punkt_tab\")\n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "\n",
    "class HistoricalTextTokenizer:\n",
    "    \"\"\"\n",
    "    All of this code is adapted from Professor Johan Boye's DD2417 assignment tokenizers \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2id = defaultdict(lambda: None)\n",
    "        self.id2word = defaultdict(lambda: None)\n",
    "        self.latest_new_word = -1 \n",
    "        self.tokens_processed = 0 \n",
    "\n",
    "        self.UNKNOWN = '<unk>'\n",
    "        self.PADDING_WORD = '<pad>'\n",
    "\n",
    "        self.get_word_id(self.PADDING_WORD)\n",
    "        self.get_word_id(self.UNKNOWN)\n",
    "\n",
    "    def get_word_id(self, word):\n",
    "        word = word.lower()\n",
    "        if word in self.word2id:\n",
    "            return self.word2id[word]\n",
    "        else:\n",
    "            self.latest_new_word += 1\n",
    "            self.id2word[self.latest_new_word] = word\n",
    "            self.word2id[word] = self.latest_new_word\n",
    "            return self.latest_new_word\n",
    "\n",
    "    def process_files(self, file_or_dir):\n",
    "        all_texts = []\n",
    "        all_labels = []\n",
    "\n",
    "        if os.path.isdir(file_or_dir):\n",
    "            decade_dirs = sorted([d for d in os.listdir(file_or_dir) if os.path.isdir(os.path.join(file_or_dir, d))])\n",
    "            for decade_dir in decade_dirs:\n",
    "                decade_path = os.path.join(file_or_dir, decade_dir)\n",
    "                decade = int(decade_dir)\n",
    "                print(f\"Processing decade: {decade}\")\n",
    "                text_files = sorted([f for f in os.listdir(decade_path) if f.endswith(\".txt\")])\n",
    "                # print(f\"number of files in {decade} directory: {len(text_files)}\")\n",
    "\n",
    "                for file in text_files:\n",
    "                    filepath = os.path.join(decade_path, file)\n",
    "                    # print(f\"tokenize file {file}\")\n",
    "                    text, labels = self.process_file(filepath, decade)\n",
    "                    all_texts.extend(text)\n",
    "                    all_labels.extend(labels)\n",
    "        else:\n",
    "            texts, labels = self.process_file(file_or_dir, 0)\n",
    "            all_texts.extend(texts)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "        return all_texts, all_labels\n",
    "\n",
    "    def process_file(self, filepath, decade):\n",
    "        # print(filepath)\n",
    "        stream = open(filepath, mode=\"r\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "        text = stream.read()\n",
    "        stream.close()\n",
    "\n",
    "        try:\n",
    "            self.tokens = nltk.word_tokenize(text)\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt\")\n",
    "            self.tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        for i, token in enumerate(self.tokens):\n",
    "            self.tokens_processed += 1\n",
    "            word_id = self.get_word_id(token)\n",
    "\n",
    "            if self.tokens_processed % 1000000000 == 0:\n",
    "                print(\"Processed\", \"{:,}\".format(self.tokens_processed), \"tokens\")\n",
    "\n",
    "        paragraphs = self.create_paragraphs(text)\n",
    "        labels = [decade] * len(paragraphs)\n",
    "\n",
    "        return paragraphs, labels\n",
    "\n",
    "    def create_paragraphs(self, text, min_words=10, max_words=210):\n",
    "        words = text.split()\n",
    "        paragraphs = []\n",
    "        start = 0\n",
    "\n",
    "        while start < len(words):\n",
    "            end = min(start + max_words, len(words))\n",
    "            paragraph_words = words[start:end]\n",
    "            if len(paragraph_words) >= min_words:\n",
    "                paragraph_text = \" \".join(paragraph_words)\n",
    "                paragraphs.append(paragraph_text)\n",
    "            start = end\n",
    "\n",
    "        return paragraphs \n",
    "\n",
    "    def tokenize_text_to_id(self, text):\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt\")\n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "        word_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.word2id:\n",
    "                word_ids.append(self.word2id[token])\n",
    "            else:\n",
    "                word_ids.append(self.word2id[self.UNKNOWN])\n",
    "        return word_ids\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3fab5d48-f7bc-422c-9f85-1f2415325a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = HistoricalTextTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a37f298-d874-43fb-b42e-f7659d012868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing decade: 1770\n",
      "Processing decade: 1780\n",
      "Processing decade: 1790\n",
      "Processing decade: 1800\n",
      "Processing decade: 1810\n",
      "Processing decade: 1820\n",
      "Processing decade: 1830\n",
      "Processing decade: 1840\n",
      "Processing decade: 1850\n",
      "Processing decade: 1860\n",
      "Processing decade: 1870\n",
      "Processing decade: 1880\n",
      "Processing decade: 1890\n"
     ]
    }
   ],
   "source": [
    "train_text_data, train_labels = text_tokenizer.process_files(clean_train_split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e39b739b-96ad-4ba3-950d-994ddc17435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing decade: 1770\n",
      "Processing decade: 1780\n",
      "Processing decade: 1790\n",
      "Processing decade: 1800\n",
      "Processing decade: 1810\n",
      "Processing decade: 1820\n",
      "Processing decade: 1830\n",
      "Processing decade: 1840\n",
      "Processing decade: 1850\n",
      "Processing decade: 1860\n",
      "Processing decade: 1870\n",
      "Processing decade: 1880\n",
      "Processing decade: 1890\n"
     ]
    }
   ],
   "source": [
    "test_text_data, test_labels = text_tokenizer.process_files(clean_test_split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c33e483a-7013-49b3-be27-c16fcc7d1a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1770: 0, 1780: 1, 1790: 2, 1800: 3, 1810: 4, 1820: 5, 1830: 6, 1840: 7, 1850: 8, 1860: 9, 1870: 10, 1880: 11, 1890: 12}\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(set(train_labels + test_labels))\n",
    "decade_to_label = {decade: i for i, decade in enumerate(labels)}\n",
    "print(f\"{decade_to_label}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e279346a-62f2-44b5-bbb8-09cc3fb8d41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train labels -> 84860\n",
      "length of train text(paragraphs) -> 84860\n",
      "\n",
      "number of test labels -> 25538\n",
      "length of test text -> 25538\n",
      "\n",
      "train text Produced by Gary R. Young THE SCHOOL FOR SCANDAL A COMEDY A PORTRAIT<1> BY R. B. SHERIDAN, ESQ. Transcriber's Comments on the preparation of this E-Text: SQUARE BRACKETS: The square brackets, i.e. [ ] are copied from the printed book, without change, except that a closing bracket \"]\" has been added to the stage directions. FOOTNOTES: For this E-Text version of the book, the footnotes have been consolidated at the end of the play. Numbering of the footnotes has been changed, and each footnote is given a unique identity in the form <X>. CHANGES TO THE TEXT: Character names have been expanded. For Example, SIR BENJAMIN was SIR BEN. THE TEXT OF THE SCHOOL FOR SCANDAL The text of THE SCHOOL FOR SCANDAL in this edition is taken, by Mr. Fraser Rae's generous permission, from his SHERIDAN'S PLAYS NOW PRINTED AS HE WROTE THEM. In his Prefatory Notes (xxxvii), Mr. Rae writes: \"The manuscript of it [THE SCHOOL FOR SCANDAL] in Sheridan's own handwriting is preserved at Frampton Court and is now printed in this volume. This version differs in many respects from that which is generally known, and I think it is even better than that which has hitherto been read and acted. As I have endeavoured to reproduce\n",
      "train label 1770\n",
      "\n",
      "test text(paragraphs) An Inquiry into the Nature and Causes of the Wealth of Nations by Adam Smith Contents INTRODUCTION AND PLAN OF THE WORK. BOOK I. OF THE CAUSES OF IMPROVEMENT IN THE PRODUCTIVE POWERS OF LABOUR, AND OF THE ORDER ACCORDING TO WHICH ITS PRODUCE IS NATURALLY DISTRIBUTED AMONG THE DIFFERENT RANKS OF THE PEOPLE. CHAPTER I. OF THE DIVISION OF LABOUR. CHAPTER II. OF THE PRINCIPLE WHICH GIVES OCCASION TO THE DIVISION OF LABOUR. CHAPTER III. THAT THE DIVISION OF LABOUR IS LIMITED BY THE EXTENT OF THE MARKET. CHAPTER IV. OF THE ORIGIN AND USE OF MONEY. CHAPTER V. OF THE REAL AND NOMINAL PRICE OF COMMODITIES, OR OF THEIR PRICE IN LABOUR, AND THEIR PRICE IN MONEY. CHAPTER VI. OF THE COMPONENT PART OF THE PRICE OF COMMODITIES. CHAPTER VII. OF THE NATURAL AND MARKET PRICE OF COMMODITIES. CHAPTER VIII. OF THE WAGES OF LABOUR. CHAPTER IX. OF THE PROFITS OF STOCK. CHAPTER X. OF WAGES AND PROFIT IN THE DIFFERENT EMPLOYMENTS OF LABOUR AND STOCK. CHAPTER XI. OF THE RENT OF LAND. BOOK II. OF THE NATURE, ACCUMULATION, AND EMPLOYMENT OF STOCK. CHAPTER I. OF THE DIVISION OF STOCK. CHAPTER II. OF MONEY, CONSIDERED AS A PARTICULAR BRANCH OF THE GENERAL STOCK OF THE SOCIETY, OR OF\n",
      "test label 1770\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of train labels -> {len(train_labels)}\")\n",
    "print(f\"length of train text(paragraphs) -> {len(train_text_data)}\")\n",
    "print()\n",
    "\n",
    "print(f\"number of test labels -> {len(test_labels)}\")\n",
    "print(f\"length of test text -> {len(test_text_data)}\")\n",
    "print()\n",
    "\n",
    "print(f\"train text {train_text_data[0]}\")\n",
    "print(f\"train label {train_labels[0]}\")\n",
    "print()\n",
    "\n",
    "print(f\"test text(paragraphs) {test_text_data[0]}\")\n",
    "print(f\"test label {test_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e4d1c21-4dad-434e-bb70-9f2d9c28f5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample -> Produced by Gary R. Young THE SCHOOL FOR SCANDAL A COMEDY A PORTRAIT<1> BY R. B. SHERIDAN, ESQ. Transcriber's Comments on the preparation of this E-Text: SQUARE BRACKETS: The square brackets, i.e. [ ] are copied from the printed book, without change, except that a closing bracket \"]\" has been added to the stage directions. FOOTNOTES: For this E-Text version of the book, the footnotes have been consolidated at the end of the play. Numbering of the footnotes has been changed, and each footnote is given a unique identity in the form <X>. CHANGES TO THE TEXT: Character names have been expanded. For Example, SIR BENJAMIN was SIR BEN. THE TEXT OF THE SCHOOL FOR SCANDAL The text of THE SCHOOL FOR SCANDAL in this edition is taken, by Mr. Fraser Rae's generous permission, from his SHERIDAN'S PLAYS NOW PRINTED AS HE WROTE THEM. In his Prefatory Notes (xxxvii), Mr. Rae writes: \"The manuscript of it [THE SCHOOL FOR SCANDAL] in Sheridan's own handwriting is preserved at Frampton Court and is now printed in this volume. This version differs in many respects from that which is generally known, and I think it is even better than that which has hitherto been read and acted. As I have endeavoured to reproduce\n",
      "train sample labe -> 1770\n",
      "tokenized train_sample -> [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 11, 13, 14, 15, 16, 3, 5, 17, 18, 19, 20, 21, 22, 23, 24, 25, 7, 26, 27, 28, 29, 30, 31, 32, 30, 7, 31, 32, 19, 33, 21, 34, 35, 36, 37, 38, 7, 39, 40, 19, 41, 42, 19, 43, 44, 11, 45, 46, 47, 35, 48, 49, 50, 51, 52, 7, 53, 54, 21, 55, 30, 9, 28, 29, 56, 27, 7, 40, 19, 7, 55, 57, 50, 58, 59, 7, 60, 27, 7, 61, 21, 62, 27, 7, 55, 49, 50, 63, 19, 64, 65, 66, 67, 68, 11, 69, 70, 71, 7, 72, 14, 73, 16, 21, 74, 52, 7, 75, 30, 76, 77, 57, 50, 78, 21, 9, 79, 19, 80, 81, 82, 80, 83, 21, 7, 75, 27, 7, 8, 9, 10, 7, 75, 27, 7, 8, 9, 10, 71, 28, 84, 67, 85, 19, 3, 86, 87, 88, 23, 89, 90, 19, 38, 91, 18, 23, 92, 93, 39, 94, 95, 96, 97, 21, 71, 91, 98, 99, 100, 101, 102, 19, 86, 88, 103, 30, 47, 7, 104, 27, 105, 34, 7, 8, 9, 10, 35, 71, 18, 23, 106, 107, 67, 108, 59, 109, 110, 64, 67, 93, 39, 71, 28, 111, 21, 28, 56, 112, 71, 113, 114, 38, 44, 115, 67, 116, 117, 19, 64, 118, 119, 105, 67, 120, 121, 122, 44, 115, 49, 123, 50, 124, 64, 125, 21, 94, 118, 57, 126, 52, 127]\n",
      "length of tokenized word 252\n"
     ]
    }
   ],
   "source": [
    "train_sample = train_text_data[0]\n",
    "train_sample_label = train_labels[0]\n",
    "word_ids = text_tokenizer.tokenize_text_to_id(train_sample)\n",
    "\n",
    "print(f\"train sample -> {train_sample}\")\n",
    "print(f\"train sample labe -> {train_sample_label}\")\n",
    "print(f\"tokenized train_sample -> {word_ids}\")\n",
    "print(f\"length of tokenized word {len(word_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e02cb2ae-ec66-490d-bbc9-d2ce6a87c95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sample -> An Inquiry into the Nature and Causes of the Wealth of Nations by Adam Smith Contents INTRODUCTION AND PLAN OF THE WORK. BOOK I. OF THE CAUSES OF IMPROVEMENT IN THE PRODUCTIVE POWERS OF LABOUR, AND OF THE ORDER ACCORDING TO WHICH ITS PRODUCE IS NATURALLY DISTRIBUTED AMONG THE DIFFERENT RANKS OF THE PEOPLE. CHAPTER I. OF THE DIVISION OF LABOUR. CHAPTER II. OF THE PRINCIPLE WHICH GIVES OCCASION TO THE DIVISION OF LABOUR. CHAPTER III. THAT THE DIVISION OF LABOUR IS LIMITED BY THE EXTENT OF THE MARKET. CHAPTER IV. OF THE ORIGIN AND USE OF MONEY. CHAPTER V. OF THE REAL AND NOMINAL PRICE OF COMMODITIES, OR OF THEIR PRICE IN LABOUR, AND THEIR PRICE IN MONEY. CHAPTER VI. OF THE COMPONENT PART OF THE PRICE OF COMMODITIES. CHAPTER VII. OF THE NATURAL AND MARKET PRICE OF COMMODITIES. CHAPTER VIII. OF THE WAGES OF LABOUR. CHAPTER IX. OF THE PROFITS OF STOCK. CHAPTER X. OF WAGES AND PROFIT IN THE DIFFERENT EMPLOYMENTS OF LABOUR AND STOCK. CHAPTER XI. OF THE RENT OF LAND. BOOK II. OF THE NATURE, ACCUMULATION, AND EMPLOYMENT OF STOCK. CHAPTER I. OF THE DIVISION OF STOCK. CHAPTER II. OF MONEY, CONSIDERED AS A PARTICULAR BRANCH OF THE GENERAL STOCK OF THE SOCIETY, OR OF\n",
      "test sample label -> 1770\n",
      "tokenized test_sample -> [213, 4503, 1171, 7, 346, 64, 5959, 27, 7, 5805, 27, 4787, 3, 37690, 729, 4827, 9485, 64, 3338, 27, 7, 4017, 21, 40, 210, 27, 7, 5959, 27, 8492, 71, 7, 7859, 13668, 27, 4546, 19, 64, 27, 7, 3374, 207, 52, 115, 850, 2461, 67, 1970, 8448, 1243, 7, 167, 12025, 27, 7, 1186, 21, 4006, 210, 27, 7, 11304, 27, 4546, 21, 4006, 1825, 21, 27, 7, 1607, 115, 2343, 3429, 52, 7, 11304, 27, 4546, 21, 4006, 2403, 21, 44, 7, 11304, 27, 4546, 67, 18473, 3, 7, 8134, 27, 7, 4142, 21, 4006, 2934, 21, 27, 7, 12512, 64, 177, 27, 2425, 21, 4006, 14814, 27, 7, 1115, 64, 34090, 3011, 27, 36241, 19, 143, 27, 537, 3011, 71, 4546, 19, 64, 537, 3011, 71, 2425, 21, 4006, 12946, 21, 27, 7, 70920, 169, 27, 7, 3011, 27, 36241, 21, 4006, 14815, 21, 27, 7, 1039, 64, 4142, 3011, 27, 36241, 21, 4006, 14816, 21, 27, 7, 2669, 27, 4546, 21, 4006, 14817, 21, 27, 7, 4054, 27, 2522, 21, 4006, 14818, 27, 2669, 64, 1123, 71, 7, 167, 19097, 27, 4546, 64, 2522, 21, 4006, 14819, 21, 27, 7, 7593, 27, 2847, 21, 40, 1825, 21, 27, 7, 346, 19, 14077, 19, 64, 9218, 27, 2522, 21, 4006, 210, 27, 7, 11304, 27, 2522, 21, 4006, 1825, 21, 27, 2425, 19, 4865, 94, 11, 2195, 7525, 27, 7, 2184, 2522, 27, 7, 1997, 19, 143, 27]\n",
      "length of tokenized word 242\n"
     ]
    }
   ],
   "source": [
    "test_sample = test_text_data[0]\n",
    "test_sample_label = test_labels[0]\n",
    "word_ids = text_tokenizer.tokenize_text_to_id(test_sample)\n",
    "\n",
    "print(f\"test sample -> {test_sample}\")\n",
    "print(f\"test sample label -> {test_sample_label}\")\n",
    "print(f\"tokenized test_sample -> {word_ids}\")\n",
    "print(f\"length of tokenized word {len(word_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3c572c98-6b01-44de-8982-d0148b304851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import codecs\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "13d584c2-c10e-4a0a-a4f8-d84cefb51e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create new tokenizer\n",
      "tokenize -> ./Datasets/clean_train_split/\n",
      "Processing decade: 1770\n",
      "Processing decade: 1780\n",
      "Processing decade: 1790\n",
      "Processing decade: 1800\n",
      "Processing decade: 1810\n",
      "Processing decade: 1820\n",
      "Processing decade: 1830\n",
      "Processing decade: 1840\n",
      "Processing decade: 1850\n",
      "Processing decade: 1860\n",
      "Processing decade: 1870\n",
      "Processing decade: 1880\n",
      "Processing decade: 1890\n",
      "succesfully tokenized <- ./Datasets/clean_train_split/\n",
      "tokenize -> ./Datasets/clean_test_split\n",
      "Processing decade: 1770\n",
      "Processing decade: 1780\n",
      "Processing decade: 1790\n",
      "Processing decade: 1800\n",
      "Processing decade: 1810\n",
      "Processing decade: 1820\n",
      "Processing decade: 1830\n",
      "Processing decade: 1840\n",
      "Processing decade: 1850\n",
      "Processing decade: 1860\n",
      "Processing decade: 1870\n",
      "Processing decade: 1880\n",
      "Processing decade: 1890\n",
      "succesfully tokenized <- ./Datasets/clean_test_split\n",
      "create decade labels\n",
      "successfully created decades labels\n"
     ]
    }
   ],
   "source": [
    "print(f\"create new tokenizer\")\n",
    "tokenizer = HistoricalTextTokenizer()\n",
    "\n",
    "print(f\"tokenize -> {clean_train_split_path}\")\n",
    "train_data, train_labels = tokenizer.process_files(clean_train_split_path)\n",
    "print(f\"succesfully tokenized <- {clean_train_split_path}\")\n",
    "\n",
    "print(f\"tokenize -> {clean_test_split_path}\")\n",
    "test_data, test_labels = tokenizer.process_files(clean_test_split_path)\n",
    "print(f\"succesfully tokenized <- {clean_test_split_path}\")\n",
    "\n",
    "print(f\"create decade labels\")\n",
    "decades = sorted(set(train_labels + test_labels))\n",
    "decade_to_label = {decade: i for i, decade in enumerate(decades)}\n",
    "label_to_decade = {i: decade for i, decade in enumerate(decades)}\n",
    "print(f\"successfully created decades labels\")\n",
    "\n",
    "UNKNOWN = \"<unk>\"  # Unknown char or unknown word\n",
    "PADDING_WORD = \"<pad>\"\n",
    "id_to_label = [f\"decade_{decade}\" for decade in decades]\n",
    "\n",
    "def label_to_id(decade):\n",
    "    return decade_to_label[decade]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5e6dedda-4b39-40f0-9017-bae8e73cdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(\n",
    "    embedding_file, padding_word=PADDING_WORD, unknown_word=UNKNOWN\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads Glove embeddings from a file.\n",
    "\n",
    "    Returns vector dimensionality, the word_to_id mapping (as a dict),\n",
    "    and the embeddings (as a list of lists).\n",
    "    \"\"\"\n",
    "    word_to_id = {}  # Dictionary to store word-to-ID mapping\n",
    "    word_to_id[padding_word] = 0\n",
    "    word_to_id[unknown_word] = 1\n",
    "    embeddings = []\n",
    "    with open(embedding_file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word = data[0]\n",
    "            vec = [float(x) for x in data[1:]]\n",
    "            embeddings.append(vec)\n",
    "            word_to_id[word] = len(word_to_id)\n",
    "    D = len(embeddings[0])\n",
    "\n",
    "    embeddings.insert(\n",
    "        word_to_id[padding_word], [0] * D\n",
    "    )  # <PAD> has an embedding of just zeros\n",
    "    embeddings.insert(\n",
    "        word_to_id[unknown_word], [-1] * D\n",
    "    )  # <UNK> has an embedding of just minus-ones\n",
    "\n",
    "    return D, word_to_id, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3222bc53-9609-4a46-b31f-7aad9ce90b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoricalTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class loading NER dataset from a CSV file to be used as an input\n",
    "    to PyTorch DataLoader.\n",
    "\n",
    "    The CSV file has 4 fields: sentence number (only at the start of a new\n",
    "    sentence), word, POS tag (ignored), and label.\n",
    "\n",
    "    Datapoints are sentences + associated labels for each word. If the\n",
    "    words have not been seen before (i.e, they are not found in the\n",
    "    'word_to_id' dict), they will be mapped to the unknown word '<UNK>'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, word_to_id, decade_to_label):\n",
    "        self.texts = texts \n",
    "        self.labels = labels \n",
    "        self.word_to_id = word_to_id\n",
    "        self.decade_to_label = decade_to_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        decade = self.labels[idx]\n",
    "        label_id = self.decade_to_label[decade]\n",
    "\n",
    "        return text, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ea4ae535-18d8-4104-ad90-8a500707d095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding for the word 'good' looks like this:\n",
      "[-0.35586, 0.5213, -0.6107, -0.30131, 0.94862, -0.31539, -0.59831, 0.12188, -0.031943, 0.55695, -0.10621, 0.63399, -0.4734, -0.075895, 0.38247, 0.081569, 0.82214, 0.2222, -0.0083764, -0.7662, -0.56253, 0.61759, 0.20292, -0.048598, 0.87815, -1.6549, -0.77418, 0.15435, 0.94823, -0.3952, 3.7302, 0.82855, -0.14104, 0.016395, 0.21115, -0.036085, -0.15587, 0.86583, 0.26309, -0.71015, -0.03677, 0.0018282, -0.17704, 0.27032, 0.11026, 0.14133, -0.057322, 0.27207, 0.31305, 0.92771]\n",
      "\n",
      "There are 25538 documents in the testset\n",
      "Document 0 starts with: An Inquiry into the Nature and Causes of the Wealth of Nations by Adam Smith Contents INTRODUCTION A ...\n",
      "It has the label 0 which corresponds to decade 1770\n"
     ]
    }
   ],
   "source": [
    "# Let's check out some of these data structures\n",
    "dim, word_to_id, embeddings = load_glove_embeddings(glove_6b_50_path)\n",
    "tokenizer.word2id = word_to_id\n",
    "tokenizer.id2word = {v: k for k, v in word_to_id.items()}\n",
    "\n",
    "print(\"The embedding for the word 'good' looks like this:\")\n",
    "print(embeddings[word_to_id[\"good\"]])\n",
    "print()\n",
    "\n",
    "# Read the data we are going to use for testing the model\n",
    "test_set = HistoricalTextDataset(test_data, test_labels, word_to_id, decade_to_label\n",
    ")\n",
    "print(\"There are\", len(test_set), \"documents in the testset\")\n",
    "dp = 0\n",
    "text, label = test_set[dp]\n",
    "print(\"Document\", dp, \"starts with:\", text[:100], \"...\")\n",
    "print(\"It has the label\", label, \"which corresponds to decade\", label_to_decade[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3273f2ed-85af-45a9-aace-de112649eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence_documents(batch, padding_word=PADDING_WORD, max_length=500):\n",
    "    batch_data, batch_labels = zip(*batch)\n",
    "\n",
    "    # convert documents to word IDs\n",
    "    padded_data = []\n",
    "    for text in batch_data:\n",
    "        word_ids = tokenizer.tokenize_text_to_id(text)\n",
    "\n",
    "        # truncate if too long\n",
    "        if len(word_ids) > max_length:\n",
    "            word_ids = word_ids[:max_length]\n",
    "\n",
    "        # pad if too short\n",
    "        padding_id = word_to_id[padding_word]\n",
    "        while len(word_ids) < max_length:\n",
    "            word_ids.append(padding_id)\n",
    "\n",
    "        padded_data.append(word_ids)\n",
    "\n",
    "    padded_labels = list(batch_labels)\n",
    "    return padded_data, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d9fd2fbd-59df-4d5d-81f3-440589844a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1016,\n",
       "   23,\n",
       "   3447,\n",
       "   3471,\n",
       "   463,\n",
       "   2,\n",
       "   166,\n",
       "   12,\n",
       "   2676,\n",
       "   9,\n",
       "   2843,\n",
       "   9,\n",
       "   6035,\n",
       "   19797,\n",
       "   178,\n",
       "   12259,\n",
       "   23,\n",
       "   3471,\n",
       "   3270,\n",
       "   16156,\n",
       "   3,\n",
       "   156451,\n",
       "   4,\n",
       "   175133,\n",
       "   11,\n",
       "   1941,\n",
       "   15,\n",
       "   2,\n",
       "   4791,\n",
       "   5,\n",
       "   39,\n",
       "   1,\n",
       "   47,\n",
       "   1251,\n",
       "   21270,\n",
       "   47,\n",
       "   2,\n",
       "   1251,\n",
       "   21270,\n",
       "   3,\n",
       "   124647,\n",
       "   4,\n",
       "   2825,\n",
       "   5282,\n",
       "   34,\n",
       "   15070,\n",
       "   27,\n",
       "   2,\n",
       "   5605,\n",
       "   541,\n",
       "   3,\n",
       "   298,\n",
       "   513,\n",
       "   3,\n",
       "   2079,\n",
       "   14,\n",
       "   9,\n",
       "   2321,\n",
       "   14451,\n",
       "   30,\n",
       "   5282,\n",
       "   29,\n",
       "   33,\n",
       "   53,\n",
       "   297,\n",
       "   6,\n",
       "   2,\n",
       "   914,\n",
       "   6977,\n",
       "   4,\n",
       "   27072,\n",
       "   47,\n",
       "   12,\n",
       "   39,\n",
       "   1,\n",
       "   1064,\n",
       "   5,\n",
       "   2,\n",
       "   541,\n",
       "   3,\n",
       "   2,\n",
       "   27072,\n",
       "   35,\n",
       "   53,\n",
       "   8648,\n",
       "   24,\n",
       "   2,\n",
       "   158,\n",
       "   5,\n",
       "   2,\n",
       "   284,\n",
       "   4,\n",
       "   15056,\n",
       "   5,\n",
       "   2,\n",
       "   27072,\n",
       "   33,\n",
       "   53,\n",
       "   1167,\n",
       "   3,\n",
       "   7,\n",
       "   238,\n",
       "   24299,\n",
       "   16,\n",
       "   456,\n",
       "   9,\n",
       "   3008,\n",
       "   3006,\n",
       "   8,\n",
       "   2,\n",
       "   685,\n",
       "   19797,\n",
       "   1587,\n",
       "   12259,\n",
       "   4,\n",
       "   1048,\n",
       "   6,\n",
       "   2,\n",
       "   2831,\n",
       "   47,\n",
       "   1397,\n",
       "   1471,\n",
       "   35,\n",
       "   53,\n",
       "   2854,\n",
       "   4,\n",
       "   12,\n",
       "   882,\n",
       "   3,\n",
       "   2701,\n",
       "   4069,\n",
       "   17,\n",
       "   2701,\n",
       "   2665,\n",
       "   4,\n",
       "   2,\n",
       "   2831,\n",
       "   5,\n",
       "   2,\n",
       "   166,\n",
       "   12,\n",
       "   2676,\n",
       "   2,\n",
       "   2831,\n",
       "   5,\n",
       "   2,\n",
       "   166,\n",
       "   12,\n",
       "   2676,\n",
       "   8,\n",
       "   39,\n",
       "   2494,\n",
       "   16,\n",
       "   494,\n",
       "   3,\n",
       "   23,\n",
       "   1997,\n",
       "   9904,\n",
       "   19693,\n",
       "   11,\n",
       "   7736,\n",
       "   4249,\n",
       "   3,\n",
       "   27,\n",
       "   28,\n",
       "   16156,\n",
       "   11,\n",
       "   1383,\n",
       "   116,\n",
       "   5605,\n",
       "   21,\n",
       "   20,\n",
       "   838,\n",
       "   103,\n",
       "   4,\n",
       "   8,\n",
       "   28,\n",
       "   154180,\n",
       "   2144,\n",
       "   25,\n",
       "   107881,\n",
       "   26,\n",
       "   3,\n",
       "   1997,\n",
       "   19693,\n",
       "   3059,\n",
       "   47,\n",
       "   30,\n",
       "   2,\n",
       "   9019,\n",
       "   5,\n",
       "   22,\n",
       "   2825,\n",
       "   2,\n",
       "   166,\n",
       "   12,\n",
       "   2676,\n",
       "   5282,\n",
       "   8,\n",
       "   16156,\n",
       "   11,\n",
       "   263,\n",
       "   23704,\n",
       "   16,\n",
       "   6731,\n",
       "   24,\n",
       "   50666,\n",
       "   204,\n",
       "   7,\n",
       "   16,\n",
       "   116,\n",
       "   5605,\n",
       "   8,\n",
       "   39,\n",
       "   2074,\n",
       "   4,\n",
       "   39,\n",
       "   1064,\n",
       "   13551,\n",
       "   8,\n",
       "   111,\n",
       "   9145,\n",
       "   27,\n",
       "   14,\n",
       "   44,\n",
       "   16,\n",
       "   1575,\n",
       "   227,\n",
       "   3,\n",
       "   7,\n",
       "   43,\n",
       "   271,\n",
       "   22,\n",
       "   16,\n",
       "   153,\n",
       "   441,\n",
       "   75,\n",
       "   14,\n",
       "   44,\n",
       "   33,\n",
       "   27154,\n",
       "   53,\n",
       "   1467,\n",
       "   7,\n",
       "   4508,\n",
       "   4,\n",
       "   21,\n",
       "   43,\n",
       "   35,\n",
       "   73569,\n",
       "   6,\n",
       "   18986,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0,\n",
       "   0]],\n",
       " [1770])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [(train_data[0], train_labels[0])]\n",
    "pad_sequence_documents(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9190c98d-5dd9-4ae3-a69f-5c563973a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClassifier(nn.Module):\n",
    "    def __init__(self, word_embeddings,  # Pre-trained word embeddings\n",
    "                    word_to_id,             # Mapping from words to ids\n",
    "                    num_classes,            # Number of decades to classify\n",
    "                    word_hidden_size=128,   # Hidden size of the RNN (paper uses 128)\n",
    "                    padding_word=PADDING_WORD,\n",
    "                    unknown_word=UNKNOWN,\n",
    "                    word_bidirectional=True,\n",
    "                    device='cpu'\n",
    "                ):\n",
    "        super(DocumentClassifier, self).__init__()\n",
    "        self.padding_word = padding_word\n",
    "        self.unknown_word = unknown_word\n",
    "        self.word_to_id = word_to_id\n",
    "        self.word_hidden_size = word_hidden_size\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.word_bidirectional = word_bidirectional\n",
    "\n",
    "        # Create an embedding tensor for the words and import the Glove\n",
    "        # embeddings. The embeddings are frozen (i.e., they will not be\n",
    "        # updated during training).\n",
    "        vocabulary_size = len(word_embeddings)\n",
    "        self.word_emb_size = len(word_embeddings[0])\n",
    "\n",
    "        self.word_emb = nn.Embedding(vocabulary_size, self.word_emb_size)\n",
    "        self.word_emb.weight = nn.Parameter(\n",
    "            torch.tensor(word_embeddings, dtype=torch.float), requires_grad=False\n",
    "        )\n",
    "\n",
    "        self.word_birnn = nn.GRU(\n",
    "            self.word_emb_size,\n",
    "            self.word_hidden_size,\n",
    "            bidirectional=word_bidirectional, \n",
    "            batch_first = True\n",
    "        )\n",
    "\n",
    "        # Document Classification\n",
    "        multiplier = 2 if self.word_bidirectional else 1\n",
    "        self.final_pred = nn.Linear(multiplier * self.word_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape: (batch_size, seq_length)\n",
    "        batch_size, seq_length = x.shape\n",
    "        word_embeddings = self.word_emb(x)\n",
    "        word_output, word_hidden = self.word_birnn(word_embeddings)\n",
    "        if self.word_bidirectional:\n",
    "            forward_final = word_hidden[0]\n",
    "            backward_final = word_hidden[1]\n",
    "            complete_doc = torch.cat([forward_final, backward_final], dim=1)\n",
    "        else:\n",
    "            complete_doc = word_hidden.squeeze(0)\n",
    "\n",
    "\n",
    "        logits = self.final_pred(complete_doc)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "da5d1722-d3d7-47bd-8abd-0ced359e6b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CUDA\n",
      "Using first 10000 documents for testing...\n",
      "Training on 10000 documents\n",
      "Testing on 50 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 625/625 [00:13<00:00, 44.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.0362, Train Acc: 99.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 625/625 [00:13<00:00, 46.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.0000, Train Acc: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 625/625 [00:13<00:00, 46.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.0000, Train Acc: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 625/625 [00:13<00:00, 45.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.0000, Train Acc: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 625/625 [00:13<00:00, 45.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.0000, Train Acc: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # ================== Hyper-parameters ==================== #\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 5  # Paper uses 5 epochs for RNN\n",
    "# ======================= Training (First 50 documents) ======================= #\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"Running on MGPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(\"Running on CUDA\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "dim, word_to_id, embeddings = load_glove_embeddings(glove_6b_50_path)\n",
    "tokenizer.word2id = word_to_id\n",
    "tokenizer.id2word = {v: k for k, v in word_to_id.items()}\n",
    "\n",
    "# Use only first 50 documents for testing\n",
    "print(\"Using first 10000 documents for testing...\")\n",
    "train_data_small = train_data[:10000]\n",
    "train_labels_small = train_labels[:10000]\n",
    "test_data_small = test_data[:50]  # First 50 for testing\n",
    "test_labels_small = test_labels[:50]\n",
    "\n",
    "training_set = HistoricalTextDataset(train_data_small, train_labels_small, word_to_id, decade_to_label)\n",
    "test_set = HistoricalTextDataset(test_data_small, test_labels_small, word_to_id, decade_to_label)\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=16, collate_fn=pad_sequence_documents)\n",
    "test_loader = DataLoader(test_set, batch_size=16, collate_fn=pad_sequence_documents)\n",
    "\n",
    "print(f\"Training on {len(training_set)} documents\")\n",
    "print(f\"Testing on {len(test_set)} documents\")\n",
    "\n",
    "birnn_classifier = DocumentClassifier(\n",
    "    word_embeddings=embeddings,\n",
    "    word_to_id=word_to_id,\n",
    "    num_classes=len(decade_to_label),\n",
    "    word_hidden_size=128, \n",
    "    word_bidirectional=True,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(birnn_classifier.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "birnn_classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    \n",
    "    for x, y in tqdm(training_loader, desc=\"Epoch {}\".format(epoch + 1)):\n",
    "        x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = birnn_classifier(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(birnn_classifier.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track training accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_acc = 100 * correct / total\n",
    "    avg_loss = epoch_loss / len(training_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e0305e93-d931-40d3-965f-dd41ac73817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "Predicted -> ['D1770', 'D1780', 'D1790', 'D1800', 'D1810', 'D1820', 'D1830', 'D1840', 'D1850', 'D1860', 'D1870', 'D1880', 'D1890']\n",
      "Actual D1770: [2100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual D1780: [810, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual D1790: [1128, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual D1800: [922, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual D1810: [1469, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual D1820: [2462, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual D1830: [1884, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual D1840: [3776, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual D1850: [2184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual D1860: [2816, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual D1870: [3480, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual D1880: [1819, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Actual D1890: [688, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Accuracy: 0.1645\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "birnn_classifier.eval()\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_set = HistoricalTextDataset(test_data, test_labels, word_to_id, decade_to_label)\n",
    "test_loader = DataLoader(test_set, batch_size=16, collate_fn=pad_sequence_documents)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        # Convert to tensors\n",
    "        x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        pred = torch.argmax(birnn_classifier(x), dim=-1).detach().cpu().numpy()\n",
    "        y_np = y.detach().cpu().numpy()\n",
    "        \n",
    "        all_predictions.extend(pred)\n",
    "        all_labels.extend(y_np)\n",
    "\n",
    "# confusion matrix \n",
    "num_classes = len(decade_to_label)\n",
    "confusion_matrix_manual = [[0 for _ in range(num_classes)] for _ in range(num_classes)]\n",
    "\n",
    "for i in range(len(all_predictions)):\n",
    "    actual = all_labels[i]\n",
    "    predicted = all_predictions[i]\n",
    "    confusion_matrix_manual[actual][predicted] += 1\n",
    "\n",
    "# Print results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"Predicted ->\", [f\"D{label_to_decade[i]}\" for i in range(num_classes)])\n",
    "for i, row in enumerate(confusion_matrix_manual):\n",
    "    print(f\"Actual D{label_to_decade[i]}: {row}\")\n",
    "\n",
    "accuracy = (confusion_matrix_manual[0][0] + sum(confusion_matrix_manual[i][i] for i in range(num_classes))) / sum(sum(row) for row in confusion_matrix_manual)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e8d8acbf-f4dc-49f6-a686-60b03868a774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CUDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1326/1326 [01:41<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7107, Train Acc: 90.02%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1326/1326 [01:38<00:00, 13.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.5939, Train Acc: 90.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1326/1326 [01:40<00:00, 13.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.7996, Train Acc: 83.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1326/1326 [01:40<00:00, 13.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.7103, Train Acc: 83.70%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1326/1326 [01:40<00:00, 13.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 1.0095, Train Acc: 71.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================== Hyper-parameters ==================== #\n",
    "learning_rate = 0.001  \n",
    "epochs = 5\n",
    "\n",
    "# ======================= Training ======================= #\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Running on MGPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Running on CUDA\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "dim, word_to_id, embeddings = load_glove_embeddings(glove_6b_300_path)\n",
    "tokenizer.word2id = word_to_id\n",
    "tokenizer.id2word = {v: k for k, v in word_to_id.items()}\n",
    "\n",
    "training_set = HistoricalTextDataset(\n",
    "    train_data, train_labels, word_to_id, decade_to_label\n",
    ")\n",
    "training_loader = DataLoader(\n",
    "    training_set, batch_size=64, collate_fn=pad_sequence_documents\n",
    ")\n",
    "\n",
    "birnn_classifier = DocumentClassifier(\n",
    "    word_embeddings=embeddings,\n",
    "    word_to_id=word_to_id,\n",
    "    num_classes=len(decade_to_label),\n",
    "    word_hidden_size=128, \n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(birnn_classifier.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "birnn_classifier.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    \n",
    "    for x, y in tqdm(training_loader, desc=\"Epoch {}\".format(epoch + 1)):\n",
    "        x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = birnn_classifier(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(birnn_classifier.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track training accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    train_acc = 100 * correct / total\n",
    "    avg_loss = epoch_loss / len(training_loader)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ef5ba2a-7f9d-4d60-b3d7-87d54b431ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "Predicted -> ['D1770', 'D1780', 'D1790', 'D1800', 'D1810', 'D1820', 'D1830', 'D1840', 'D1850', 'D1860', 'D1870', 'D1880', 'D1890']\n",
      "Actual D1770: [0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 2095]\n",
      "Actual D1780: [1, 1, 0, 1, 0, 0, 1, 0, 2, 0, 0, 2, 802]\n",
      "Actual D1790: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1126]\n",
      "Actual D1800: [1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 2, 4, 914]\n",
      "Actual D1810: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 6, 1460]\n",
      "Actual D1820: [6, 1, 2, 0, 0, 0, 3, 0, 11, 12, 15, 15, 2397]\n",
      "Actual D1830: [2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 4, 1875]\n",
      "Actual D1840: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 4, 5, 3766]\n",
      "Actual D1850: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 3, 2179]\n",
      "Actual D1860: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 3, 2810]\n",
      "Actual D1870: [2, 0, 0, 0, 0, 0, 0, 0, 0, 1, 5, 8, 3464]\n",
      "Actual D1880: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 1, 1812]\n",
      "Actual D1890: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 686]\n",
      "Accuracy: 0.0271\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "birnn_classifier.eval()\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_set = HistoricalTextDataset(test_data, test_labels, word_to_id, decade_to_label)\n",
    "test_loader = DataLoader(test_set, batch_size=64, collate_fn=pad_sequence_documents)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        # Convert to tensors\n",
    "        x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        pred = torch.argmax(birnn_classifier(x), dim=-1).detach().cpu().numpy()\n",
    "        y_np = y.detach().cpu().numpy()\n",
    "        \n",
    "        all_predictions.extend(pred)\n",
    "        all_labels.extend(y_np)\n",
    "\n",
    "# confusion matrix \n",
    "num_classes = len(decade_to_label)\n",
    "confusion_matrix_manual = [[0 for _ in range(num_classes)] for _ in range(num_classes)]\n",
    "\n",
    "for i in range(len(all_predictions)):\n",
    "    actual = all_labels[i]\n",
    "    predicted = all_predictions[i]\n",
    "    confusion_matrix_manual[actual][predicted] += 1\n",
    "\n",
    "# Print results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"Predicted ->\", [f\"D{label_to_decade[i]}\" for i in range(num_classes)])\n",
    "for i, row in enumerate(confusion_matrix_manual):\n",
    "    print(f\"Actual D{label_to_decade[i]}: {row}\")\n",
    "\n",
    "accuracy = (confusion_matrix_manual[0][0] + sum(confusion_matrix_manual[i][i] for i in range(num_classes))) / sum(sum(row) for row in confusion_matrix_manual)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d7ceb-797f-44e7-b756-6322bab11594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e81c1a-4077-4807-a645-a078ad1ab36c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
