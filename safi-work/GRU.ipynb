{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "71faf995",
      "metadata": {
        "id": "71faf995"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "27ef5443",
      "metadata": {
        "id": "27ef5443"
      },
      "outputs": [],
      "source": [
        "# First run this cell\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import codecs\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils import clip_grad_norm_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt_tab')\n",
        "except LookupError:\n",
        "    print(\"Downloading punkt_tab...\")\n",
        "    nltk.download('punkt_tab')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    print(\"Downloading punkt...\")\n",
        "    nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xttqWDEpwjqr",
        "outputId": "191a23ce-b4c4-4e89-baa9-7bd5c44b99d1"
      },
      "id": "xttqWDEpwjqr",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading punkt_tab...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading punkt...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "def advanced_tokenize(text):\n",
        "    import traceback\n",
        "    try:\n",
        "        sentences = sent_tokenize(text)\n",
        "    except Exception as e:\n",
        "        print(\"Error during sent_tokenize:\")\n",
        "        traceback.print_exc()\n",
        "        raise e  # Re-raise to see full trace\n",
        "\n",
        "    all_tokens = []\n",
        "    for sentence in sentences:\n",
        "        tokens = word_tokenize(sentence.lower())\n",
        "        filtered_tokens = [\n",
        "            token for token in tokens\n",
        "            if token.isalnum() or token in [\"'s\", \"'t\", \"'re\", \"'ve\", \"'ll\", \"'d\", \"n't\", '!', '?', '.', ',', ';', ':', '--', '—']\n",
        "        ]\n",
        "        all_tokens.extend(filtered_tokens)\n",
        "    return all_tokens\n"
      ],
      "metadata": {
        "id": "4JHAJIJFwoXU"
      },
      "id": "4JHAJIJFwoXU",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "52acb596",
      "metadata": {
        "id": "52acb596"
      },
      "outputs": [],
      "source": [
        "# Labels for each text\n",
        "start_year = 1700\n",
        "nb_decades = 20\n",
        "def int_to_decades(t):\n",
        "    return start_year + 10*t"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "glove_path = '/content/drive/MyDrive/Assignment 4/Project/glove.6B.50d.txt'\n",
        "classifier_train_path = '/content/drive/MyDrive/Assignment 4/Project/dd2417-dating-historical-texts/snippets_train_dataset.csv'\n",
        "classifier_test_path = '/content/drive/MyDrive/Assignment 4/Project/dd2417-dating-historical-texts/snippets_test_dataset.csv'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQgHi0XT_yrv",
        "outputId": "3876b7b0-b037-4d9f-ade4-49d57e540936"
      },
      "id": "pQgHi0XT_yrv",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "4e410735",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e410735",
        "outputId": "b9670aaa-bef3-4ff2-a719-8b5607844ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<unk>', '’', '—', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
            "['1700', '1710', '1720', '1730', '1740', '1750', '1760', '1770', '1780', '1790', '1800', '1810', '1820', '1830', '1840', '1850', '1860', '1870', '1880', '1890']\n"
          ]
        }
      ],
      "source": [
        "# Run this cell to init mappings from characters to IDs and back again,\n",
        "# from words to IDs and back again, and from labels to IDs and back again\n",
        "\n",
        "UNKNOWN = '<unk>'  # Unknown char or unknown word\n",
        "CHARS = [UNKNOWN, '’', '—'] + list(string.punctuation) + list(string.ascii_letters) + list(string.digits)\n",
        "char_to_id = {c:i for i,c in enumerate(CHARS)}\n",
        "PADDING_WORD = '<pad>'\n",
        "id_to_label = ['{}'.format(int_to_decades(d)) for d in range(nb_decades)]\n",
        "\n",
        "def label_to_id(label):\n",
        "    return (int(label)-start_year)//10\n",
        "\n",
        "print(CHARS)\n",
        "print(id_to_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "be8e1fbc",
      "metadata": {
        "id": "be8e1fbc"
      },
      "outputs": [],
      "source": [
        "def load_glove_embeddings(embedding_file,\n",
        "                          padding_word=PADDING_WORD,\n",
        "                          unknown_word=UNKNOWN):\n",
        "    \"\"\"\n",
        "    Reads Glove embeddings from a file.\n",
        "\n",
        "    Returns vector dimensionality, the word_to_id mapping (as a dict),\n",
        "    and the embeddings (as a list of lists).\n",
        "    \"\"\"\n",
        "    word_to_id = {}  # Dictionary to store word-to-ID mapping\n",
        "    word_to_id[padding_word] = 0\n",
        "    word_to_id[unknown_word] = 1\n",
        "    embeddings = []\n",
        "    with open(embedding_file, encoding='utf8') as f:\n",
        "        for line in f:\n",
        "            data = line.split()\n",
        "            word = data[0]\n",
        "            vec = [float(x) for x in data[1:]]\n",
        "            embeddings.append(vec)\n",
        "            word_to_id[word] = len(word_to_id)\n",
        "    D = len(embeddings[0])\n",
        "\n",
        "    embeddings.insert(word_to_id[padding_word], [0]*D)  # <PAD> has an embedding of just zeros\n",
        "    embeddings.insert(word_to_id[unknown_word], [-1]*D)      # <UNK> has an embedding of just minus-ones\n",
        "\n",
        "    return D, word_to_id, embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2ca7d55d",
      "metadata": {
        "id": "2ca7d55d"
      },
      "outputs": [],
      "source": [
        "class HistDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A class loading a dataset from a CSV file to be used as an input\n",
        "    to PyTorch DataLoader.\n",
        "\n",
        "    The CSV file has 2 fields: chunk of text and label.\n",
        "\n",
        "    Datapoints are sentences + associated labels for each word. If the\n",
        "    words have not been seen before (i.e, they are not found in the\n",
        "    'word_to_id' dict), they will be mapped to the unknown word '<UNK>'.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, filename, word_to_id):\n",
        "        reader = csv.reader(codecs.open(filename, encoding='utf8',\n",
        "                                        errors='ignore'), delimiter='\\t')\n",
        "\n",
        "        self.passages = []\n",
        "        self.labels = []\n",
        "\n",
        "        for (i,row) in enumerate(reader):\n",
        "            if row and len(row) >= 2:\n",
        "                text = row[0].strip().lower()\n",
        "                # Tokenize using NLTK\n",
        "                tokens = advanced_tokenize(text)\n",
        "                if len(tokens) > 0:  # Only add non-empty passages\n",
        "                    self.passages.append(tokens)\n",
        "                    self.labels.append(label_to_id(row[1].strip()))\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.passages)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.passages[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e0cd72d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0cd72d6",
        "outputId": "d9cb205c-cb67-4b3e-87a5-3f6fd919a46e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The embedding for the word 'good' looks like this:\n",
            "[-0.35586, 0.5213, -0.6107, -0.30131, 0.94862, -0.31539, -0.59831, 0.12188, -0.031943, 0.55695, -0.10621, 0.63399, -0.4734, -0.075895, 0.38247, 0.081569, 0.82214, 0.2222, -0.0083764, -0.7662, -0.56253, 0.61759, 0.20292, -0.048598, 0.87815, -1.6549, -0.77418, 0.15435, 0.94823, -0.3952, 3.7302, 0.82855, -0.14104, 0.016395, 0.21115, -0.036085, -0.15587, 0.86583, 0.26309, -0.71015, -0.03677, 0.0018282, -0.17704, 0.27032, 0.11026, 0.14133, -0.057322, 0.27207, 0.31305, 0.92771]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Let's check out some of these data structures\n",
        "dim, word_to_id, embeddings = load_glove_embeddings(glove_path)\n",
        "print(\"The embedding for the word 'good' looks like this:\")\n",
        "print(embeddings[word_to_id['good']])\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "54c3226a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54c3226a",
        "outputId": "6b7dd493-c0f4-40e7-896e-69dfe5cf80d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 8200 data points in the test set\n",
            "Data point 0 is ['the', 'professor', 'laid', 'his', 'hand', 'tenderly', 'on', 'his', 'shoulder', 'as', 'he', 'spoke', ':', '--', 'ah', ',', 'my', 'child', ',', 'i', 'will', 'be', 'plain', '.', 'do', 'you', 'not', 'see', 'how', ',', 'of', 'late', ',', 'this', 'monster', 'has', 'been', 'creeping', 'into', 'knowledge', 'experimentally', '.', 'how', 'he', 'has', 'been', 'making', 'use', 'of', 'the', 'zoöphagous', 'patient', 'to', 'effect', 'his', 'entry', 'into', 'friend', 'john', 's', 'home', ';', 'for', 'your', 'vampire', ',', 'though', 'in', 'all', 'afterwards', 'he', 'can', 'come', 'when', 'and', 'how', 'he', 'will', ',', 'must', 'at', 'the', 'first', 'make', 'entry', 'only', 'when', 'asked', 'thereto', 'by', 'an', 'inmate', '.', 'but', 'these', 'are', 'not', 'his', 'most', 'important', 'experiments', '.', 'do', 'we', 'not', 'see', 'how', 'at', 'the', 'first', 'all', 'these', 'so', 'great', 'boxes', 'were']\n",
            "It has the label 19\n",
            "Data point 1 is ['--', 'i', 'suppose', 'it', 'was', 'natural', 'that', 'we', 'should', 'have', 'all', 'overslept', 'ourselves', ',', 'for', 'the', 'day', 'was', 'a', 'busy', 'one', ',', 'and', 'the', 'night', 'had', 'no', 'rest', 'at', 'all', '.', 'even', 'mina', 'must', 'have', 'felt', 'its', 'exhaustion', ',', 'for', 'though', 'i', 'slept', 'till', 'the', 'sun', 'was', 'high', ',', 'i', 'was', 'awake', 'before', 'her', ',', 'and', 'had', 'to', 'call', 'two', 'or', 'three', 'times', 'before', 'she', 'awoke', '.', 'indeed', ',', 'she', 'was', 'so', 'sound', 'asleep', 'that', 'for', 'a', 'few', 'seconds', 'she', 'did', 'not', 'recognize', 'me', ',', 'but', 'looked', 'at', 'me', 'with', 'a', 'sort', 'of', 'blank', 'terror', ',', 'as', 'one', 'looks', 'who', 'has', 'been', 'waked', 'out', 'of', 'a', 'bad', 'dream', '.', 'she', 'complained', 'a', 'little', 'of', 'being', 'tired', ',', 'and', 'i', 'let', 'her', 'rest', 'till']\n",
            "It has the label 19\n"
          ]
        }
      ],
      "source": [
        "# Read the data we are going to use for testing the model\n",
        "test_set = HistDataset(classifier_train_path, word_to_id)\n",
        "print(\"There are\", len(test_set), \"data points in the test set\")\n",
        "\n",
        "for dp in range(2):\n",
        "    sentence, label = test_set[dp]\n",
        "    print(\"Data point\", dp, \"is\", sentence)\n",
        "    print(\"It has the label\", label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b921d227",
      "metadata": {
        "id": "b921d227"
      },
      "outputs": [],
      "source": [
        "# Run this cell. The function below will take care of the case of\n",
        "# sequences of unequal lengths.\n",
        "\n",
        "def pad_sequence(batch, padding_word=PADDING_WORD):\n",
        "    batch_data, batch_labels = zip(*batch)\n",
        "    max_len = max(map(len, batch_data))\n",
        "    padded_data = [[b[i] if i < len(b) else padding_word for i in range(max_len)] for b in batch_data]\n",
        "    return padded_data, batch_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2e1989db",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2e1989db",
        "outputId": "93dda914-69c0-4805-b15e-559ca8b082fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([[1, 2, 3, '<pad>'], [4, 5, '<pad>', '<pad>'], [6, 7, 8, 9]],\n",
              " (1750, 1890, 1900))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# This is how it works\n",
        "x = [([1,2,3],1750), ([4,5],1890), ([6,7,8,9],1900)]\n",
        "pad_sequence(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "8c97552e",
      "metadata": {
        "id": "8c97552e"
      },
      "outputs": [],
      "source": [
        "class HistClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, word_embeddings,  # Pre-trained word embeddings\n",
        "                 char_to_id,             # Mapping from chars to ids\n",
        "                 word_to_id,             # Mapping from words to ids\n",
        "                 char_emb_size=16,\n",
        "                 char_hidden_size=25,    # Hidden size of the character-level biRNN\n",
        "                 word_hidden_size=100,   # Hidden size of the word-level biRNN\n",
        "                 class_size=nb_decades,\n",
        "                 padding_word=PADDING_WORD,\n",
        "                 unknown_word=UNKNOWN,\n",
        "                 char_bidirectional=True,\n",
        "                 word_bidirectional=True,\n",
        "                 device='cpu'\n",
        "            ):\n",
        "\n",
        "        super(HistClassifier, self).__init__()\n",
        "        self.padding_word = padding_word\n",
        "        self.unknown_word = unknown_word\n",
        "        self.char_to_id = char_to_id\n",
        "        self.word_to_id = word_to_id\n",
        "        self.char_emb_size = char_emb_size\n",
        "        self.char_hidden_size = char_hidden_size\n",
        "        self.word_hidden_size = word_hidden_size\n",
        "        self.class_size = class_size\n",
        "        self.char_bidirectional = char_bidirectional\n",
        "        self.word_bidirectional = word_bidirectional\n",
        "        self.device = device\n",
        "\n",
        "        # Create an embedding tensor for the words and import the Glove\n",
        "        # embeddings. The embeddings are frozen (i.e., they will not be\n",
        "        # updated during training).\n",
        "        vocabulary_size = len(word_embeddings)\n",
        "        self.word_emb_size = len(word_embeddings[0])\n",
        "\n",
        "        self.word_emb = nn.Embedding(vocabulary_size, self.word_emb_size)\n",
        "        self.word_emb.weight = nn.Parameter(torch.tensor(word_embeddings, dtype=torch.float),\n",
        "                                            requires_grad=False)\n",
        "\n",
        "        # Create an embedding tensor for character embeddings. These embeddings\n",
        "        # are learnt from scratch (i.e., they are not frozen).\n",
        "        if self.char_emb_size > 0:\n",
        "            self.char_emb = nn.Embedding(len(char_to_id), char_emb_size)\n",
        "            self.char_birnn = nn.GRU(\n",
        "                self.char_emb_size,\n",
        "                self.char_hidden_size,\n",
        "                bidirectional=char_bidirectional,\n",
        "                batch_first=True\n",
        "            )\n",
        "        else:\n",
        "            self.char_hidden_size = 0\n",
        "\n",
        "        multiplier = 2 if self.char_bidirectional else 1\n",
        "        self.word_birnn = nn.GRU(\n",
        "            self.word_emb_size + multiplier * self.char_hidden_size, # input size\n",
        "            self.word_hidden_size,\n",
        "            bidirectional=word_bidirectional,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Binary classification - 0 if not part of the name, 1 if a name\n",
        "        multiplier = 2 if self.word_bidirectional else 1\n",
        "        self.final_pred = nn.Linear(multiplier * self.word_hidden_size, self.class_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Performs a forward pass of a NER classifier\n",
        "        Takes as input a 2D list `x` of dimensionality (B, T),\n",
        "        where B is the batch size;\n",
        "              T is the max sentence length in the batch (shorter sentences\n",
        "              are already padded with the special token <PAD>)\n",
        "\n",
        "        Returns logits, i.e. the output of the last linear layer before applying softmax.\n",
        "\n",
        "        :param      x:    A batch of sentences\n",
        "        :type       x:    list of strings\n",
        "        \"\"\"\n",
        "\n",
        "        # First find all word IDs of all words in all sentences in the batch\n",
        "        # and the character IDs of all characters in all words in all sentences\n",
        "        word_ids = [[word_to_id[word] if word in word_to_id.keys() else 1 for word in sentence] for sentence in x]\n",
        "        char_ids = [[[char_to_id[char] if char in char_to_id.keys() else 0 for char in word] for word in sentence] for sentence in x]\n",
        "        max_word_len = max(len(word) for sentence in char_ids for word in sentence)\n",
        "        char_ids = [[[word[i] if i < len(word) else 0 for i in range(max_word_len)] for word in sentence] for sentence in char_ids]\n",
        "\n",
        "        # The 'to(self.device)' below is necessary for making sure that\n",
        "        # the model and the data are on the same device (CPU or CUDA).\n",
        "        word_tensor = torch.tensor(word_ids).to(self.device)\n",
        "        char_tensor = torch.tensor(char_ids).to(self.device)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        #Dataset parameters\n",
        "        batch_size = len(x)\n",
        "        len_sentence = len(x[0])\n",
        "\n",
        "        #Embedding layer\n",
        "        E_w = self.word_emb(word_tensor)\n",
        "        E_c = self.char_emb(char_tensor)\n",
        "        E_c = E_c.reshape(batch_size*len_sentence,max_word_len,self.char_emb_size)\n",
        "\n",
        "        #RNN\n",
        "        _,hidden_char = self.char_birnn(E_c)\n",
        "        H_c = torch.cat([hidden_char[0],hidden_char[1]],dim=1)\n",
        "        H_c = H_c.reshape(batch_size,len_sentence,2*self.char_hidden_size)\n",
        "\n",
        "        E_final = torch.cat([E_w,H_c],dim=2)\n",
        "        outputs,last_hidden = self.word_birnn(E_final)\n",
        "\n",
        "        output = torch.cat([last_hidden[0],last_hidden[1]],dim=1)\n",
        "        final_pred = self.final_pred(output)\n",
        "\n",
        "        return final_pred\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b3b26006",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3b26006",
        "outputId": "ce33b893-2028-4c99-f958-7d2a5b9e0117"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda\n"
          ]
        }
      ],
      "source": [
        "# ================== Hyper-parameters ==================== #\n",
        "\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "# ======================= Training ======================= #\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print( \"Running on\", device )\n",
        "\n",
        "#dim, word_to_id, embeddings = load_glove_embeddings('../glove.6B.50d.txt')\n",
        "training_set = HistDataset(classifier_train_path, word_to_id)\n",
        "training_loader = DataLoader(training_set, batch_size=64, shuffle=True, collate_fn=pad_sequence)\n",
        "\n",
        "classifier = HistClassifier(embeddings, char_to_id, word_to_id, device=device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "fd80acdf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fd80acdf",
        "outputId": "a727acbc-0ab2-49c2-c2e4-144ee1ae7575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 129/129 [00:13<00:00,  9.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 382.4138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 129/129 [00:11<00:00, 11.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 365.2642\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 129/129 [00:12<00:00, 10.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 337.8273\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 129/129 [00:12<00:00, 10.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 314.4121\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 129/129 [00:12<00:00, 10.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 296.7398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 129/129 [00:12<00:00, 10.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 276.4802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 129/129 [00:12<00:00, 10.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 259.6438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 129/129 [00:12<00:00, 10.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 241.8734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 129/129 [00:12<00:00, 10.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 225.8657\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 129/129 [00:12<00:00, 10.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 210.0486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "optimizer = optim.Adam(classifier.parameters(), lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "classifier.train()\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for x, y in tqdm(training_loader, desc=\"Epoch {}\".format(epoch + 1)):\n",
        "        optimizer.zero_grad()\n",
        "        logits = classifier(x)\n",
        "        loss = criterion(logits, torch.tensor(y).to(device).reshape(-1,))\n",
        "        loss.backward()\n",
        "        total_loss += loss.item()\n",
        "        clip_grad_norm_(classifier.parameters(), 5)\n",
        "        optimizer.step()\n",
        "    print(f\"Loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "save_path = \"/content/drive/MyDrive/Assignment 4/Project/dd2417-dating-historical-texts/hist_classifier-100-epochs.pt\"\n",
        "torch.save(classifier.state_dict(), save_path)"
      ],
      "metadata": {
        "id": "NJWJi9Yckqfs"
      },
      "id": "NJWJi9Yckqfs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-create the model\n",
        "classifier2 = HistClassifier(embeddings, char_to_id, word_to_id, device=device).to(device)\n",
        "load_path = \"/content/drive/MyDrive/Assignment 4/Project/dd2417-dating-historical-texts/hist_classifier-100-epochs.pt\"\n",
        "classifier2.load_state_dict(torch.load(load_path, map_location=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGaNSFu1mVQA",
        "outputId": "b5a01281-819b-4b4e-a53d-98badf2dff46"
      },
      "id": "kGaNSFu1mVQA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4d139e71",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d139e71",
        "outputId": "fcecede4-6eab-4538-dfe3-41bd613cb525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.11/dist-packages (3.1.10)\n"
          ]
        }
      ],
      "source": [
        "# Evaluation\n",
        "import numpy as np\n",
        "!pip install terminaltables\n",
        "from terminaltables import AsciiTable\n",
        "\n",
        "classifier.eval()\n",
        "\n",
        "confusion_matrix = np.zeros((nb_decades,nb_decades))\n",
        "test_set = HistDataset(classifier_test_path, word_to_id) ### TO CHANGE WITH TRAINING FILE\n",
        "for x, y in test_set:\n",
        "    pred = torch.argmax(classifier([x]), dim=-1).item()\n",
        "    confusion_matrix[y,pred]+=1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create header row: predicted class names\n",
        "header = [''] + [f'Predicted {i}' for i in range(nb_decades)]\n",
        "\n",
        "# Create each row: true class + predicted counts\n",
        "table = [header]\n",
        "for i in range(nb_decades):\n",
        "    row = [f'Real {i}'] + list(confusion_matrix[i,:])\n",
        "    table.append(row)\n",
        "\n",
        "t = AsciiTable(table)\n",
        "print(t.table)\n",
        "correct = np.trace(confusion_matrix)               # sum of diagonal\n",
        "total = np.sum(confusion_matrix)                  # sum of all values\n",
        "accuracy = round(correct / total, 4)\n",
        "\n",
        "print(\"Accuracy: {}\".format(accuracy))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiugmPwwCBYl",
        "outputId": "7fd0f043-6cb2-4392-e47c-4cc028f70678"
      },
      "id": "WiugmPwwCBYl",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+\n",
            "|         | Predicted 0 | Predicted 1 | Predicted 2 | Predicted 3 | Predicted 4 | Predicted 5 | Predicted 6 | Predicted 7 | Predicted 8 | Predicted 9 | Predicted 10 | Predicted 11 | Predicted 12 | Predicted 13 | Predicted 14 | Predicted 15 | Predicted 16 | Predicted 17 | Predicted 18 | Predicted 19 |\n",
            "+---------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+\n",
            "| Real 0  | 2.0         | 34.0        | 4.0         | 5.0         | 0.0         | 0.0         | 0.0         | 0.0         | 0.0         | 0.0         | 41.0         | 0.0          | 0.0          | 10.0         | 0.0          | 1.0          | 0.0          | 0.0          | 0.0          | 3.0          |\n",
            "| Real 1  | 12.0        | 32.0        | 1.0         | 9.0         | 1.0         | 2.0         | 0.0         | 13.0        | 6.0         | 0.0         | 1.0          | 0.0          | 0.0          | 15.0         | 1.0          | 2.0          | 0.0          | 4.0          | 1.0          | 0.0          |\n",
            "| Real 2  | 14.0        | 9.0         | 2.0         | 1.0         | 12.0        | 4.0         | 0.0         | 29.0        | 0.0         | 0.0         | 0.0          | 0.0          | 0.0          | 17.0         | 3.0          | 5.0          | 0.0          | 0.0          | 3.0          | 1.0          |\n",
            "| Real 3  | 11.0        | 40.0        | 4.0         | 15.0        | 6.0         | 3.0         | 1.0         | 6.0         | 6.0         | 0.0         | 1.0          | 2.0          | 0.0          | 4.0          | 0.0          | 0.0          | 0.0          | 1.0          | 0.0          | 0.0          |\n",
            "| Real 4  | 0.0         | 0.0         | 2.0         | 0.0         | 18.0        | 48.0        | 3.0         | 6.0         | 1.0         | 2.0         | 4.0          | 3.0          | 3.0          | 5.0          | 1.0          | 0.0          | 0.0          | 4.0          | 0.0          | 0.0          |\n",
            "| Real 5  | 13.0        | 8.0         | 9.0         | 24.0        | 3.0         | 2.0         | 1.0         | 0.0         | 9.0         | 2.0         | 12.0         | 1.0          | 0.0          | 15.0         | 1.0          | 0.0          | 0.0          | 0.0          | 0.0          | 0.0          |\n",
            "| Real 6  | 0.0         | 1.0         | 0.0         | 0.0         | 5.0         | 24.0        | 1.0         | 0.0         | 2.0         | 19.0        | 15.0         | 2.0          | 19.0         | 4.0          | 2.0          | 1.0          | 2.0          | 2.0          | 0.0          | 1.0          |\n",
            "| Real 7  | 0.0         | 0.0         | 0.0         | 1.0         | 37.0        | 38.0        | 1.0         | 5.0         | 4.0         | 0.0         | 5.0          | 1.0          | 0.0          | 0.0          | 8.0          | 0.0          | 0.0          | 0.0          | 0.0          | 0.0          |\n",
            "| Real 8  | 0.0         | 0.0         | 1.0         | 0.0         | 0.0         | 0.0         | 0.0         | 2.0         | 1.0         | 0.0         | 0.0          | 0.0          | 0.0          | 5.0          | 6.0          | 58.0         | 19.0         | 0.0          | 7.0          | 1.0          |\n",
            "| Real 9  | 0.0         | 0.0         | 1.0         | 1.0         | 5.0         | 12.0        | 0.0         | 2.0         | 0.0         | 6.0         | 15.0         | 3.0          | 2.0          | 5.0          | 20.0         | 3.0          | 3.0          | 22.0         | 0.0          | 0.0          |\n",
            "| Real 10 | 1.0         | 0.0         | 4.0         | 3.0         | 14.0        | 10.0        | 4.0         | 7.0         | 4.0         | 3.0         | 5.0          | 3.0          | 2.0          | 13.0         | 6.0          | 2.0          | 6.0          | 6.0          | 3.0          | 4.0          |\n",
            "| Real 11 | 0.0         | 0.0         | 7.0         | 1.0         | 2.0         | 22.0        | 5.0         | 1.0         | 18.0        | 1.0         | 1.0          | 0.0          | 26.0         | 13.0         | 0.0          | 0.0          | 2.0          | 1.0          | 0.0          | 0.0          |\n",
            "| Real 12 | 2.0         | 20.0        | 0.0         | 2.0         | 3.0         | 3.0         | 0.0         | 31.0        | 1.0         | 0.0         | 1.0          | 0.0          | 0.0          | 21.0         | 0.0          | 8.0          | 3.0          | 2.0          | 1.0          | 2.0          |\n",
            "| Real 13 | 3.0         | 4.0         | 4.0         | 15.0        | 0.0         | 1.0         | 0.0         | 12.0        | 10.0        | 1.0         | 3.0          | 1.0          | 0.0          | 26.0         | 0.0          | 20.0         | 0.0          | 0.0          | 0.0          | 0.0          |\n",
            "| Real 14 | 1.0         | 0.0         | 0.0         | 0.0         | 4.0         | 14.0        | 0.0         | 24.0        | 4.0         | 0.0         | 2.0          | 7.0          | 3.0          | 9.0          | 4.0          | 2.0          | 7.0          | 16.0         | 0.0          | 3.0          |\n",
            "| Real 15 | 0.0         | 0.0         | 1.0         | 0.0         | 2.0         | 10.0        | 0.0         | 2.0         | 1.0         | 4.0         | 3.0          | 2.0          | 0.0          | 17.0         | 15.0         | 7.0          | 15.0         | 14.0         | 3.0          | 4.0          |\n",
            "| Real 16 | 1.0         | 0.0         | 0.0         | 0.0         | 1.0         | 2.0         | 0.0         | 4.0         | 0.0         | 0.0         | 2.0          | 1.0          | 0.0          | 17.0         | 5.0          | 9.0          | 23.0         | 15.0         | 4.0          | 16.0         |\n",
            "| Real 17 | 0.0         | 0.0         | 0.0         | 0.0         | 5.0         | 6.0         | 0.0         | 7.0         | 0.0         | 0.0         | 1.0          | 1.0          | 0.0          | 10.0         | 2.0          | 0.0          | 8.0          | 38.0         | 2.0          | 20.0         |\n",
            "| Real 18 | 0.0         | 0.0         | 6.0         | 0.0         | 1.0         | 1.0         | 1.0         | 5.0         | 0.0         | 0.0         | 0.0          | 0.0          | 3.0          | 15.0         | 3.0          | 9.0          | 10.0         | 1.0          | 26.0         | 19.0         |\n",
            "| Real 19 | 0.0         | 1.0         | 0.0         | 0.0         | 0.0         | 0.0         | 0.0         | 0.0         | 0.0         | 1.0         | 4.0          | 1.0          | 3.0          | 5.0          | 3.0          | 8.0          | 3.0          | 9.0          | 5.0          | 57.0         |\n",
            "+---------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+--------------+\n",
            "Accuracy: 0.135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kq9i-jBxDAwE",
        "outputId": "ac99c765-50ff-4725-9bb5-038f50d4d1b8"
      },
      "id": "kq9i-jBxDAwE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "300.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}