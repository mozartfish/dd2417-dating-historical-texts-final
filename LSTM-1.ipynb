{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38cde6a-5334-400f-b277-27ffefff7cc5",
   "metadata": {},
   "source": [
    "# DD2417 Final Project - Dating Historical Texts - LSTM 1-Layer RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b8eb6-40a5-4b13-be6e-1a4030a24ca0",
   "metadata": {},
   "source": [
    "## Libraries + Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9734d7f-45ae-4d7a-aed6-4b052c750249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29189904",
   "metadata": {},
   "source": [
    "## Embeddings - Download + Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ae1d9f-73ba-48eb-8666-4197fc8f60ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"./Embeddings\"\n",
    "\n",
    "\n",
    "def download_progress(block_num, block_size, total_size):\n",
    "    if not hasattr(download_progress, \"pbar\"):\n",
    "        download_progress.pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True)\n",
    "    download_progress.pbar.update(block_size)\n",
    "\n",
    "\n",
    "if not os.path.exists(embeddings_path):\n",
    "    print(f\"create directory to store pre-trained glove embeddings\")\n",
    "    os.makedirs(embeddings_path)\n",
    "    print(f\"download pre-trained Glove Embeddings\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "        \"./Embeddings/glove.6B.zip\",\n",
    "        download_progress,\n",
    "    )\n",
    "    print(\"unpack embeddings\")\n",
    "    with zipfile.ZipFile(\"./Embeddings/glove.6B.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"./Embeddings/\")\n",
    "    os.remove(\"./Embeddings/glove.6B.zip\")\n",
    "\n",
    "    print(\"embeddings download complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b703a-92c8-4051-b031-32242ab9bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6b_50_path = \"./Embeddings/glove.6B.50d.txt\"\n",
    "glove_6b_100_path = \"./Embeddings/glove.6B.100d.txt\"\n",
    "glove_6b_200_path = \"./Embeddings/glove.6B.200d.txt\"\n",
    "glove_6b_300_path = \"./Embeddings/glove.6B.300d.txt\"\n",
    "clean_train_split_path = \"./Datasets/clean_train_split/\"\n",
    "clean_test_split_path = \"./Datasets/clean_test_split\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e14822",
   "metadata": {},
   "source": [
    "## Tokenization - NLTk Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72149524-7cac-4343-8bbf-6599e361e1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "from collections import defaultdict\n",
    "\n",
    "class HistoricalTextTokenizer:\n",
    "    \"\"\"\n",
    "    This class defines a tokenizer adapted from the tokenizers developed by Professor Johan Boye for the DD2417 assignments\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2id = defaultdict(lambda: None)\n",
    "        self.id2word = defaultdict(lambda: None)\n",
    "        self.latest_new_word = -1\n",
    "        self.tokens_processed = 0\n",
    "\n",
    "        self.UNKNOWN = \"<unk>\"\n",
    "        self.PADDING_WORD = \"<pad>\"\n",
    "\n",
    "        self.get_word_id(self.PADDING_WORD)\n",
    "        self.get_word_id(self.UNKNOWN)\n",
    "\n",
    "    def get_word_id(self, word):\n",
    "        word = word.lower()\n",
    "        if word in self.word2id:\n",
    "            return self.word2id[word]\n",
    "        else:\n",
    "            self.latest_new_word += 1\n",
    "            self.id2word[self.latest_new_word] = word\n",
    "            self.word2id[word] = self.latest_new_word\n",
    "            return self.latest_new_word\n",
    "\n",
    "    \"\"\"\n",
    "    Process and tokenize all files at the directory level \n",
    "    \"\"\"\n",
    "\n",
    "    def process_files(self, file_or_dir):\n",
    "        all_texts = []\n",
    "        all_labels = []\n",
    "\n",
    "        if os.path.isdir(file_or_dir):\n",
    "            decade_dirs = sorted(\n",
    "                [\n",
    "                    d\n",
    "                    for d in os.listdir(file_or_dir)\n",
    "                    if os.path.isdir(os.path.join(file_or_dir, d))\n",
    "                ]\n",
    "            )\n",
    "            for decade_dir in decade_dirs:\n",
    "                decade_path = os.path.join(file_or_dir, decade_dir)\n",
    "                decade = int(decade_dir)\n",
    "                print(f\"Processing decade: {decade}\")\n",
    "                text_files = sorted(\n",
    "                    [f for f in os.listdir(decade_path) if f.endswith(\".txt\")]\n",
    "                )\n",
    "                print(f\"number of files in {decade} directory: {len(text_files)}\")\n",
    "\n",
    "                for file in text_files:\n",
    "                    filepath = os.path.join(decade_path, file)\n",
    "                    print(f\"tokenize file {file}\")\n",
    "                    text, labels = self.process_file(filepath, decade)\n",
    "                    all_texts.extend(text)\n",
    "                    all_labels.extend(labels)\n",
    "        else:\n",
    "            texts, labels = self.process_file(file_or_dir, 0)\n",
    "            all_texts.extend(texts)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "        return all_texts, all_labels\n",
    "\n",
    "    \"\"\"\n",
    "    Process and tokenize all files within a particular directory\n",
    "    \"\"\"\n",
    "\n",
    "    def process_file(self, filepath, decade):\n",
    "        stream = open(filepath, mode=\"r\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "        text = stream.read()\n",
    "        stream.close()\n",
    "\n",
    "        try:\n",
    "            self.tokens = nltk.word_tokenize(text)\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt\")\n",
    "            self.tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        for i, token in enumerate(self.tokens):\n",
    "            self.tokens_processed += 1\n",
    "            word_id = self.get_word_id(token)\n",
    "\n",
    "            if self.tokens_processed % 1000000000 == 0:\n",
    "                print(\"Processed\", \"{:,}\".format(self.tokens_processed), \"tokens\")\n",
    "\n",
    "        paragraphs = self.create_paragraphs(text)\n",
    "        labels = [decade] * len(paragraphs)\n",
    "\n",
    "        return paragraphs, labels\n",
    "\n",
    "    \"\"\"\n",
    "    This function creates paragraphs of text by adapting the paragraph strategy \n",
    "    in the Deep Learning for Period Classification of Historical Texts paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def create_paragraphs(self, text, min_words=10, max_words=210):\n",
    "        words = text.split()\n",
    "        paragraphs = []\n",
    "        start = 0\n",
    "\n",
    "        while start < len(words):\n",
    "            end = min(start + max_words, len(words))\n",
    "            paragraph_words = words[start:end]\n",
    "            if len(paragraph_words) >= min_words:\n",
    "                paragraph_text = \" \".join(paragraph_words)\n",
    "                paragraphs.append(paragraph_text)\n",
    "            start = end\n",
    "\n",
    "        return paragraphs\n",
    "\n",
    "    \"\"\"\n",
    "    Tokenize paragraphs and assign them an id\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize_text_to_id(self, text):\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt\")\n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "        word_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.word2id:\n",
    "                word_ids.append(self.word2id[token])\n",
    "            else:\n",
    "                word_ids.append(self.word2id[self.UNKNOWN])\n",
    "        return word_ids\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.word2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fab5d48-f7bc-422c-9f85-1f2415325a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = HistoricalTextTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a37f298-d874-43fb-b42e-f7659d012868",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_data, train_labels = text_tokenizer.process_files(clean_train_split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39b739b-96ad-4ba3-950d-994ddc17435f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_data, test_labels = text_tokenizer.process_files(clean_test_split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33e483a-7013-49b3-be27-c16fcc7d1a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = sorted(set(train_labels + test_labels))\n",
    "decade_to_label = {decade: i for i, decade in enumerate(labels)}\n",
    "print(f\"{decade_to_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e279346a-62f2-44b5-bbb8-09cc3fb8d41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"number of train labels -> {len(train_labels)}\")\n",
    "print(f\"length of train text(paragraphs) -> {len(train_text_data)}\")\n",
    "print()\n",
    "\n",
    "print(f\"number of test labels -> {len(test_labels)}\")\n",
    "print(f\"length of test text -> {len(test_text_data)}\")\n",
    "print()\n",
    "\n",
    "print(f\"train text {train_text_data[0]}\")\n",
    "print(f\"train label {train_labels[0]}\")\n",
    "print()\n",
    "\n",
    "print(f\"test text(paragraphs) {test_text_data[0]}\")\n",
    "print(f\"test label {test_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4d1c21-4dad-434e-bb70-9f2d9c28f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample = train_text_data[0]\n",
    "train_sample_label = train_labels[0]\n",
    "word_ids = text_tokenizer.tokenize_text_to_id(train_sample)\n",
    "\n",
    "print(f\"train sample -> {train_sample}\")\n",
    "print(f\"train sample labe -> {train_sample_label}\")\n",
    "print(f\"tokenized train_sample -> {word_ids}\")\n",
    "print(f\"length of tokenized word {len(word_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02cb2ae-ec66-490d-bbc9-d2ce6a87c95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = test_text_data[0]\n",
    "test_sample_label = test_labels[0]\n",
    "word_ids = text_tokenizer.tokenize_text_to_id(test_sample)\n",
    "\n",
    "print(f\"test sample -> {test_sample}\")\n",
    "print(f\"test sample label -> {test_sample_label}\")\n",
    "print(f\"tokenized test_sample -> {word_ids}\")\n",
    "print(f\"length of tokenized word {len(word_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ea7893-2e19-419d-9d66-06cfb1edb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import random\n",
    "\n",
    "\"\"\"\n",
    "This function recomputes books by creating a set number of paragraphs per word and decade to ensure that \n",
    "a single book or period does not heavily weight the training dataset. Each book in each decade has a different number of paragraphs \n",
    "and thus books with more paragraphs in a particular decade can weight and skew the model's classification. \n",
    "\n",
    "The number of paragraphs \n",
    "created is the minimum number of paragraphs in each book in a particular decade and then further limited with the number of paragraphs \n",
    "specified by the decade paragraph. The downside to this approach is that data is lost that could be really helpful for training the model\n",
    "\"\"\"\n",
    "\n",
    "def balance_paragraphs(\n",
    "    train_data, train_labels, max_paragraphs_per_book=25, decade_paragraphs=600\n",
    "):\n",
    "    original_paragraph_count = Counter(train_labels)\n",
    "    max_paragraphs_per_book = float(\"inf\")\n",
    "    for decade, count in sorted(original_paragraph_count.items()):\n",
    "        print(f\"{decade}: {count} paragraphs\")\n",
    "        max_paragraphs_per_book = min(max_paragraphs_per_book, count)\n",
    "\n",
    "    # group by decade\n",
    "    decade_data = defaultdict(list)\n",
    "    for text, label in zip(train_data, train_labels):\n",
    "        decade_data[label].append(text)\n",
    "\n",
    "    new_paragraphs = []\n",
    "    new_labels = []\n",
    "    for decade, texts in decade_data.items():\n",
    "        book_size = 50\n",
    "        new_books = [texts[i : i + book_size] for i in range(0, len(texts), book_size)]\n",
    "        decade_books = []\n",
    "        for book in new_books:\n",
    "            if len(book) > max_paragraphs_per_book:\n",
    "                sample_paragraphs = random.sample(book, max_paragraphs_per_book)\n",
    "            else:\n",
    "                sample_paragraphs = book\n",
    "            decade_books.extend(sample_paragraphs)\n",
    "\n",
    "        new_paragraphs.extend(decade_books)\n",
    "        new_labels.extend([decade] * len(decade_books))\n",
    "\n",
    "    # balance decades\n",
    "    new_decade_data = defaultdict(list)\n",
    "    for text, label in zip(new_paragraphs, new_labels):\n",
    "        new_decade_data[label].append(text)\n",
    "\n",
    "    min_paragraph_size = min(len(text) for text in new_decade_data.values())\n",
    "    new_decade_paragraphs = min(decade_paragraphs, min_paragraph_size)\n",
    "\n",
    "    balance_paragraphs = []\n",
    "    balance_labels = []\n",
    "\n",
    "    for decade in sorted(new_decade_data.keys()):\n",
    "        text = new_decade_data[decade]\n",
    "        if len(text) >= new_decade_paragraphs:\n",
    "            sample_paragraphs = random.sample(text, new_decade_paragraphs)\n",
    "        else:\n",
    "            sample_paragraphs = random.choices(text, k=new_decade_paragraphs)\n",
    "\n",
    "        balance_paragraphs.extend(sample_paragraphs)\n",
    "        balance_labels.extend([decade] * len(sample_paragraphs))\n",
    "\n",
    "    return balance_paragraphs, balance_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c46468",
   "metadata": {},
   "source": [
    "## Libraries + Imports for building neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c572c98-6b01-44de-8982-d0148b304851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import codecs\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dea9c18",
   "metadata": {},
   "source": [
    "### Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d584c2-c10e-4a0a-a4f8-d84cefb51e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"create new tokenizer\")\n",
    "tokenizer = HistoricalTextTokenizer()\n",
    "\n",
    "print(f\"tokenize -> {clean_train_split_path}\")\n",
    "original_train_data, original_train_labels = tokenizer.process_files(\n",
    "    clean_train_split_path\n",
    ")\n",
    "print(f\"succesfully tokenized <- {clean_train_split_path}\")\n",
    "print(\n",
    "    f\"balance training data -> {len(original_train_data)} and balance labels: {len(original_train_labels)}\"\n",
    ")\n",
    "balance_train_data, balance_train_labels = balance_paragraphs(\n",
    "    original_train_data, original_train_labels\n",
    ")\n",
    "print(f\"succesfully balanced train data and labels <-\")\n",
    "\n",
    "# create train/validation splits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(f\"create cross validation splits from the training data using {80 - 20} split\")\n",
    "model_train_data, model_valid_data, model_train_labels, model_valid_labels = (\n",
    "    train_test_split(\n",
    "        balance_train_data,\n",
    "        balance_train_labels,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=balance_train_labels,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"tokenize -> {clean_test_split_path}\")\n",
    "test_data, test_labels = tokenizer.process_files(clean_test_split_path)\n",
    "print(f\"succesfully tokenized <- {clean_test_split_path}\")\n",
    "\n",
    "print(f\"create decade labels\")\n",
    "decades = sorted(set(model_train_labels + test_labels))\n",
    "decade_to_label = {decade: i for i, decade in enumerate(decades)}\n",
    "label_to_decade = {i: decade for i, decade in enumerate(decades)}\n",
    "print(f\"successfully created decades labels\")\n",
    "\n",
    "UNKNOWN = \"<unk>\"  # Unknown char or unknown word\n",
    "PADDING_WORD = \"<pad>\"\n",
    "id_to_label = [f\"decade_{decade}\" for decade in decades]\n",
    "\n",
    "\n",
    "def label_to_id(decade):\n",
    "    return decade_to_label[decade]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95471ba",
   "metadata": {},
   "source": [
    "### Load Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6dedda-4b39-40f0-9017-bae8e73cdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(\n",
    "    embedding_file, tokenizer, padding_word=PADDING_WORD, unknown_word=UNKNOWN\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads Glove embeddings from a file and aligns them with tokenizer vocabulary.\n",
    "    \"\"\"\n",
    "    glove_vectors = {}\n",
    "    D = None\n",
    "\n",
    "    with open(embedding_file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word = data[0]\n",
    "            vec = [float(x) for x in data[1:]]\n",
    "            if D is None:\n",
    "                D = len(vec)\n",
    "            glove_vectors[word] = vec\n",
    "\n",
    "    print(f\"Loaded {len(glove_vectors)} GloVe vectors with dimension {D}\")\n",
    "\n",
    "    embeddings = []\n",
    "    vocab_size = len(tokenizer.word2id)\n",
    "    found_in_glove = 0\n",
    "\n",
    "    # each word in tokenizer vocabulary\n",
    "    for word_id in range(vocab_size):\n",
    "        word = tokenizer.id2word[word_id]\n",
    "\n",
    "        if word == padding_word:\n",
    "            embeddings.append([0] * D)\n",
    "        elif word == unknown_word:\n",
    "            embeddings.append([-1] * D)\n",
    "        elif word in glove_vectors:\n",
    "            embeddings.append(glove_vectors[word])\n",
    "            found_in_glove += 1\n",
    "        # if word is not in glove create a random embedding\n",
    "        else:\n",
    "            embeddings.append([0.1] * D)\n",
    "\n",
    "    print(f\"Found {found_in_glove}/{vocab_size} words from vocabulary in GloVe\")\n",
    "    print(f\"Glove token Coverage: {100 * found_in_glove / vocab_size:.2f}%\")\n",
    "\n",
    "    return D, tokenizer.word2id, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e17ede",
   "metadata": {},
   "source": [
    "### Create Dataset and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3222bc53-9609-4a46-b31f-7aad9ce90b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoricalTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class loading NER dataset from a CSV file to be used as an input\n",
    "    to PyTorch DataLoader.\n",
    "\n",
    "    The CSV file has 4 fields: sentence number (only at the start of a new\n",
    "    sentence), word, POS tag (ignored), and label.\n",
    "\n",
    "    Datapoints are sentences + associated labels for each word. If the\n",
    "    words have not been seen before (i.e, they are not found in the\n",
    "    'word_to_id' dict), they will be mapped to the unknown word '<UNK>'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, word_to_id, decade_to_label):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.word_to_id = word_to_id\n",
    "        self.decade_to_label = decade_to_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        decade = self.labels[idx]\n",
    "        label_id = self.decade_to_label[decade]\n",
    "\n",
    "        return text, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ae535-18d8-4104-ad90-8a500707d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out some of these data structures\n",
    "dim, word_to_id, embeddings = load_glove_embeddings(glove_6b_100_path, tokenizer)\n",
    "\n",
    "print(\"The embedding for the word 'good' looks like this:\")\n",
    "print(embeddings[word_to_id[\"good\"]])\n",
    "print()\n",
    "\n",
    "# Read the data we are going to use for testing the model\n",
    "test_set = HistoricalTextDataset(test_data, test_labels, word_to_id, decade_to_label)\n",
    "print(\"There are\", len(test_set), \"documents in the testset\")\n",
    "dp = 0\n",
    "text, label = test_set[dp]\n",
    "print(\"Document\", dp, \"starts with:\", text[:100], \"...\")\n",
    "print(\"It has the label\", label, \"which corresponds to decade\", label_to_decade[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3273f2ed-85af-45a9-aace-de112649eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence_documents(batch, padding_word=PADDING_WORD):\n",
    "    \"\"\"\n",
    "    Dynamic Padding\n",
    "    \"\"\"\n",
    "    batch_data, batch_labels = zip(*batch)\n",
    "\n",
    "    # Convert documents to word ID sequences\n",
    "    batch_sequences = []\n",
    "    for text in batch_data:\n",
    "        word_ids = tokenizer.tokenize_text_to_id(text)\n",
    "        batch_sequences.append(word_ids)\n",
    "    max_len = max(map(len, batch_sequences))\n",
    "    padding_id = tokenizer.word2id[padding_word]\n",
    "\n",
    "    padded_data = [\n",
    "        [seq[i] if i < len(seq) else padding_id for i in range(max_len)]\n",
    "        for seq in batch_sequences\n",
    "    ]\n",
    "\n",
    "    return padded_data, list(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fd2fbd-59df-4d5d-81f3-440589844a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [(model_train_data[0], model_train_labels[0])]\n",
    "pad_sequence_documents(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00074a00",
   "metadata": {},
   "source": [
    "### LSTM Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9190c98d-5dd9-4ae3-a69f-5c563973a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        word_embeddings,  # Pre-trained word embeddings\n",
    "        word_to_id,  # Mapping from words to ids\n",
    "        num_classes,\n",
    "        word_hidden_size=128,\n",
    "        padding_word=PADDING_WORD,\n",
    "        unknown_word=UNKNOWN,\n",
    "        dropout_rate=0.3,\n",
    "        num_layers=1,\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super(DocumentClassifier, self).__init__()\n",
    "        self.padding_word = padding_word\n",
    "        self.unknown_word = unknown_word\n",
    "        self.word_to_id = word_to_id\n",
    "        self.word_hidden_size = word_hidden_size\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Create an embedding tensor for the words and import the Glove embeddings\n",
    "        vocabulary_size = len(word_embeddings)\n",
    "        self.word_emb_size = len(word_embeddings[0])\n",
    "\n",
    "        self.word_emb = nn.Embedding(vocabulary_size, self.word_emb_size)\n",
    "        self.word_emb.weight = nn.Parameter(\n",
    "            torch.tensor(word_embeddings, dtype=torch.float), requires_grad=False\n",
    "        )\n",
    "        self.embedding_dropout = nn.Dropout(dropout_rate * 0.3)\n",
    "        self.output_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # 1-layer LSTM\n",
    "        self.word_lstm = nn.LSTM(\n",
    "            self.word_emb_size,\n",
    "            self.word_hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0,\n",
    "        )\n",
    "\n",
    "        # Document Classification\n",
    "        self.final_pred = nn.Linear(self.word_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape\n",
    "        word_embeddings = self.word_emb(x)\n",
    "        word_embeddings = self.embedding_dropout(word_embeddings)\n",
    "\n",
    "        lstm_output, (hidden, cell) = self.word_lstm(word_embeddings)\n",
    "\n",
    "        complete_doc = hidden[0]\n",
    "\n",
    "        complete_doc = self.output_dropout(complete_doc)\n",
    "        logits = self.final_pred(complete_doc)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555d390d",
   "metadata": {},
   "source": [
    "### Test on small subset of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5d1722-d3d7-47bd-8abd-0ced359e6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ================== Hyper-parameters ==================== #\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Running on MGPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Running on CUDA\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "dim, word_to_id, embeddings = load_glove_embeddings(glove_6b_300_path, tokenizer)\n",
    "\n",
    "print(\"Using first 1000 documents for testing...\")\n",
    "train_data_small = model_train_data[:1000]\n",
    "train_labels_small = model_train_labels[:1000]\n",
    "valid_data_small = model_valid_data[:200]\n",
    "valid_labels_small = model_valid_labels[:200]\n",
    "test_data_small = test_data[:100]\n",
    "test_labels_small = test_labels[:100]\n",
    "\n",
    "training_set = HistoricalTextDataset(\n",
    "    train_data_small, train_labels_small, word_to_id, decade_to_label\n",
    ")\n",
    "validation_set = HistoricalTextDataset(\n",
    "    valid_data_small, valid_labels_small, word_to_id, decade_to_label\n",
    ")\n",
    "test_set = HistoricalTextDataset(\n",
    "    test_data_small, test_labels_small, word_to_id, decade_to_label\n",
    ")\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    training_set, batch_size=batch_size, collate_fn=pad_sequence_documents\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_sequence_documents,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=batch_size, collate_fn=pad_sequence_documents\n",
    ")\n",
    "\n",
    "print(f\"Training on {len(training_set)} documents\")\n",
    "print(f\"Validation on {len(validation_set)} documents\")\n",
    "print(f\"Testing on {len(test_set)} documents\")\n",
    "\n",
    "lstm_classifier = DocumentClassifier(\n",
    "    word_embeddings=embeddings,\n",
    "    word_to_id=word_to_id,\n",
    "    num_classes=len(decades),\n",
    "    word_hidden_size=64,\n",
    "    dropout_rate=0.2,\n",
    "    num_layers=1,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    lstm_classifier.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in tqdm(training_loader, desc=\"Epoch {}\".format(epoch + 1)):\n",
    "        x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = lstm_classifier(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(lstm_classifier.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "    lstm_classifier.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in validation_loader:\n",
    "            x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "            y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "            logits = lstm_classifier(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            val_total += y.size(0)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "\n",
    "    # Calculate metrics\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(validation_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_loss = epoch_loss / len(training_loader)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42d754",
   "metadata": {},
   "source": [
    "### Test on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d8acbf-f4dc-49f6-a686-60b03868a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Hyper-parameters ==================== #\n",
    "learning_rate = 0.001\n",
    "epochs = 8\n",
    "batch_size = 24\n",
    "# ======================= Training ======================= #\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Running on MGPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Running on CUDA\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "dim, word_to_id, embeddings = load_glove_embeddings(glove_6b_50_path, tokenizer)\n",
    "\n",
    "training_set = HistoricalTextDataset(\n",
    "    model_train_data, model_train_labels, word_to_id, decade_to_label\n",
    ")\n",
    "validation_set = HistoricalTextDataset(\n",
    "    model_valid_data, model_valid_labels, word_to_id, decade_to_label\n",
    ")\n",
    "test_set = HistoricalTextDataset(test_data, test_labels, word_to_id, decade_to_label)\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    training_set, batch_size=batch_size, shuffle=True, collate_fn=pad_sequence_documents\n",
    ")\n",
    "validation_loader = DataLoader(\n",
    "    validation_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=pad_sequence_documents,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set, batch_size=batch_size, collate_fn=pad_sequence_documents\n",
    ")\n",
    "\n",
    "print(f\"Training on {len(training_set)} documents\")\n",
    "print(f\"Validation on {len(validation_set)} documents\")\n",
    "print(f\"Testing on {len(test_set)} documents\")\n",
    "\n",
    "lstm_classifier = DocumentClassifier(\n",
    "    word_embeddings=embeddings,\n",
    "    word_to_id=word_to_id,\n",
    "    num_classes=len(decades),\n",
    "    word_hidden_size=64,\n",
    "    num_layers=1,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    lstm_classifier.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for x, y in tqdm(training_loader, desc=\"Epoch {}\".format(epoch + 1)):\n",
    "        x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = lstm_classifier(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(lstm_classifier.parameters(), 5)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "\n",
    "    lstm_classifier.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in validation_loader:\n",
    "            x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "            y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "            logits = lstm_classifier(x)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            val_total += y.size(0)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "\n",
    "    # calculate metrics\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(validation_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_loss = epoch_loss / len(training_loader)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch + 1}/{epochs}], Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
