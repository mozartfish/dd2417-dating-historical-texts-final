{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a38cde6a-5334-400f-b277-27ffefff7cc5",
   "metadata": {},
   "source": [
    "# RNN Architecture - Vanilla RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434b8eb6-40a5-4b13-be6e-1a4030a24ca0",
   "metadata": {},
   "source": [
    "## Setup - Libraries, Packages, Embeddings, Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9734d7f-45ae-4d7a-aed6-4b052c750249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os \n",
    "import urllib.request\n",
    "import zipfile \n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7ae1d9f-73ba-48eb-8666-4197fc8f60ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_path = \"./Embeddings\"\n",
    "def download_progress(block_num, block_size, total_size):\n",
    "    if not hasattr(download_progress, \"pbar\"):\n",
    "        download_progress.pbar = tqdm(total=total_size, unit=\"B\", unit_scale=True)\n",
    "    download_progress.pbar.update(block_size)\n",
    "\n",
    "if not os.path.exists(embeddings_path):\n",
    "    print(f\"create directory to store pre-trained glove embeddings\")\n",
    "    os.makedirs(embeddings_path)\n",
    "    print(f\"download pre-trained Glove Embeddings\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"http://nlp.stanford.edu/data/glove.6B.zip\",\n",
    "        \"./Embeddings/glove.6B.zip\",\n",
    "        download_progress,\n",
    "    )\n",
    "    print(\"unpack embeddings\")\n",
    "    with zipfile.ZipFile(\"./Embeddings/glove.6B.zip\", \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"./Embeddings/\")\n",
    "    os.remove(\"./Embeddings/glove.6B.zip\")\n",
    "    \n",
    "    print(\"embeddings download complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "664b703a-92c8-4051-b031-32242ab9bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_6b_50_path = \"./Embeddings/glove.6B.50d.txt\"\n",
    "glove_6b_100_path = \"./Embeddings/glove.6B.100d.txt\"\n",
    "glove_6b_200_path = \"./Embeddings/glove.6B.200d.txt\"\n",
    "glove_6b_300_path = \"./Embeddings/glove.6B.300d.txt\"\n",
    "clean_train_split_path = \"./Datasets/clean_train_split/\"\n",
    "clean_test_split_path = \"./Datasets/clean_test_split\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72149524-7cac-4343-8bbf-6599e361e1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download(\"punkt_tab\")\n",
    "import numpy as np \n",
    "from collections import defaultdict\n",
    "\n",
    "class HistoricalTextTokenizer:\n",
    "    \"\"\"\n",
    "    All of this code is adapted from Professor Johan Boye's DD2417 assignment tokenizers \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2id = defaultdict(lambda: None)\n",
    "        self.id2word = defaultdict(lambda: None)\n",
    "        self.latest_new_word = -1 \n",
    "        self.tokens_processed = 0 \n",
    "\n",
    "        self.UNKNOWN = '<unk>'\n",
    "        self.PADDING_WORD = '<pad>'\n",
    "\n",
    "        self.get_word_id(self.PADDING_WORD)\n",
    "        self.get_word_id(self.UNKNOWN)\n",
    "\n",
    "    def get_word_id(self, word):\n",
    "        word = word.lower()\n",
    "        if word in self.word2id:\n",
    "            return self.word2id[word]\n",
    "        else:\n",
    "            self.latest_new_word += 1\n",
    "            self.id2word[self.latest_new_word] = word\n",
    "            self.word2id[word] = self.latest_new_word\n",
    "            return self.latest_new_word\n",
    "\n",
    "    def process_files(self, file_or_dir):\n",
    "        all_texts = []\n",
    "        all_labels = []\n",
    "\n",
    "        if os.path.isdir(file_or_dir):\n",
    "            decade_dirs = sorted([d for d in os.listdir(file_or_dir) if os.path.isdir(os.path.join(file_or_dir, d))])\n",
    "            for decade_dir in decade_dirs:\n",
    "                decade_path = os.path.join(file_or_dir, decade_dir)\n",
    "                decade = int(decade_dir)\n",
    "                print(f\"Processing decade: {decade}\")\n",
    "                text_files = sorted([f for f in os.listdir(decade_path) if f.endswith(\".txt\")])\n",
    "                # print(f\"number of files in {decade} directory: {len(text_files)}\")\n",
    "\n",
    "                for file in text_files:\n",
    "                    filepath = os.path.join(decade_path, file)\n",
    "                    # print(f\"tokenize file {file}\")\n",
    "                    text, labels = self.process_file(filepath, decade)\n",
    "                    all_texts.extend(text)\n",
    "                    all_labels.extend(labels)\n",
    "        else:\n",
    "            texts, labels = self.process_file(file_or_dir, 0)\n",
    "            all_texts.extend(texts)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "        return all_texts, all_labels\n",
    "\n",
    "    def process_file(self, filepath, decade):\n",
    "        # print(filepath)\n",
    "        stream = open(filepath, mode=\"r\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "        text = stream.read()\n",
    "        stream.close()\n",
    "\n",
    "        try:\n",
    "            self.tokens = nltk.word_tokenize(text)\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt\")\n",
    "            self.tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        for i, token in enumerate(self.tokens):\n",
    "            self.tokens_processed += 1\n",
    "            word_id = self.get_word_id(token)\n",
    "\n",
    "            if self.tokens_processed % 1000000000 == 0:\n",
    "                print(\"Processed\", \"{:,}\".format(self.tokens_processed), \"tokens\")\n",
    "\n",
    "        paragraphs = self.create_paragraphs(text)\n",
    "        labels = [decade] * len(paragraphs)\n",
    "\n",
    "        return paragraphs, labels\n",
    "\n",
    "    def create_paragraphs(self, text, min_words=10, max_words=210):\n",
    "        words = text.split()\n",
    "        paragraphs = []\n",
    "        start = 0\n",
    "\n",
    "        while start < len(words):\n",
    "            end = min(start + max_words, len(words))\n",
    "            paragraph_words = words[start:end]\n",
    "            if len(paragraph_words) >= min_words:\n",
    "                paragraph_text = \" \".join(paragraph_words)\n",
    "                paragraphs.append(paragraph_text)\n",
    "            start = end\n",
    "\n",
    "        return paragraphs \n",
    "\n",
    "    def tokenize_text_to_id(self, text):\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "        except LookupError:\n",
    "            nltk.download(\"punkt\")\n",
    "            tokens = nltk.word_tokenize(text.lower())\n",
    "        word_ids = []\n",
    "        for token in tokens:\n",
    "            if token in self.word2id:\n",
    "                word_ids.append(self.word2id[token])\n",
    "            else:\n",
    "                word_ids.append(self.word2id[self.UNKNOWN])\n",
    "        return word_ids\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        return len(self.word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fab5d48-f7bc-422c-9f85-1f2415325a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokenizer = HistoricalTextTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a37f298-d874-43fb-b42e-f7659d012868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing decade: 1770\n",
      "Processing decade: 1810\n",
      "Processing decade: 1850\n",
      "Processing decade: 1890\n"
     ]
    }
   ],
   "source": [
    "train_text_data, train_labels = text_tokenizer.process_files(clean_train_split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e39b739b-96ad-4ba3-950d-994ddc17435f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing decade: 1770\n",
      "Processing decade: 1810\n",
      "Processing decade: 1850\n",
      "Processing decade: 1890\n"
     ]
    }
   ],
   "source": [
    "test_text_data, test_labels = text_tokenizer.process_files(clean_test_split_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c33e483a-7013-49b3-be27-c16fcc7d1a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1770: 0, 1810: 1, 1850: 2, 1890: 3}\n"
     ]
    }
   ],
   "source": [
    "labels = sorted(set(train_labels + test_labels))\n",
    "decade_to_label = {decade: i for i, decade in enumerate(labels)}\n",
    "print(f\"{decade_to_label}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e279346a-62f2-44b5-bbb8-09cc3fb8d41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train labels -> 28336\n",
      "length of train text(paragraphs) -> 28336\n",
      "\n",
      "number of test labels -> 7886\n",
      "length of test text -> 7886\n",
      "\n",
      "train text Produced by Gary R. Young THE SCHOOL FOR SCANDAL A COMEDY A PORTRAIT<1> BY R. B. SHERIDAN, ESQ. Transcriber's Comments on the preparation of this E-Text: SQUARE BRACKETS: The square brackets, i.e. [ ] are copied from the printed book, without change, except that a closing bracket \"]\" has been added to the stage directions. FOOTNOTES: For this E-Text version of the book, the footnotes have been consolidated at the end of the play. Numbering of the footnotes has been changed, and each footnote is given a unique identity in the form <X>. CHANGES TO THE TEXT: Character names have been expanded. For Example, SIR BENJAMIN was SIR BEN. THE TEXT OF THE SCHOOL FOR SCANDAL The text of THE SCHOOL FOR SCANDAL in this edition is taken, by Mr. Fraser Rae's generous permission, from his SHERIDAN'S PLAYS NOW PRINTED AS HE WROTE THEM. In his Prefatory Notes (xxxvii), Mr. Rae writes: \"The manuscript of it [THE SCHOOL FOR SCANDAL] in Sheridan's own handwriting is preserved at Frampton Court and is now printed in this volume. This version differs in many respects from that which is generally known, and I think it is even better than that which has hitherto been read and acted. As I have endeavoured to reproduce\n",
      "train label 1770\n",
      "\n",
      "test text(paragraphs) An Inquiry into the Nature and Causes of the Wealth of Nations by Adam Smith Contents INTRODUCTION AND PLAN OF THE WORK. BOOK I. OF THE CAUSES OF IMPROVEMENT IN THE PRODUCTIVE POWERS OF LABOUR, AND OF THE ORDER ACCORDING TO WHICH ITS PRODUCE IS NATURALLY DISTRIBUTED AMONG THE DIFFERENT RANKS OF THE PEOPLE. CHAPTER I. OF THE DIVISION OF LABOUR. CHAPTER II. OF THE PRINCIPLE WHICH GIVES OCCASION TO THE DIVISION OF LABOUR. CHAPTER III. THAT THE DIVISION OF LABOUR IS LIMITED BY THE EXTENT OF THE MARKET. CHAPTER IV. OF THE ORIGIN AND USE OF MONEY. CHAPTER V. OF THE REAL AND NOMINAL PRICE OF COMMODITIES, OR OF THEIR PRICE IN LABOUR, AND THEIR PRICE IN MONEY. CHAPTER VI. OF THE COMPONENT PART OF THE PRICE OF COMMODITIES. CHAPTER VII. OF THE NATURAL AND MARKET PRICE OF COMMODITIES. CHAPTER VIII. OF THE WAGES OF LABOUR. CHAPTER IX. OF THE PROFITS OF STOCK. CHAPTER X. OF WAGES AND PROFIT IN THE DIFFERENT EMPLOYMENTS OF LABOUR AND STOCK. CHAPTER XI. OF THE RENT OF LAND. BOOK II. OF THE NATURE, ACCUMULATION, AND EMPLOYMENT OF STOCK. CHAPTER I. OF THE DIVISION OF STOCK. CHAPTER II. OF MONEY, CONSIDERED AS A PARTICULAR BRANCH OF THE GENERAL STOCK OF THE SOCIETY, OR OF\n",
      "test label 1770\n"
     ]
    }
   ],
   "source": [
    "print(f\"number of train labels -> {len(train_labels)}\")\n",
    "print(f\"length of train text(paragraphs) -> {len(train_text_data)}\")\n",
    "print()\n",
    "\n",
    "print(f\"number of test labels -> {len(test_labels)}\")\n",
    "print(f\"length of test text -> {len(test_text_data)}\")\n",
    "print()\n",
    "\n",
    "print(f\"train text {train_text_data[0]}\")\n",
    "print(f\"train label {train_labels[0]}\")\n",
    "print()\n",
    "\n",
    "print(f\"test text(paragraphs) {test_text_data[0]}\")\n",
    "print(f\"test label {test_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e4d1c21-4dad-434e-bb70-9f2d9c28f5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train sample -> Produced by Gary R. Young THE SCHOOL FOR SCANDAL A COMEDY A PORTRAIT<1> BY R. B. SHERIDAN, ESQ. Transcriber's Comments on the preparation of this E-Text: SQUARE BRACKETS: The square brackets, i.e. [ ] are copied from the printed book, without change, except that a closing bracket \"]\" has been added to the stage directions. FOOTNOTES: For this E-Text version of the book, the footnotes have been consolidated at the end of the play. Numbering of the footnotes has been changed, and each footnote is given a unique identity in the form <X>. CHANGES TO THE TEXT: Character names have been expanded. For Example, SIR BENJAMIN was SIR BEN. THE TEXT OF THE SCHOOL FOR SCANDAL The text of THE SCHOOL FOR SCANDAL in this edition is taken, by Mr. Fraser Rae's generous permission, from his SHERIDAN'S PLAYS NOW PRINTED AS HE WROTE THEM. In his Prefatory Notes (xxxvii), Mr. Rae writes: \"The manuscript of it [THE SCHOOL FOR SCANDAL] in Sheridan's own handwriting is preserved at Frampton Court and is now printed in this volume. This version differs in many respects from that which is generally known, and I think it is even better than that which has hitherto been read and acted. As I have endeavoured to reproduce\n",
      "train sample labe -> 1770\n",
      "tokenized train_sample -> [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 11, 13, 14, 15, 16, 3, 5, 17, 18, 19, 20, 21, 22, 23, 24, 25, 7, 26, 27, 28, 29, 30, 31, 32, 30, 7, 31, 32, 19, 33, 21, 34, 35, 36, 37, 38, 7, 39, 40, 19, 41, 42, 19, 43, 44, 11, 45, 46, 47, 35, 48, 49, 50, 51, 52, 7, 53, 54, 21, 55, 30, 9, 28, 29, 56, 27, 7, 40, 19, 7, 55, 57, 50, 58, 59, 7, 60, 27, 7, 61, 21, 62, 27, 7, 55, 49, 50, 63, 19, 64, 65, 66, 67, 68, 11, 69, 70, 71, 7, 72, 14, 73, 16, 21, 74, 52, 7, 75, 30, 76, 77, 57, 50, 78, 21, 9, 79, 19, 80, 81, 82, 80, 83, 21, 7, 75, 27, 7, 8, 9, 10, 7, 75, 27, 7, 8, 9, 10, 71, 28, 84, 67, 85, 19, 3, 86, 87, 88, 23, 89, 90, 19, 38, 91, 18, 23, 92, 93, 39, 94, 95, 96, 97, 21, 71, 91, 98, 99, 100, 101, 102, 19, 86, 88, 103, 30, 47, 7, 104, 27, 105, 34, 7, 8, 9, 10, 35, 71, 18, 23, 106, 107, 67, 108, 59, 109, 110, 64, 67, 93, 39, 71, 28, 111, 21, 28, 56, 112, 71, 113, 114, 38, 44, 115, 67, 116, 117, 19, 64, 118, 119, 105, 67, 120, 121, 122, 44, 115, 49, 123, 50, 124, 64, 125, 21, 94, 118, 57, 126, 52, 127]\n",
      "length of tokenized word 252\n"
     ]
    }
   ],
   "source": [
    "train_sample = train_text_data[0]\n",
    "train_sample_label = train_labels[0]\n",
    "word_ids = text_tokenizer.tokenize_text_to_id(train_sample)\n",
    "\n",
    "print(f\"train sample -> {train_sample}\")\n",
    "print(f\"train sample labe -> {train_sample_label}\")\n",
    "print(f\"tokenized train_sample -> {word_ids}\")\n",
    "print(f\"length of tokenized word {len(word_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e02cb2ae-ec66-490d-bbc9-d2ce6a87c95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test sample -> An Inquiry into the Nature and Causes of the Wealth of Nations by Adam Smith Contents INTRODUCTION AND PLAN OF THE WORK. BOOK I. OF THE CAUSES OF IMPROVEMENT IN THE PRODUCTIVE POWERS OF LABOUR, AND OF THE ORDER ACCORDING TO WHICH ITS PRODUCE IS NATURALLY DISTRIBUTED AMONG THE DIFFERENT RANKS OF THE PEOPLE. CHAPTER I. OF THE DIVISION OF LABOUR. CHAPTER II. OF THE PRINCIPLE WHICH GIVES OCCASION TO THE DIVISION OF LABOUR. CHAPTER III. THAT THE DIVISION OF LABOUR IS LIMITED BY THE EXTENT OF THE MARKET. CHAPTER IV. OF THE ORIGIN AND USE OF MONEY. CHAPTER V. OF THE REAL AND NOMINAL PRICE OF COMMODITIES, OR OF THEIR PRICE IN LABOUR, AND THEIR PRICE IN MONEY. CHAPTER VI. OF THE COMPONENT PART OF THE PRICE OF COMMODITIES. CHAPTER VII. OF THE NATURAL AND MARKET PRICE OF COMMODITIES. CHAPTER VIII. OF THE WAGES OF LABOUR. CHAPTER IX. OF THE PROFITS OF STOCK. CHAPTER X. OF WAGES AND PROFIT IN THE DIFFERENT EMPLOYMENTS OF LABOUR AND STOCK. CHAPTER XI. OF THE RENT OF LAND. BOOK II. OF THE NATURE, ACCUMULATION, AND EMPLOYMENT OF STOCK. CHAPTER I. OF THE DIVISION OF STOCK. CHAPTER II. OF MONEY, CONSIDERED AS A PARTICULAR BRANCH OF THE GENERAL STOCK OF THE SOCIETY, OR OF\n",
      "test sample label -> 1770\n",
      "tokenized test_sample -> [213, 4503, 1171, 7, 346, 64, 5959, 27, 7, 5805, 27, 4787, 3, 37690, 729, 4827, 9485, 64, 3338, 27, 7, 4017, 21, 40, 210, 27, 7, 5959, 27, 8492, 71, 7, 7859, 13668, 27, 4546, 19, 64, 27, 7, 3374, 207, 52, 115, 850, 2461, 67, 1970, 8448, 1243, 7, 167, 12025, 27, 7, 1186, 21, 4006, 210, 27, 7, 11304, 27, 4546, 21, 4006, 1825, 21, 27, 7, 1607, 115, 2343, 3429, 52, 7, 11304, 27, 4546, 21, 4006, 2403, 21, 44, 7, 11304, 27, 4546, 67, 18473, 3, 7, 8134, 27, 7, 4142, 21, 4006, 2934, 21, 27, 7, 12512, 64, 177, 27, 2425, 21, 4006, 14814, 27, 7, 1115, 64, 34090, 3011, 27, 36241, 19, 143, 27, 537, 3011, 71, 4546, 19, 64, 537, 3011, 71, 2425, 21, 4006, 12946, 21, 27, 7, 70920, 169, 27, 7, 3011, 27, 36241, 21, 4006, 14815, 21, 27, 7, 1039, 64, 4142, 3011, 27, 36241, 21, 4006, 14816, 21, 27, 7, 2669, 27, 4546, 21, 4006, 14817, 21, 27, 7, 4054, 27, 2522, 21, 4006, 14818, 27, 2669, 64, 1123, 71, 7, 167, 19097, 27, 4546, 64, 2522, 21, 4006, 14819, 21, 27, 7, 7593, 27, 2847, 21, 40, 1825, 21, 27, 7, 346, 19, 14077, 19, 64, 9218, 27, 2522, 21, 4006, 210, 27, 7, 11304, 27, 2522, 21, 4006, 1825, 21, 27, 2425, 19, 4865, 94, 11, 2195, 7525, 27, 7, 2184, 2522, 27, 7, 1997, 19, 143, 27]\n",
      "length of tokenized word 242\n"
     ]
    }
   ],
   "source": [
    "test_sample = test_text_data[0]\n",
    "test_sample_label = test_labels[0]\n",
    "word_ids = text_tokenizer.tokenize_text_to_id(test_sample)\n",
    "\n",
    "print(f\"test sample -> {test_sample}\")\n",
    "print(f\"test sample label -> {test_sample_label}\")\n",
    "print(f\"tokenized test_sample -> {word_ids}\")\n",
    "print(f\"length of tokenized word {len(word_ids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5ea7893-2e19-419d-9d66-06cfb1edb6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import random \n",
    "def balance_paragraphs(train_data, train_labels, max_paragraphs_per_book=25, decade_paragraphs=600):\n",
    "    original_paragraph_count = Counter(train_labels)\n",
    "    max_paragraphs_per_book = float('inf')\n",
    "    for decade, count in sorted(original_paragraph_count.items()):\n",
    "        print(f\"{decade}: {count} paragraphs\")\n",
    "        max_paragraphs_per_book = min(max_paragraphs_per_book, count)\n",
    "\n",
    "    # Group by decade \n",
    "    decade_data = defaultdict(list)\n",
    "    for text, label in zip(train_data, train_labels):\n",
    "        decade_data[label].append(text)\n",
    "\n",
    "    new_paragraphs = []\n",
    "    new_labels = []\n",
    "    for decade, texts in decade_data.items():\n",
    "        book_size = 50 \n",
    "        new_books = [texts[i:i+book_size] for i in range(0, len(texts), book_size)]\n",
    "        decade_books = []\n",
    "        for book in new_books:\n",
    "            if len(book) > max_paragraphs_per_book:\n",
    "                sample_paragraphs = random.sample(book, max_paragraphs_per_book)\n",
    "            else:\n",
    "                sample_paragraphs = book \n",
    "            decade_books.extend(sample_paragraphs)\n",
    "                \n",
    "        new_paragraphs.extend(decade_books)  \n",
    "        new_labels.extend([decade] * len(decade_books))\n",
    "\n",
    "    # Balance decades \n",
    "    new_decade_data = defaultdict(list)\n",
    "    for text, label in zip(new_paragraphs, new_labels):\n",
    "        new_decade_data[label].append(text)\n",
    "\n",
    "    min_paragraph_size = min(len(text) for text in new_decade_data.values())\n",
    "    new_decade_paragraphs = min(decade_paragraphs, min_paragraph_size)\n",
    "\n",
    "    balance_paragraphs = []\n",
    "    balance_labels = []\n",
    "\n",
    "    for decade in sorted(new_decade_data.keys()):      \n",
    "        text = new_decade_data[decade]                  \n",
    "        if len(text) >= new_decade_paragraphs:          \n",
    "            sample_paragraphs = random.sample(text, new_decade_paragraphs) \n",
    "        else:\n",
    "            sample_paragraphs = random.choices(text, k=new_decade_paragraphs)\n",
    "    \n",
    "        balance_paragraphs.extend(sample_paragraphs)    \n",
    "        balance_labels.extend([decade] * len(sample_paragraphs))  \n",
    "\n",
    "    return balance_paragraphs, balance_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c572c98-6b01-44de-8982-d0148b304851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import codecs\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13d584c2-c10e-4a0a-a4f8-d84cefb51e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create new tokenizer\n",
      "tokenize -> ./Datasets/clean_train_split/\n",
      "Processing decade: 1770\n",
      "Processing decade: 1810\n",
      "Processing decade: 1850\n",
      "Processing decade: 1890\n",
      "succesfully tokenized <- ./Datasets/clean_train_split/\n",
      "balance training data -> 28336 and balance labels: 28336\n",
      "1770: 10579 paragraphs\n",
      "1810: 7485 paragraphs\n",
      "1850: 6847 paragraphs\n",
      "1890: 3425 paragraphs\n",
      "succesfully balanced train data and labels <-\n",
      "tokenize -> ./Datasets/clean_test_split\n",
      "Processing decade: 1770\n",
      "Processing decade: 1810\n",
      "Processing decade: 1850\n",
      "Processing decade: 1890\n",
      "succesfully tokenized <- ./Datasets/clean_test_split\n",
      "create decade labels\n",
      "successfully created decades labels\n"
     ]
    }
   ],
   "source": [
    "print(f\"create new tokenizer\")\n",
    "tokenizer = HistoricalTextTokenizer()\n",
    "\n",
    "print(f\"tokenize -> {clean_train_split_path}\")\n",
    "original_train_data, original_train_labels = tokenizer.process_files(clean_train_split_path)\n",
    "print(f\"succesfully tokenized <- {clean_train_split_path}\")\n",
    "print(f\"balance training data -> {len(original_train_data)} and balance labels: {len(original_train_labels)}\")\n",
    "balance_train_data, balance_train_labels = balance_paragraphs(original_train_data, original_train_labels)\n",
    "print(f\"succesfully balanced train data and labels <-\")\n",
    "\n",
    "# create train/validation splits \n",
    "from sklearn.model_selection import train_test_split\n",
    "model_train_data, model_valid_data, model_train_labels, model_valid_labels = train_test_split(\n",
    "    balance_train_data, balance_train_labels, \n",
    "    test_size=0.2, random_state=42, stratify=balance_train_labels\n",
    ")\n",
    "\n",
    "print(f\"tokenize -> {clean_test_split_path}\")\n",
    "test_data, test_labels = tokenizer.process_files(clean_test_split_path)\n",
    "print(f\"succesfully tokenized <- {clean_test_split_path}\")\n",
    "\n",
    "print(f\"create decade labels\")\n",
    "decades = sorted(set(model_train_labels + test_labels))\n",
    "decade_to_label = {decade: i for i, decade in enumerate(decades)}\n",
    "label_to_decade = {i: decade for i, decade in enumerate(decades)}\n",
    "print(f\"successfully created decades labels\")\n",
    "\n",
    "UNKNOWN = \"<unk>\"  # Unknown char or unknown word\n",
    "PADDING_WORD = \"<pad>\"\n",
    "id_to_label = [f\"decade_{decade}\" for decade in decades]\n",
    "\n",
    "def label_to_id(decade):\n",
    "    return decade_to_label[decade]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e6dedda-4b39-40f0-9017-bae8e73cdd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings_aligned(\n",
    "    embedding_file, tokenizer, padding_word=PADDING_WORD, unknown_word=UNKNOWN\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads Glove embeddings from a file and aligns them with tokenizer vocabulary.\n",
    "    \"\"\"\n",
    "    glove_vectors = {}\n",
    "    D = None\n",
    "    \n",
    "    with open(embedding_file, encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word = data[0]\n",
    "            vec = [float(x) for x in data[1:]]\n",
    "            if D is None:\n",
    "                D = len(vec)\n",
    "            glove_vectors[word] = vec\n",
    "    \n",
    "    print(f\"Loaded {len(glove_vectors)} GloVe vectors with dimension {D}\")\n",
    "    \n",
    "    embeddings = []\n",
    "    vocab_size = len(tokenizer.word2id)\n",
    "    found_in_glove = 0\n",
    "    \n",
    "    # each word in tokenizer vocabulary\n",
    "    for word_id in range(vocab_size):\n",
    "        word = tokenizer.id2word[word_id]\n",
    "        \n",
    "        if word == padding_word:\n",
    "            embeddings.append([0] * D)  \n",
    "        elif word == unknown_word:\n",
    "            embeddings.append([-1] * D)  \n",
    "        elif word in glove_vectors:\n",
    "            embeddings.append(glove_vectors[word])\n",
    "            found_in_glove += 1\n",
    "        else:\n",
    "            embeddings.append([0.1] * D)\n",
    "    \n",
    "    print(f\"Found {found_in_glove}/{vocab_size} words from your vocabulary in GloVe\")\n",
    "    print(f\"Coverage: {100 * found_in_glove / vocab_size:.2f}%\")\n",
    "    \n",
    "    return D, tokenizer.word2id, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3222bc53-9609-4a46-b31f-7aad9ce90b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoricalTextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class loading NER dataset from a CSV file to be used as an input\n",
    "    to PyTorch DataLoader.\n",
    "\n",
    "    The CSV file has 4 fields: sentence number (only at the start of a new\n",
    "    sentence), word, POS tag (ignored), and label.\n",
    "\n",
    "    Datapoints are sentences + associated labels for each word. If the\n",
    "    words have not been seen before (i.e, they are not found in the\n",
    "    'word_to_id' dict), they will be mapped to the unknown word '<UNK>'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels, word_to_id, decade_to_label):\n",
    "        self.texts = texts \n",
    "        self.labels = labels \n",
    "        self.word_to_id = word_to_id\n",
    "        self.decade_to_label = decade_to_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        decade = self.labels[idx]\n",
    "        label_id = self.decade_to_label[decade]\n",
    "\n",
    "        return text, label_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea4ae535-18d8-4104-ad90-8a500707d095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 GloVe vectors with dimension 100\n",
      "Found 55798/146797 words from your vocabulary in GloVe\n",
      "Coverage: 38.01%\n",
      "The embedding for the word 'good' looks like this:\n",
      "[-0.030769, 0.11993, 0.53909, -0.43696, -0.73937, -0.15345, 0.081126, -0.38559, -0.68797, -0.41632, -0.13183, -0.24922, 0.441, 0.085919, 0.20871, -0.063582, 0.062228, -0.051234, -0.13398, 1.1418, 0.036526, 0.49029, -0.24567, -0.412, 0.12349, 0.41336, -0.48397, -0.54243, -0.27787, -0.26015, -0.38485, 0.78656, 0.1023, -0.20712, 0.40751, 0.32026, -0.51052, 0.48362, -0.0099498, -0.38685, 0.034975, -0.167, 0.4237, -0.54164, -0.30323, -0.36983, 0.082836, -0.52538, -0.064531, -1.398, -0.14873, -0.35327, -0.1118, 1.0912, 0.095864, -2.8129, 0.45238, 0.46213, 1.6012, -0.20837, -0.27377, 0.71197, -1.0754, -0.046974, 0.67479, -0.065839, 0.75824, 0.39405, 0.15507, -0.64719, 0.32796, -0.031748, 0.52899, -0.43886, 0.67405, 0.42136, -0.11981, -0.21777, -0.29756, -0.1351, 0.59898, 0.46529, -0.58258, -0.02323, -1.5442, 0.01901, -0.015877, 0.024499, -0.58017, -0.67659, -0.040379, -0.44043, 0.083292, 0.20035, -0.75499, 0.16918, -0.26573, -0.52878, 0.17584, 1.065]\n",
      "\n",
      "There are 7886 documents in the testset\n",
      "Document 0 starts with: An Inquiry into the Nature and Causes of the Wealth of Nations by Adam Smith Contents INTRODUCTION A ...\n",
      "It has the label 0 which corresponds to decade 1770\n"
     ]
    }
   ],
   "source": [
    "# Let's check out some of these data structures\n",
    "dim, word_to_id, embeddings = load_glove_embeddings_aligned(glove_6b_100_path, tokenizer)\n",
    "\n",
    "print(\"The embedding for the word 'good' looks like this:\")\n",
    "print(embeddings[word_to_id[\"good\"]])\n",
    "print()\n",
    "\n",
    "# Read the data we are going to use for testing the model\n",
    "test_set = HistoricalTextDataset(test_data, test_labels, word_to_id, decade_to_label\n",
    ")\n",
    "print(\"There are\", len(test_set), \"documents in the testset\")\n",
    "dp = 0\n",
    "text, label = test_set[dp]\n",
    "print(\"Document\", dp, \"starts with:\", text[:100], \"...\")\n",
    "print(\"It has the label\", label, \"which corresponds to decade\", label_to_decade[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3273f2ed-85af-45a9-aace-de112649eaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence_documents(batch, padding_word=PADDING_WORD):\n",
    "    \"\"\"\n",
    "    Dynamic Padding\n",
    "    \"\"\"\n",
    "    batch_data, batch_labels = zip(*batch)\n",
    "    \n",
    "    # Convert documents to word ID sequences\n",
    "    batch_sequences = []\n",
    "    for text in batch_data:\n",
    "        word_ids = tokenizer.tokenize_text_to_id(text)\n",
    "        batch_sequences.append(word_ids)\n",
    "    max_len = max(map(len, batch_sequences))\n",
    "    padding_id = tokenizer.word2id[padding_word]\n",
    "    \n",
    "    padded_data = [[seq[i] if i < len(seq) else padding_id for i in range(max_len)] \n",
    "                   for seq in batch_sequences]\n",
    "    \n",
    "    return padded_data, list(batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9fd2fbd-59df-4d5d-81f3-440589844a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[105,\n",
       "   67,\n",
       "   7,\n",
       "   1172,\n",
       "   21695,\n",
       "   1697,\n",
       "   64,\n",
       "   2278,\n",
       "   52,\n",
       "   194,\n",
       "   64,\n",
       "   850,\n",
       "   21411,\n",
       "   19,\n",
       "   2906,\n",
       "   19,\n",
       "   64,\n",
       "   105,\n",
       "   19074,\n",
       "   130,\n",
       "   52,\n",
       "   285,\n",
       "   21,\n",
       "   105,\n",
       "   67,\n",
       "   7,\n",
       "   4017,\n",
       "   27,\n",
       "   11,\n",
       "   13376,\n",
       "   1099,\n",
       "   2054,\n",
       "   19,\n",
       "   71,\n",
       "   1068,\n",
       "   161,\n",
       "   82,\n",
       "   163,\n",
       "   12821,\n",
       "   310,\n",
       "   95,\n",
       "   79654,\n",
       "   28,\n",
       "   2731,\n",
       "   195,\n",
       "   91,\n",
       "   133,\n",
       "   19,\n",
       "   81783,\n",
       "   64,\n",
       "   108631,\n",
       "   105,\n",
       "   71,\n",
       "   91,\n",
       "   567,\n",
       "   19,\n",
       "   64,\n",
       "   71,\n",
       "   91,\n",
       "   275,\n",
       "   15053,\n",
       "   105,\n",
       "   52,\n",
       "   7903,\n",
       "   21,\n",
       "   118,\n",
       "   1246,\n",
       "   3,\n",
       "   850,\n",
       "   422,\n",
       "   44,\n",
       "   105,\n",
       "   67,\n",
       "   5909,\n",
       "   3,\n",
       "   7,\n",
       "   1172,\n",
       "   2347,\n",
       "   343,\n",
       "   64,\n",
       "   118,\n",
       "   162,\n",
       "   1449,\n",
       "   899,\n",
       "   19,\n",
       "   93347,\n",
       "   19,\n",
       "   67,\n",
       "   105,\n",
       "   301,\n",
       "   394,\n",
       "   105,\n",
       "   67,\n",
       "   163,\n",
       "   3797,\n",
       "   27,\n",
       "   1266,\n",
       "   19,\n",
       "   52,\n",
       "   12878,\n",
       "   11,\n",
       "   427,\n",
       "   343,\n",
       "   118,\n",
       "   162,\n",
       "   224,\n",
       "   382,\n",
       "   6944,\n",
       "   52,\n",
       "   3979,\n",
       "   64,\n",
       "   436,\n",
       "   122,\n",
       "   118,\n",
       "   2599,\n",
       "   52,\n",
       "   93347,\n",
       "   120,\n",
       "   21,\n",
       "   118,\n",
       "   932,\n",
       "   850,\n",
       "   50147,\n",
       "   10979,\n",
       "   19,\n",
       "   64,\n",
       "   7,\n",
       "   10708,\n",
       "   44,\n",
       "   1156,\n",
       "   1238,\n",
       "   18932,\n",
       "   27995,\n",
       "   343,\n",
       "   71,\n",
       "   7,\n",
       "   9602,\n",
       "   27,\n",
       "   375,\n",
       "   133,\n",
       "   36,\n",
       "   850,\n",
       "   2731,\n",
       "   64,\n",
       "   850,\n",
       "   10819,\n",
       "   19,\n",
       "   64,\n",
       "   850,\n",
       "   16978,\n",
       "   5988,\n",
       "   495,\n",
       "   2017,\n",
       "   71,\n",
       "   375,\n",
       "   567,\n",
       "   21,\n",
       "   7,\n",
       "   12467,\n",
       "   832,\n",
       "   566,\n",
       "   52,\n",
       "   1012,\n",
       "   59,\n",
       "   105,\n",
       "   343,\n",
       "   225,\n",
       "   118,\n",
       "   3039,\n",
       "   44,\n",
       "   7,\n",
       "   20860,\n",
       "   64,\n",
       "   41398,\n",
       "   64,\n",
       "   108704,\n",
       "   19,\n",
       "   64,\n",
       "   552,\n",
       "   9361,\n",
       "   200,\n",
       "   57,\n",
       "   11,\n",
       "   6014,\n",
       "   16652,\n",
       "   64,\n",
       "   1246,\n",
       "   105,\n",
       "   999,\n",
       "   19,\n",
       "   36,\n",
       "   121,\n",
       "   918,\n",
       "   9,\n",
       "   7,\n",
       "   596,\n",
       "   21,\n",
       "   7,\n",
       "   8988,\n",
       "   1234,\n",
       "   224,\n",
       "   1242,\n",
       "   59,\n",
       "   779,\n",
       "   19,\n",
       "   143,\n",
       "   91,\n",
       "   346,\n",
       "   1234,\n",
       "   224,\n",
       "   19,\n",
       "   44,\n",
       "   95,\n",
       "   49,\n",
       "   4641,\n",
       "   28,\n",
       "   9451,\n",
       "   27,\n",
       "   9408,\n",
       "   64,\n",
       "   13004,\n",
       "   865,\n",
       "   59,\n",
       "   1007,\n",
       "   1695,\n",
       "   7,\n",
       "   1736,\n",
       "   21,\n",
       "   539,\n",
       "   991,\n",
       "   137,\n",
       "   865,\n",
       "   19,\n",
       "   105,\n",
       "   20503,\n",
       "   52,\n",
       "   6160,\n",
       "   975,\n",
       "   108705,\n",
       "   64]],\n",
       " [1850])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [(model_train_data[0], model_train_labels[0])]\n",
    "pad_sequence_documents(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9190c98d-5dd9-4ae3-a69f-5c563973a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        word_embeddings,  # Pre-trained word embeddings\n",
    "        word_to_id,  # Mapping from words to ids\n",
    "        num_classes,  # Number of decades to classify\n",
    "        word_hidden_size=128,\n",
    "        padding_word=PADDING_WORD,\n",
    "        unknown_word=UNKNOWN,\n",
    "        dropout_rate=0.3,\n",
    "        num_layers=1,  # Keep as 1 for vanilla RNN\n",
    "        device=\"cpu\",\n",
    "    ):\n",
    "        super(DocumentClassifier, self).__init__()\n",
    "        self.padding_word = padding_word\n",
    "        self.unknown_word = unknown_word\n",
    "        self.word_to_id = word_to_id\n",
    "        self.word_hidden_size = word_hidden_size\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Create an embedding tensor for the words and import the Glove embeddings\n",
    "        vocabulary_size = len(word_embeddings)\n",
    "        self.word_emb_size = len(word_embeddings[0])\n",
    "\n",
    "        self.word_emb = nn.Embedding(vocabulary_size, self.word_emb_size)\n",
    "        self.word_emb.weight = nn.Parameter(\n",
    "            torch.tensor(word_embeddings, dtype=torch.float), requires_grad=False\n",
    "        )\n",
    "        self.embedding_dropout = nn.Dropout(dropout_rate * 0.3)\n",
    "        self.output_dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.word_rnn = nn.RNN(\n",
    "            self.word_emb_size,\n",
    "            self.word_hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=0,  \n",
    "            nonlinearity='tanh' \n",
    "        )\n",
    "\n",
    "        # Document Classification\n",
    "        self.final_pred = nn.Linear(self.word_hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length = x.shape\n",
    "        word_embeddings = self.word_emb(x)\n",
    "        word_embeddings = self.embedding_dropout(word_embeddings)\n",
    "\n",
    "        rnn_output, hidden = self.word_rnn(word_embeddings)\n",
    "\n",
    "        complete_doc = hidden[0]  \n",
    "\n",
    "        complete_doc = self.output_dropout(complete_doc)\n",
    "        logits = self.final_pred(complete_doc)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da5d1722-d3d7-47bd-8abd-0ced359e6b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CUDA\n",
      "Loaded 400000 GloVe vectors with dimension 300\n",
      "Found 55798/146797 words from your vocabulary in GloVe\n",
      "Coverage: 38.01%\n",
      "Using first 10000 documents for testing...\n",
      "Training on 1000 documents\n",
      "Validation on 200 documents\n",
      "Testing on 100 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 125/125 [00:01<00:00, 80.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 1.7273, Train Acc: 22.90%, Val Loss: 1.5581, Val Acc: 34.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 125/125 [00:01<00:00, 104.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Train Loss: 1.6227, Train Acc: 25.70%, Val Loss: 1.4563, Val Acc: 34.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 125/125 [00:01<00:00, 98.57it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Train Loss: 1.5241, Train Acc: 27.60%, Val Loss: 1.3812, Val Acc: 31.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 125/125 [00:01<00:00, 99.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Train Loss: 1.5071, Train Acc: 26.80%, Val Loss: 1.3886, Val Acc: 29.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 125/125 [00:01<00:00, 101.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Train Loss: 1.4687, Train Acc: 24.80%, Val Loss: 1.3810, Val Acc: 28.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 125/125 [00:01<00:00, 99.86it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Train Loss: 1.4588, Train Acc: 24.70%, Val Loss: 1.3796, Val Acc: 28.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 125/125 [00:01<00:00, 104.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Train Loss: 1.4373, Train Acc: 24.20%, Val Loss: 1.4027, Val Acc: 25.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 125/125 [00:01<00:00, 103.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Train Loss: 1.4156, Train Acc: 28.20%, Val Loss: 1.4204, Val Acc: 20.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 125/125 [00:01<00:00, 100.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Train Loss: 1.4243, Train Acc: 26.80%, Val Loss: 1.4168, Val Acc: 24.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 125/125 [00:01<00:00, 103.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 1.4192, Train Acc: 27.00%, Val Loss: 1.3883, Val Acc: 28.50%\n"
     ]
    }
   ],
   "source": [
    "# # ================== Hyper-parameters ==================== #\n",
    "\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 8\n",
    "# ======================= Training (First 50 documents) ======================= #\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(\"Running on MGPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(\"Running on CUDA\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "dim, word_to_id, embeddings = load_glove_embeddings_aligned(glove_6b_300_path, tokenizer)\n",
    "\n",
    "# Use only first 50 documents for testing\n",
    "print(\"Using first 10000 documents for testing...\")\n",
    "train_data_small = model_train_data[:1000]\n",
    "train_labels_small = model_train_labels[:1000]\n",
    "valid_data_small = model_valid_data[:200]\n",
    "valid_labels_small = model_valid_labels[:200]\n",
    "test_data_small = test_data[:100] \n",
    "test_labels_small = test_labels[:100]\n",
    "\n",
    "training_set = HistoricalTextDataset(train_data_small, train_labels_small, word_to_id, decade_to_label)\n",
    "validation_set = HistoricalTextDataset(valid_data_small, valid_labels_small, word_to_id, decade_to_label)\n",
    "test_set = HistoricalTextDataset(test_data_small, test_labels_small, word_to_id, decade_to_label)\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=batch_size, collate_fn=pad_sequence_documents)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, collate_fn=pad_sequence_documents)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=pad_sequence_documents)\n",
    "\n",
    "print(f\"Training on {len(training_set)} documents\")\n",
    "print(f\"Validation on {len(validation_set)} documents\")\n",
    "print(f\"Testing on {len(test_set)} documents\")\n",
    "\n",
    "lstm_classifier = DocumentClassifier(\n",
    "    word_embeddings=embeddings,\n",
    "    word_to_id=word_to_id,\n",
    "    num_classes=len(decades), \n",
    "    word_hidden_size=128, \n",
    "    dropout_rate=0.2,\n",
    "    num_layers=1,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # TRAINING PHASE\n",
    "    lstm_classifier.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    \n",
    "    for x, y in tqdm(training_loader, desc=\"Epoch {}\".format(epoch + 1)):\n",
    "        x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = lstm_classifier(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(lstm_classifier.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track training accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "    \n",
    "    # VALIDATION PHASE\n",
    "    lstm_classifier.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in validation_loader:\n",
    "            x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "            y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "            \n",
    "            logits = lstm_classifier(x)\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            val_total += y.size(0)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(validation_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_loss = epoch_loss / len(training_loader)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0305e93-d931-40d3-965f-dd41ac73817c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "Predicted -> ['D1770', 'D1810', 'D1850', 'D1890']\n",
      "Actual D1770: [945, 510, 605, 40]\n",
      "Actual D1810: [711, 399, 341, 18]\n",
      "Actual D1850: [1753, 986, 909, 65]\n",
      "Actual D1890: [304, 180, 113, 7]\n",
      "Accuracy: 0.2866\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "lstm_classifier.eval()\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_set = HistoricalTextDataset(test_data, test_labels, word_to_id, decade_to_label)\n",
    "test_loader = DataLoader(test_set, batch_size=16, collate_fn=pad_sequence_documents)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        # Convert to tensors\n",
    "        x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        pred = torch.argmax(lstm_classifier(x), dim=-1).detach().cpu().numpy()\n",
    "        y_np = y.detach().cpu().numpy()\n",
    "        \n",
    "        all_predictions.extend(pred)\n",
    "        all_labels.extend(y_np)\n",
    "\n",
    "# confusion matrix \n",
    "num_classes = len(decade_to_label)\n",
    "confusion_matrix_manual = [[0 for _ in range(num_classes)] for _ in range(num_classes)]\n",
    "\n",
    "for i in range(len(all_predictions)):\n",
    "    actual = all_labels[i]\n",
    "    predicted = all_predictions[i]\n",
    "    confusion_matrix_manual[actual][predicted] += 1\n",
    "\n",
    "# Print results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"Predicted ->\", [f\"D{label_to_decade[i]}\" for i in range(num_classes)])\n",
    "for i, row in enumerate(confusion_matrix_manual):\n",
    "    print(f\"Actual D{label_to_decade[i]}: {row}\")\n",
    "\n",
    "accuracy = sum(confusion_matrix_manual[i][i] for i in range(num_classes)) / sum(sum(row) for row in confusion_matrix_manual)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8d8acbf-f4dc-49f6-a686-60b03868a774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CUDA\n",
      "Loaded 400000 GloVe vectors with dimension 50\n",
      "Found 55798/146797 words from your vocabulary in GloVe\n",
      "Coverage: 38.01%\n",
      "Training on 1920 documents\n",
      "Validation on 480 documents\n",
      "Testing on 7886 documents\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 80/80 [00:02<00:00, 39.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8], Train Loss: 1.3882, Train Acc: 26.15%, Val Loss: 1.3845, Val Acc: 26.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 80/80 [00:02<00:00, 38.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8], Train Loss: 1.3841, Train Acc: 25.62%, Val Loss: 1.3867, Val Acc: 25.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 80/80 [00:02<00:00, 38.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8], Train Loss: 1.3806, Train Acc: 24.69%, Val Loss: 1.3864, Val Acc: 26.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 80/80 [00:02<00:00, 39.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8], Train Loss: 1.3793, Train Acc: 26.15%, Val Loss: 1.3848, Val Acc: 25.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 80/80 [00:02<00:00, 38.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8], Train Loss: 1.3774, Train Acc: 26.77%, Val Loss: 1.3846, Val Acc: 26.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 80/80 [00:02<00:00, 39.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8], Train Loss: 1.3810, Train Acc: 25.68%, Val Loss: 1.3841, Val Acc: 26.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 80/80 [00:02<00:00, 39.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8], Train Loss: 1.3776, Train Acc: 25.36%, Val Loss: 1.3839, Val Acc: 26.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 80/80 [00:02<00:00, 39.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8], Train Loss: 1.3785, Train Acc: 26.25%, Val Loss: 1.3837, Val Acc: 26.67%\n"
     ]
    }
   ],
   "source": [
    "# ================== Hyper-parameters ==================== #\n",
    "learning_rate = 0.001  \n",
    "epochs = 8\n",
    "batch_size = 24 \n",
    "weight_decay = 1e-4\n",
    "\n",
    "# ======================= Training ======================= #\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(\"Running on MGPU\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(\"Running on CUDA\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"Running on CPU\")\n",
    "\n",
    "dim, word_to_id, embeddings = load_glove_embeddings_aligned(glove_6b_50_path, tokenizer)\n",
    "\n",
    "training_set = HistoricalTextDataset(model_train_data, model_train_labels, word_to_id, decade_to_label)\n",
    "validation_set = HistoricalTextDataset(model_valid_data, model_valid_labels, word_to_id, decade_to_label)\n",
    "test_set = HistoricalTextDataset(test_data, test_labels, word_to_id, decade_to_label)\n",
    "\n",
    "training_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True, collate_fn=pad_sequence_documents)\n",
    "validation_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, collate_fn=pad_sequence_documents)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=pad_sequence_documents)\n",
    "\n",
    "print(f\"Training on {len(training_set)} documents\")\n",
    "print(f\"Validation on {len(validation_set)} documents\")\n",
    "print(f\"Testing on {len(test_set)} documents\")\n",
    "\n",
    "lstm_classifier = DocumentClassifier(\n",
    "    word_embeddings=embeddings,\n",
    "    word_to_id=word_to_id,\n",
    "    num_classes=len(decades), \n",
    "    word_hidden_size=64, \n",
    "    num_layers=1,\n",
    "    device=device,\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(lstm_classifier.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    # TRAINING PHASE\n",
    "    lstm_classifier.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    \n",
    "    for x, y in tqdm(training_loader, desc=\"Epoch {}\".format(epoch + 1)):\n",
    "        x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = lstm_classifier(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(lstm_classifier.parameters(), 5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track training accuracy\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted == y).sum().item()\n",
    "    \n",
    "    # VALIDATION PHASE\n",
    "    lstm_classifier.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in validation_loader:\n",
    "            x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "            y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "            \n",
    "            logits = lstm_classifier(x)\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            val_total += y.size(0)\n",
    "            val_correct += (predicted == y).sum().item()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    avg_val_loss = val_loss / len(validation_loader)\n",
    "    train_acc = 100 * correct / total\n",
    "    avg_loss = epoch_loss / len(training_loader)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ef5ba2a-7f9d-4d60-b3d7-87d54b431ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "Predicted -> ['D1770', 'D1810', 'D1850', 'D1890']\n",
      "Actual D1770: [72, 1672, 165, 191]\n",
      "Actual D1810: [20, 1329, 82, 38]\n",
      "Actual D1850: [43, 3402, 196, 72]\n",
      "Actual D1890: [6, 536, 36, 26]\n",
      "Accuracy: 0.2058\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "lstm_classifier.eval()\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_set = HistoricalTextDataset(test_data, test_labels, word_to_id, decade_to_label)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, collate_fn=pad_sequence_documents)\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        # Convert to tensors\n",
    "        x = torch.tensor(x, dtype=torch.long).to(device)\n",
    "        y = torch.tensor(y, dtype=torch.long).to(device)\n",
    "        pred = torch.argmax(lstm_classifier(x), dim=-1).detach().cpu().numpy()\n",
    "        y_np = y.detach().cpu().numpy()\n",
    "        \n",
    "        all_predictions.extend(pred)\n",
    "        all_labels.extend(y_np)\n",
    "\n",
    "# confusion matrix \n",
    "num_classes = len(decade_to_label)\n",
    "confusion_matrix_manual = [[0 for _ in range(num_classes)] for _ in range(num_classes)]\n",
    "\n",
    "for i in range(len(all_predictions)):\n",
    "    actual = all_labels[i]\n",
    "    predicted = all_predictions[i]\n",
    "    confusion_matrix_manual[actual][predicted] += 1\n",
    "\n",
    "# Print results\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"Predicted ->\", [f\"D{label_to_decade[i]}\" for i in range(num_classes)])\n",
    "for i, row in enumerate(confusion_matrix_manual):\n",
    "    print(f\"Actual D{label_to_decade[i]}: {row}\")\n",
    "\n",
    "accuracy = sum(confusion_matrix_manual[i][i] for i in range(num_classes)) / sum(sum(row) for row in confusion_matrix_manual)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060d7ceb-797f-44e7-b756-6322bab11594",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e81c1a-4077-4807-a645-a078ad1ab36c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a3155-99d5-4705-bf63-2b8a89c940da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1706ebab-3cbc-435b-b1f3-84b26ff15dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
